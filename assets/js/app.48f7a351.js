(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function t(t){for(var o,a,s=t[0],l=t[1],c=t[2],u=0,h=[];u<s.length;u++)a=s[u],Object.prototype.hasOwnProperty.call(i,a)&&i[a]&&h.push(i[a][0]),i[a]=0;for(o in l)Object.prototype.hasOwnProperty.call(l,o)&&(e[o]=l[o]);for(d&&d(t);h.length;)h.shift()();return r.push.apply(r,c||[]),n()}function n(){for(var e,t=0;t<r.length;t++){for(var n=r[t],o=!0,s=1;s<n.length;s++){var l=n[s];0!==i[l]&&(o=!1)}o&&(r.splice(t--,1),e=a(a.s=n[0]))}return e}var o={},i={1:0},r=[];function a(t){if(o[t])return o[t].exports;var n=o[t]={i:t,l:!1,exports:{}};return e[t].call(n.exports,n,n.exports,a),n.l=!0,n.exports}a.e=function(e){var t=[],n=i[e];if(0!==n)if(n)t.push(n[2]);else{var o=new Promise((function(t,o){n=i[e]=[t,o]}));t.push(n[2]=o);var r,s=document.createElement("script");s.charset="utf-8",s.timeout=120,a.nc&&s.setAttribute("nonce",a.nc),s.src=function(e){return a.p+"assets/js/"+({}[e]||e)+"."+{2:"c128029e",3:"67e9b347",4:"46ddc725",5:"07ba0faf",6:"95f58db5",7:"f07d7603",8:"7d2da622",9:"1e48d241",10:"44c6faa4",11:"9b5d561f",12:"b832f7ae",13:"6f04a818",14:"76df4b85",15:"51b28471",16:"3e15f319",17:"83782be7",18:"1358dac2",19:"c1589f0d",20:"269a86f4",21:"7f016381",22:"230757fa",23:"7602a859",24:"bebb1aea",25:"f84e6372",26:"9996de01",27:"df6238bf",28:"1f51e292",29:"92543450",30:"c5033060",31:"494c0452",32:"5a7df32d",33:"69397f89",34:"915bc3ac",35:"0005d609",36:"892bdac4",37:"35180fa3",38:"97224486",39:"ee1ca0b2",40:"0ceb7996",41:"2dabf3c9",42:"d2a087ac",43:"454695d2",44:"8b3471dc",45:"3c93b021",46:"db4fa6c9",47:"aa9b8f9c",48:"348ad7de",49:"75b9609f",50:"d8c9a430",51:"2dc787fb",52:"448f489d",53:"fc981fcf",54:"977822a7",55:"0f8a3b3c",56:"a2f2f0a4",57:"acf4487e",58:"111d50d8",59:"4b95506f",60:"632b8fae",61:"f0514164",62:"d4975ae8",63:"2652b9c3",64:"c0dfc909",65:"8b4c26bc",66:"2d5d0bce",67:"88f5d520",68:"7686f20c",69:"ae8b5ca5",70:"f91b07fa",71:"7bf55580",72:"a734a837",73:"4f398dbf",74:"36530076",75:"1d7deb84",76:"61d1d8a1",77:"3be635ce",78:"b811061c",79:"f6719018",80:"cf0a7330",81:"31b303ce",82:"8d489018",83:"327092e7",84:"fa58d2e7"}[e]+".js"}(e);var l=new Error;r=function(t){s.onerror=s.onload=null,clearTimeout(c);var n=i[e];if(0!==n){if(n){var o=t&&("load"===t.type?"missing":t.type),r=t&&t.target&&t.target.src;l.message="Loading chunk "+e+" failed.\n("+o+": "+r+")",l.name="ChunkLoadError",l.type=o,l.request=r,n[1](l)}i[e]=void 0}};var c=setTimeout((function(){r({type:"timeout",target:s})}),12e4);s.onerror=s.onload=r,document.head.appendChild(s)}return Promise.all(t)},a.m=e,a.c=o,a.d=function(e,t,n){a.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:n})},a.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},a.t=function(e,t){if(1&t&&(e=a(e)),8&t)return e;if(4&t&&"object"==typeof e&&e&&e.__esModule)return e;var n=Object.create(null);if(a.r(n),Object.defineProperty(n,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var o in e)a.d(n,o,function(t){return e[t]}.bind(null,o));return n},a.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return a.d(t,"a",t),t},a.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},a.p="/",a.oe=function(e){throw console.error(e),e};var s=window.webpackJsonp=window.webpackJsonp||[],l=s.push.bind(s);s.push=t,s=s.slice();for(var c=0;c<s.length;c++)t(s[c]);var d=l;r.push([200,0]),n()}([function(e,t,n){var o=n(2),i=n(22).f,r=n(15),a=n(11),s=n(78),l=n(133),c=n(76);e.exports=function(e,t){var n,d,u,h,f,p=e.target,m=e.global,w=e.stat;if(n=m?o:w?o[p]||s(p,{}):(o[p]||{}).prototype)for(d in t){if(h=t[d],u=e.noTargetGet?(f=i(n,d))&&f.value:n[d],!c(m?d:p+(w?".":"#")+d,e.forced)&&void 0!==u){if(typeof h==typeof u)continue;l(h,u)}(e.sham||u&&u.sham)&&r(h,"sham",!0),a(n,d,h,e)}}},function(e,t){e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,t){var n=function(e){return e&&e.Math==Math&&e};e.exports=n("object"==typeof globalThis&&globalThis)||n("object"==typeof window&&window)||n("object"==typeof self&&self)||n("object"==typeof global&&global)||Function("return this")()},function(e,t,n){var o=n(2),i=n(77),r=n(7),a=n(56),s=n(80),l=n(127),c=i("wks"),d=o.Symbol,u=l?d:d&&d.withoutSetter||a;e.exports=function(e){return r(c,e)||(s&&r(d,e)?c[e]=d[e]:c[e]=u("Symbol."+e)),c[e]}},function(e,t){e.exports=function(e){return"object"==typeof e?null!==e:"function"==typeof e}},function(e,t,n){var o=n(1);e.exports=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,t,n){var o=n(4);e.exports=function(e){if(!o(e))throw TypeError(String(e)+" is not an object");return e}},function(e,t){var n={}.hasOwnProperty;e.exports=function(e,t){return n.call(e,t)}},function(e,t,n){"use strict";function o(e,t,n,o,i,r,a,s){var l,c="function"==typeof e?e.options:e;if(t&&(c.render=t,c.staticRenderFns=n,c._compiled=!0),o&&(c.functional=!0),r&&(c._scopeId="data-v-"+r),a?(l=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),i&&i.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(a)},c._ssrRegister=l):i&&(l=s?function(){i.call(this,(c.functional?this.parent:this).$root.$options.shadowRoot)}:i),l)if(c.functional){c._injectStyles=l;var d=c.render;c.render=function(e,t){return l.call(t),d(e,t)}}else{var u=c.beforeCreate;c.beforeCreate=u?[].concat(u,l):[l]}return{exports:e,options:c}}n.d(t,"a",(function(){return o}))},function(e,t,n){var o=n(5),i=n(126),r=n(6),a=n(39),s=Object.defineProperty;t.f=o?s:function(e,t,n){if(r(e),t=a(t,!0),r(n),i)try{return s(e,t,n)}catch(e){}if("get"in n||"set"in n)throw TypeError("Accessors not supported");return"value"in n&&(e[t]=n.value),e}},function(e,t,n){var o=n(88),i=n(11),r=n(210);o||i(Object.prototype,"toString",r,{unsafe:!0})},function(e,t,n){var o=n(2),i=n(15),r=n(7),a=n(78),s=n(83),l=n(32),c=l.get,d=l.enforce,u=String(String).split("String");(e.exports=function(e,t,n,s){var l=!!s&&!!s.unsafe,c=!!s&&!!s.enumerable,h=!!s&&!!s.noTargetGet;"function"==typeof n&&("string"!=typeof t||r(n,"name")||i(n,"name",t),d(n).source=u.join("string"==typeof t?t:"")),e!==o?(l?!h&&e[t]&&(c=!0):delete e[t],c?e[t]=n:i(e,t,n)):c?e[t]=n:a(t,n)})(Function.prototype,"toString",(function(){return"function"==typeof this&&c(this).source||s(this)}))},function(e,t,n){var o=n(47),i=Math.min;e.exports=function(e){return e>0?i(o(e),9007199254740991):0}},function(e,t,n){var o=n(38),i=n(20);e.exports=function(e){return o(i(e))}},function(e,t,n){var o=n(20);e.exports=function(e){return Object(o(e))}},function(e,t,n){var o=n(5),i=n(9),r=n(35);e.exports=o?function(e,t,n){return i.f(e,t,r(1,n))}:function(e,t,n){return e[t]=n,e}},function(e,t,n){var o=n(5),i=n(1),r=n(7),a=Object.defineProperty,s={},l=function(e){throw e};e.exports=function(e,t){if(r(s,e))return s[e];t||(t={});var n=[][e],c=!!r(t,"ACCESSORS")&&t.ACCESSORS,d=r(t,0)?t[0]:l,u=r(t,1)?t[1]:void 0;return s[e]=!!n&&!i((function(){if(c&&!o)return!0;var e={length:-1};c?a(e,1,{enumerable:!0,get:l}):e[1]=1,n.call(e,d,u)}))}},function(e,t){var n=Array.isArray;e.exports=n},function(e,t){var n={}.toString;e.exports=function(e){return n.call(e).slice(8,-1)}},function(e,t,n){var o=n(157),i="object"==typeof self&&self&&self.Object===Object&&self,r=o||i||Function("return this")();e.exports=r},function(e,t){e.exports=function(e){if(null==e)throw TypeError("Can't call method on "+e);return e}},function(e,t,n){var o=n(131),i=n(2),r=function(e){return"function"==typeof e?e:void 0};e.exports=function(e,t){return arguments.length<2?r(o[e])||r(i[e]):o[e]&&o[e][t]||i[e]&&i[e][t]}},function(e,t,n){var o=n(5),i=n(84),r=n(35),a=n(13),s=n(39),l=n(7),c=n(126),d=Object.getOwnPropertyDescriptor;t.f=o?d:function(e,t){if(e=a(e),t=s(t,!0),c)try{return d(e,t)}catch(e){}if(l(e,t))return r(!i.f.call(e,t),e[t])}},function(e,t,n){"use strict";var o=n(0),i=n(62);o({target:"RegExp",proto:!0,forced:/./.exec!==i},{exec:i})},function(e,t,n){"use strict";var o=n(0),i=n(30).filter,r=n(59),a=n(16),s=r("filter"),l=a("filter");o({target:"Array",proto:!0,forced:!s||!l},{filter:function(e){return i(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,t){e.exports=!1},function(e,t){e.exports=function(e){if("function"!=typeof e)throw TypeError(String(e)+" is not a function");return e}},function(e,t,n){var o=n(235),i=n(238);e.exports=function(e,t){var n=i(e,t);return o(n)?n:void 0}},function(e,t,n){"use strict";var o=n(116).charAt,i=n(32),r=n(132),a=i.set,s=i.getterFor("String Iterator");r(String,"String",(function(e){a(this,{type:"String Iterator",string:String(e),index:0})}),(function(){var e,t=s(this),n=t.string,i=t.index;return i>=n.length?{value:void 0,done:!0}:(e=o(n,i),t.index+=e.length,{value:e,done:!1})}))},function(e,t,n){var o,i=n(6),r=n(113),a=n(82),s=n(40),l=n(130),c=n(79),d=n(58),u=d("IE_PROTO"),h=function(){},f=function(e){return"<script>"+e+"<\/script>"},p=function(){try{o=document.domain&&new ActiveXObject("htmlfile")}catch(e){}var e,t;p=o?function(e){e.write(f("")),e.close();var t=e.parentWindow.Object;return e=null,t}(o):((t=c("iframe")).style.display="none",l.appendChild(t),t.src=String("javascript:"),(e=t.contentWindow.document).open(),e.write(f("document.F=Object")),e.close(),e.F);for(var n=a.length;n--;)delete p.prototype[a[n]];return p()};s[u]=!0,e.exports=Object.create||function(e,t){var n;return null!==e?(h.prototype=i(e),n=new h,h.prototype=null,n[u]=e):n=p(),void 0===t?n:r(n,t)}},function(e,t,n){var o=n(49),i=n(38),r=n(14),a=n(12),s=n(115),l=[].push,c=function(e){var t=1==e,n=2==e,c=3==e,d=4==e,u=6==e,h=5==e||u;return function(f,p,m,w){for(var g,y,v=r(f),b=i(v),k=o(p,m,3),x=a(b.length),S=0,C=w||s,T=t?C(f,x):n?C(f,0):void 0;x>S;S++)if((h||S in b)&&(y=k(g=b[S],S,v),e))if(t)T[S]=y;else if(y)switch(e){case 3:return!0;case 5:return g;case 6:return S;case 2:l.call(T,g)}else if(d)return!1;return u?-1:c||d?d:T}};e.exports={forEach:c(0),map:c(1),filter:c(2),some:c(3),every:c(4),find:c(5),findIndex:c(6)}},function(e,t){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,t,n){var o,i,r,a=n(201),s=n(2),l=n(4),c=n(15),d=n(7),u=n(58),h=n(40),f=s.WeakMap;if(a){var p=new f,m=p.get,w=p.has,g=p.set;o=function(e,t){return g.call(p,e,t),t},i=function(e){return m.call(p,e)||{}},r=function(e){return w.call(p,e)}}else{var y=u("state");h[y]=!0,o=function(e,t){return c(e,y,t),t},i=function(e){return d(e,y)?e[y]:{}},r=function(e){return d(e,y)}}e.exports={set:o,get:i,has:r,enforce:function(e){return r(e)?i(e):o(e,{})},getterFor:function(e){return function(t){var n;if(!l(t)||(n=i(t)).type!==e)throw TypeError("Incompatible receiver, "+e+" required");return n}}}},function(e,t,n){var o=n(2),i=n(145),r=n(112),a=n(15),s=n(3),l=s("iterator"),c=s("toStringTag"),d=r.values;for(var u in i){var h=o[u],f=h&&h.prototype;if(f){if(f[l]!==d)try{a(f,l,d)}catch(e){f[l]=d}if(f[c]||a(f,c,u),i[u])for(var p in r)if(f[p]!==r[p])try{a(f,p,r[p])}catch(e){f[p]=r[p]}}}},function(e,t,n){"use strict";var o=n(1);e.exports=function(e,t){var n=[][e];return!!n&&o((function(){n.call(null,t||function(){throw 1},1)}))}},function(e,t){e.exports=function(e,t){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:t}}},function(e,t,n){var o=n(18);e.exports=Array.isArray||function(e){return"Array"==o(e)}},function(e,t,n){var o=n(43),i=n(220),r=n(221),a=o?o.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":a&&a in Object(e)?i(e):r(e)}},function(e,t,n){var o=n(1),i=n(18),r="".split;e.exports=o((function(){return!Object("z").propertyIsEnumerable(0)}))?function(e){return"String"==i(e)?r.call(e,""):Object(e)}:Object},function(e,t,n){var o=n(4);e.exports=function(e,t){if(!o(e))return e;var n,i;if(t&&"function"==typeof(n=e.toString)&&!o(i=n.call(e)))return i;if("function"==typeof(n=e.valueOf)&&!o(i=n.call(e)))return i;if(!t&&"function"==typeof(n=e.toString)&&!o(i=n.call(e)))return i;throw TypeError("Can't convert object to primitive value")}},function(e,t){e.exports={}},function(e,t){e.exports={}},function(e,t,n){"use strict";var o=n(0),i=n(2),r=n(21),a=n(25),s=n(5),l=n(80),c=n(127),d=n(1),u=n(7),h=n(36),f=n(4),p=n(6),m=n(14),w=n(13),g=n(39),y=n(35),v=n(29),b=n(57),k=n(54),x=n(216),S=n(85),C=n(22),T=n(9),I=n(84),A=n(15),_=n(11),E=n(77),j=n(58),O=n(40),W=n(56),P=n(3),q=n(153),R=n(154),D=n(48),N=n(32),z=n(30).forEach,F=j("hidden"),L=P("toPrimitive"),M=N.set,G=N.getterFor("Symbol"),H=Object.prototype,$=i.Symbol,U=r("JSON","stringify"),B=C.f,V=T.f,Q=x.f,Y=I.f,K=E("symbols"),J=E("op-symbols"),X=E("string-to-symbol-registry"),Z=E("symbol-to-string-registry"),ee=E("wks"),te=i.QObject,ne=!te||!te.prototype||!te.prototype.findChild,oe=s&&d((function(){return 7!=v(V({},"a",{get:function(){return V(this,"a",{value:7}).a}})).a}))?function(e,t,n){var o=B(H,t);o&&delete H[t],V(e,t,n),o&&e!==H&&V(H,t,o)}:V,ie=function(e,t){var n=K[e]=v($.prototype);return M(n,{type:"Symbol",tag:e,description:t}),s||(n.description=t),n},re=c?function(e){return"symbol"==typeof e}:function(e){return Object(e)instanceof $},ae=function(e,t,n){e===H&&ae(J,t,n),p(e);var o=g(t,!0);return p(n),u(K,o)?(n.enumerable?(u(e,F)&&e[F][o]&&(e[F][o]=!1),n=v(n,{enumerable:y(0,!1)})):(u(e,F)||V(e,F,y(1,{})),e[F][o]=!0),oe(e,o,n)):V(e,o,n)},se=function(e,t){p(e);var n=w(t),o=b(n).concat(ue(n));return z(o,(function(t){s&&!le.call(n,t)||ae(e,t,n[t])})),e},le=function(e){var t=g(e,!0),n=Y.call(this,t);return!(this===H&&u(K,t)&&!u(J,t))&&(!(n||!u(this,t)||!u(K,t)||u(this,F)&&this[F][t])||n)},ce=function(e,t){var n=w(e),o=g(t,!0);if(n!==H||!u(K,o)||u(J,o)){var i=B(n,o);return!i||!u(K,o)||u(n,F)&&n[F][o]||(i.enumerable=!0),i}},de=function(e){var t=Q(w(e)),n=[];return z(t,(function(e){u(K,e)||u(O,e)||n.push(e)})),n},ue=function(e){var t=e===H,n=Q(t?J:w(e)),o=[];return z(n,(function(e){!u(K,e)||t&&!u(H,e)||o.push(K[e])})),o};(l||(_(($=function(){if(this instanceof $)throw TypeError("Symbol is not a constructor");var e=arguments.length&&void 0!==arguments[0]?String(arguments[0]):void 0,t=W(e),n=function(e){this===H&&n.call(J,e),u(this,F)&&u(this[F],t)&&(this[F][t]=!1),oe(this,t,y(1,e))};return s&&ne&&oe(H,t,{configurable:!0,set:n}),ie(t,e)}).prototype,"toString",(function(){return G(this).tag})),_($,"withoutSetter",(function(e){return ie(W(e),e)})),I.f=le,T.f=ae,C.f=ce,k.f=x.f=de,S.f=ue,q.f=function(e){return ie(P(e),e)},s&&(V($.prototype,"description",{configurable:!0,get:function(){return G(this).description}}),a||_(H,"propertyIsEnumerable",le,{unsafe:!0}))),o({global:!0,wrap:!0,forced:!l,sham:!l},{Symbol:$}),z(b(ee),(function(e){R(e)})),o({target:"Symbol",stat:!0,forced:!l},{for:function(e){var t=String(e);if(u(X,t))return X[t];var n=$(t);return X[t]=n,Z[n]=t,n},keyFor:function(e){if(!re(e))throw TypeError(e+" is not a symbol");if(u(Z,e))return Z[e]},useSetter:function(){ne=!0},useSimple:function(){ne=!1}}),o({target:"Object",stat:!0,forced:!l,sham:!s},{create:function(e,t){return void 0===t?v(e):se(v(e),t)},defineProperty:ae,defineProperties:se,getOwnPropertyDescriptor:ce}),o({target:"Object",stat:!0,forced:!l},{getOwnPropertyNames:de,getOwnPropertySymbols:ue}),o({target:"Object",stat:!0,forced:d((function(){S.f(1)}))},{getOwnPropertySymbols:function(e){return S.f(m(e))}}),U)&&o({target:"JSON",stat:!0,forced:!l||d((function(){var e=$();return"[null]"!=U([e])||"{}"!=U({a:e})||"{}"!=U(Object(e))}))},{stringify:function(e,t,n){for(var o,i=[e],r=1;arguments.length>r;)i.push(arguments[r++]);if(o=t,(f(t)||void 0!==e)&&!re(e))return h(t)||(t=function(e,t){if("function"==typeof o&&(t=o.call(this,e,t)),!re(t))return t}),i[1]=t,U.apply(null,i)}});$.prototype[L]||A($.prototype,L,$.prototype.valueOf),D($,"Symbol"),O[F]=!0},function(e,t,n){var o=n(19).Symbol;e.exports=o},function(e,t,n){"use strict";n.d(t,"a",(function(){return r}));n(109);var o=n(45);n(42),n(63),n(92),n(155),n(10),n(28),n(33);var i=n(69);function r(e){return function(e){if(Array.isArray(e))return Object(o.a)(e)}(e)||function(e){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(e))return Array.from(e)}(e)||Object(i.a)(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,t,n){"use strict";function o(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,o=new Array(t);n<t;n++)o[n]=e[n];return o}n.d(t,"a",(function(){return o}))},function(e,t,n){"use strict";var o=n(107),i=n(6),r=n(14),a=n(12),s=n(47),l=n(20),c=n(119),d=n(108),u=Math.max,h=Math.min,f=Math.floor,p=/\$([$&'`]|\d\d?|<[^>]*>)/g,m=/\$([$&'`]|\d\d?)/g;o("replace",2,(function(e,t,n,o){var w=o.REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE,g=o.REPLACE_KEEPS_$0,y=w?"$":"$0";return[function(n,o){var i=l(this),r=null==n?void 0:n[e];return void 0!==r?r.call(n,i,o):t.call(String(i),n,o)},function(e,o){if(!w&&g||"string"==typeof o&&-1===o.indexOf(y)){var r=n(t,e,this,o);if(r.done)return r.value}var l=i(e),f=String(this),p="function"==typeof o;p||(o=String(o));var m=l.global;if(m){var b=l.unicode;l.lastIndex=0}for(var k=[];;){var x=d(l,f);if(null===x)break;if(k.push(x),!m)break;""===String(x[0])&&(l.lastIndex=c(f,a(l.lastIndex),b))}for(var S,C="",T=0,I=0;I<k.length;I++){x=k[I];for(var A=String(x[0]),_=u(h(s(x.index),f.length),0),E=[],j=1;j<x.length;j++)E.push(void 0===(S=x[j])?S:String(S));var O=x.groups;if(p){var W=[A].concat(E,_,f);void 0!==O&&W.push(O);var P=String(o.apply(void 0,W))}else P=v(A,f,_,E,O,o);_>=T&&(C+=f.slice(T,_)+P,T=_+A.length)}return C+f.slice(T)}];function v(e,n,o,i,a,s){var l=o+e.length,c=i.length,d=m;return void 0!==a&&(a=r(a),d=p),t.call(s,d,(function(t,r){var s;switch(r.charAt(0)){case"$":return"$";case"&":return e;case"`":return n.slice(0,o);case"'":return n.slice(l);case"<":s=a[r.slice(1,-1)];break;default:var d=+r;if(0===d)return t;if(d>c){var u=f(d/10);return 0===u?t:u<=c?void 0===i[u-1]?r.charAt(1):i[u-1]+r.charAt(1):t}s=i[d-1]}return void 0===s?"":s}))}}))},function(e,t){var n=Math.ceil,o=Math.floor;e.exports=function(e){return isNaN(e=+e)?0:(e>0?o:n)(e)}},function(e,t,n){var o=n(9).f,i=n(7),r=n(3)("toStringTag");e.exports=function(e,t,n){e&&!i(e=n?e:e.prototype,r)&&o(e,r,{configurable:!0,value:t})}},function(e,t,n){var o=n(26);e.exports=function(e,t,n){if(o(e),void 0===t)return e;switch(n){case 0:return function(){return e.call(t)};case 1:return function(n){return e.call(t,n)};case 2:return function(n,o){return e.call(t,n,o)};case 3:return function(n,o,i){return e.call(t,n,o,i)}}return function(){return e.apply(t,arguments)}}},function(e,t,n){"use strict";var o=n(0),i=n(4),r=n(36),a=n(129),s=n(12),l=n(13),c=n(60),d=n(3),u=n(59),h=n(16),f=u("slice"),p=h("slice",{ACCESSORS:!0,0:0,1:2}),m=d("species"),w=[].slice,g=Math.max;o({target:"Array",proto:!0,forced:!f||!p},{slice:function(e,t){var n,o,d,u=l(this),h=s(u.length),f=a(e,h),p=a(void 0===t?h:t,h);if(r(u)&&("function"!=typeof(n=u.constructor)||n!==Array&&!r(n.prototype)?i(n)&&null===(n=n[m])&&(n=void 0):n=void 0,n===Array||void 0===n))return w.call(u,f,p);for(o=new(void 0===n?Array:n)(g(p-f,0)),d=0;f<p;f++,d++)f in u&&c(o,d,u[f]);return o.length=d,o}})},function(e,t,n){"use strict";var o=n(0),i=n(146);o({target:"Array",proto:!0,forced:[].forEach!=i},{forEach:i})},function(e,t,n){var o=n(2),i=n(145),r=n(146),a=n(15);for(var s in i){var l=o[s],c=l&&l.prototype;if(c&&c.forEach!==r)try{a(c,"forEach",r)}catch(e){c.forEach=r}}},function(e,t,n){"use strict";var o=n(0),i=n(1),r=n(36),a=n(4),s=n(14),l=n(12),c=n(60),d=n(115),u=n(59),h=n(3),f=n(91),p=h("isConcatSpreadable"),m=f>=51||!i((function(){var e=[];return e[p]=!1,e.concat()[0]!==e})),w=u("concat"),g=function(e){if(!a(e))return!1;var t=e[p];return void 0!==t?!!t:r(e)};o({target:"Array",proto:!0,forced:!m||!w},{concat:function(e){var t,n,o,i,r,a=s(this),u=d(a,0),h=0;for(t=-1,o=arguments.length;t<o;t++)if(g(r=-1===t?a:arguments[t])){if(h+(i=l(r.length))>9007199254740991)throw TypeError("Maximum allowed index exceeded");for(n=0;n<i;n++,h++)n in r&&c(u,h,r[n])}else{if(h>=9007199254740991)throw TypeError("Maximum allowed index exceeded");c(u,h++,r)}return u.length=h,u}})},function(e,t,n){var o=n(128),i=n(82).concat("length","prototype");t.f=Object.getOwnPropertyNames||function(e){return o(e,i)}},function(e,t,n){var o=n(5),i=n(9).f,r=Function.prototype,a=r.toString,s=/^\s*function ([^ (]*)/;o&&!("name"in r)&&i(r,"name",{configurable:!0,get:function(){try{return a.call(this).match(s)[1]}catch(e){return""}}})},function(e,t){var n=0,o=Math.random();e.exports=function(e){return"Symbol("+String(void 0===e?"":e)+")_"+(++n+o).toString(36)}},function(e,t,n){var o=n(128),i=n(82);e.exports=Object.keys||function(e){return o(e,i)}},function(e,t,n){var o=n(77),i=n(56),r=o("keys");e.exports=function(e){return r[e]||(r[e]=i(e))}},function(e,t,n){var o=n(1),i=n(3),r=n(91),a=i("species");e.exports=function(e){return r>=51||!o((function(){var t=[];return(t.constructor={})[a]=function(){return{foo:1}},1!==t[e](Boolean).foo}))}},function(e,t,n){"use strict";var o=n(39),i=n(9),r=n(35);e.exports=function(e,t,n){var a=o(t);a in e?i.f(e,a,r(0,n)):e[a]=n}},function(e,t,n){"use strict";n.d(t,"a",(function(){return i}));n(10);function o(e,t,n,o,i,r,a){try{var s=e[r](a),l=s.value}catch(e){return void n(e)}s.done?t(l):Promise.resolve(l).then(o,i)}function i(e){return function(){var t=this,n=arguments;return new Promise((function(i,r){var a=e.apply(t,n);function s(e){o(a,i,r,s,l,"next",e)}function l(e){o(a,i,r,s,l,"throw",e)}s(void 0)}))}}},function(e,t,n){"use strict";var o,i,r=n(118),a=n(194),s=RegExp.prototype.exec,l=String.prototype.replace,c=s,d=(o=/a/,i=/b*/g,s.call(o,"a"),s.call(i,"a"),0!==o.lastIndex||0!==i.lastIndex),u=a.UNSUPPORTED_Y||a.BROKEN_CARET,h=void 0!==/()??/.exec("")[1];(d||h||u)&&(c=function(e){var t,n,o,i,a=this,c=u&&a.sticky,f=r.call(a),p=a.source,m=0,w=e;return c&&(-1===(f=f.replace("y","")).indexOf("g")&&(f+="g"),w=String(e).slice(a.lastIndex),a.lastIndex>0&&(!a.multiline||a.multiline&&"\n"!==e[a.lastIndex-1])&&(p="(?: "+p+")",w=" "+w,m++),n=new RegExp("^(?:"+p+")",f)),h&&(n=new RegExp("^"+p+"$(?!\\s)",f)),d&&(t=a.lastIndex),o=s.call(c?n:a,w),c?o?(o.input=o.input.slice(m),o[0]=o[0].slice(m),o.index=a.lastIndex,a.lastIndex+=o[0].length):a.lastIndex=0:d&&o&&(a.lastIndex=a.global?o.index+o[0].length:t),h&&o&&o.length>1&&l.call(o[0],n,(function(){for(i=1;i<arguments.length-2;i++)void 0===arguments[i]&&(o[i]=void 0)})),o}),e.exports=c},function(e,t,n){"use strict";var o=n(0),i=n(5),r=n(2),a=n(7),s=n(4),l=n(9).f,c=n(133),d=r.Symbol;if(i&&"function"==typeof d&&(!("description"in d.prototype)||void 0!==d().description)){var u={},h=function(){var e=arguments.length<1||void 0===arguments[0]?void 0:String(arguments[0]),t=this instanceof h?new d(e):void 0===e?d():d(e);return""===e&&(u[t]=!0),t};c(h,d);var f=h.prototype=d.prototype;f.constructor=h;var p=f.toString,m="Symbol(test)"==String(d("test")),w=/^Symbol\((.*)\)[^)]+$/;l(f,"description",{configurable:!0,get:function(){var e=s(this)?this.valueOf():this,t=p.call(e);if(a(u,e))return"";var n=m?t.slice(7,-1):t.replace(w,"$1");return""===n?void 0:n}}),o({global:!0,forced:!0},{Symbol:h})}},function(e,t,n){var o=n(225),i=n(226),r=n(227),a=n(228),s=n(229);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t,n){var o=n(159);e.exports=function(e,t){for(var n=e.length;n--;)if(o(e[n][0],t))return n;return-1}},function(e,t,n){var o=n(27)(Object,"create");e.exports=o},function(e,t,n){var o=n(247);e.exports=function(e,t){var n=e.__data__;return o(t)?n["string"==typeof t?"string":"hash"]:n.map}},function(e,t,n){var o=n(100);e.exports=function(e){if("string"==typeof e||o(e))return e;var t=e+"";return"0"==t&&1/e==-1/0?"-0":t}},function(e,t,n){"use strict";n.d(t,"a",(function(){return i}));n(155),n(50),n(120),n(55),n(10),n(110),n(28);var o=n(45);function i(e,t){if(e){if("string"==typeof e)return Object(o.a)(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);return"Object"===n&&e.constructor&&(n=e.constructor.name),"Map"===n||"Set"===n?Array.from(e):"Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?Object(o.a)(e,t):void 0}}},function(e,t){var n=/^\s+|\s+$/g,o=/^[-+]0x[0-9a-f]+$/i,i=/^0b[01]+$/i,r=/^0o[0-7]+$/i,a=parseInt,s="object"==typeof global&&global&&global.Object===Object&&global,l="object"==typeof self&&self&&self.Object===Object&&self,c=s||l||Function("return this")(),d=Object.prototype.toString,u=Math.max,h=Math.min,f=function(){return c.Date.now()};function p(e){var t=typeof e;return!!e&&("object"==t||"function"==t)}function m(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==d.call(e)}(e))return NaN;if(p(e)){var t="function"==typeof e.valueOf?e.valueOf():e;e=p(t)?t+"":t}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(n,"");var s=i.test(e);return s||r.test(e)?a(e.slice(2),s?2:8):o.test(e)?NaN:+e}e.exports=function(e,t,n){var o,i,r,a,s,l,c=0,d=!1,w=!1,g=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function y(t){var n=o,r=i;return o=i=void 0,c=t,a=e.apply(r,n)}function v(e){return c=e,s=setTimeout(k,t),d?y(e):a}function b(e){var n=e-l;return void 0===l||n>=t||n<0||w&&e-c>=r}function k(){var e=f();if(b(e))return x(e);s=setTimeout(k,function(e){var n=t-(e-l);return w?h(n,r-(e-c)):n}(e))}function x(e){return s=void 0,g&&o?y(e):(o=i=void 0,a)}function S(){var e=f(),n=b(e);if(o=arguments,i=this,l=e,n){if(void 0===s)return v(l);if(w)return s=setTimeout(k,t),y(l)}return void 0===s&&(s=setTimeout(k,t)),a}return t=m(t)||0,p(n)&&(d=!!n.leading,r=(w="maxWait"in n)?u(m(n.maxWait)||0,t):r,g="trailing"in n?!!n.trailing:g),S.cancel=function(){void 0!==s&&clearTimeout(s),c=0,o=l=i=s=void 0},S.flush=function(){return void 0===s?a:x(f())},S}},function(e,t,n){var o,i;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(i="function"==typeof(o=function(){var e,t,n={version:"0.2.0"},o=n.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function i(e,t,n){return e<t?t:e>n?n:e}function r(e){return 100*(-1+e)}n.configure=function(e){var t,n;for(t in e)void 0!==(n=e[t])&&e.hasOwnProperty(t)&&(o[t]=n);return this},n.status=null,n.set=function(e){var t=n.isStarted();e=i(e,o.minimum,1),n.status=1===e?null:e;var l=n.render(!t),c=l.querySelector(o.barSelector),d=o.speed,u=o.easing;return l.offsetWidth,a((function(t){""===o.positionUsing&&(o.positionUsing=n.getPositioningCSS()),s(c,function(e,t,n){var i;return(i="translate3d"===o.positionUsing?{transform:"translate3d("+r(e)+"%,0,0)"}:"translate"===o.positionUsing?{transform:"translate("+r(e)+"%,0)"}:{"margin-left":r(e)+"%"}).transition="all "+t+"ms "+n,i}(e,d,u)),1===e?(s(l,{transition:"none",opacity:1}),l.offsetWidth,setTimeout((function(){s(l,{transition:"all "+d+"ms linear",opacity:0}),setTimeout((function(){n.remove(),t()}),d)}),d)):setTimeout(t,d)})),this},n.isStarted=function(){return"number"==typeof n.status},n.start=function(){n.status||n.set(0);var e=function(){setTimeout((function(){n.status&&(n.trickle(),e())}),o.trickleSpeed)};return o.trickle&&e(),this},n.done=function(e){return e||n.status?n.inc(.3+.5*Math.random()).set(1):this},n.inc=function(e){var t=n.status;return t?("number"!=typeof e&&(e=(1-t)*i(Math.random()*t,.1,.95)),t=i(t+e,0,.994),n.set(t)):n.start()},n.trickle=function(){return n.inc(Math.random()*o.trickleRate)},e=0,t=0,n.promise=function(o){return o&&"resolved"!==o.state()?(0===t&&n.start(),e++,t++,o.always((function(){0==--t?(e=0,n.done()):n.set((e-t)/e)})),this):this},n.render=function(e){if(n.isRendered())return document.getElementById("nprogress");c(document.documentElement,"nprogress-busy");var t=document.createElement("div");t.id="nprogress",t.innerHTML=o.template;var i,a=t.querySelector(o.barSelector),l=e?"-100":r(n.status||0),d=document.querySelector(o.parent);return s(a,{transition:"all 0 linear",transform:"translate3d("+l+"%,0,0)"}),o.showSpinner||(i=t.querySelector(o.spinnerSelector))&&h(i),d!=document.body&&c(d,"nprogress-custom-parent"),d.appendChild(t),t},n.remove=function(){d(document.documentElement,"nprogress-busy"),d(document.querySelector(o.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&h(e)},n.isRendered=function(){return!!document.getElementById("nprogress")},n.getPositioningCSS=function(){var e=document.body.style,t="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return t+"Perspective"in e?"translate3d":t+"Transform"in e?"translate":"margin"};var a=function(){var e=[];function t(){var n=e.shift();n&&n(t)}return function(n){e.push(n),1==e.length&&t()}}(),s=function(){var e=["Webkit","O","Moz","ms"],t={};function n(n){return n=n.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,t){return t.toUpperCase()})),t[n]||(t[n]=function(t){var n=document.body.style;if(t in n)return t;for(var o,i=e.length,r=t.charAt(0).toUpperCase()+t.slice(1);i--;)if((o=e[i]+r)in n)return o;return t}(n))}function o(e,t,o){t=n(t),e.style[t]=o}return function(e,t){var n,i,r=arguments;if(2==r.length)for(n in t)void 0!==(i=t[n])&&t.hasOwnProperty(n)&&o(e,n,i);else o(e,r[1],r[2])}}();function l(e,t){return("string"==typeof e?e:u(e)).indexOf(" "+t+" ")>=0}function c(e,t){var n=u(e),o=n+t;l(n,t)||(e.className=o.substring(1))}function d(e,t){var n,o=u(e);l(e,t)&&(n=o.replace(" "+t+" "," "),e.className=n.substring(1,n.length-1))}function u(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function h(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return n})?o.call(t,n,t,e):o)||(e.exports=i)},function(e,t,n){"use strict";var o=n(0),i=n(30).map,r=n(59),a=n(16),s=r("map"),l=a("map");o({target:"Array",proto:!0,forced:!s||!l},{map:function(e){return i(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,t,n){var o=n(0),i=n(14),r=n(57);o({target:"Object",stat:!0,forced:n(1)((function(){r(1)}))},{keys:function(e){return r(i(e))}})},function(e,t,n){"use strict";var o=n(0),i=n(38),r=n(13),a=n(34),s=[].join,l=i!=Object,c=a("join",",");o({target:"Array",proto:!0,forced:l||!c},{join:function(e){return s.call(r(this),void 0===e?",":e)}})},function(e,t,n){var o=n(3),i=n(29),r=n(9),a=o("unscopables"),s=Array.prototype;null==s[a]&&r.f(s,a,{configurable:!0,value:i(null)}),e.exports=function(e){s[a][e]=!0}},function(e,t,n){var o=n(1),i=/#|\.prototype\./,r=function(e,t){var n=s[a(e)];return n==c||n!=l&&("function"==typeof t?o(t):!!t)},a=r.normalize=function(e){return String(e).replace(i,".").toLowerCase()},s=r.data={},l=r.NATIVE="N",c=r.POLYFILL="P";e.exports=r},function(e,t,n){var o=n(25),i=n(125);(e.exports=function(e,t){return i[e]||(i[e]=void 0!==t?t:{})})("versions",[]).push({version:"3.6.5",mode:o?"pure":"global",copyright:"© 2020 Denis Pushkarev (zloirock.ru)"})},function(e,t,n){var o=n(2),i=n(15);e.exports=function(e,t){try{i(o,e,t)}catch(n){o[e]=t}return t}},function(e,t,n){var o=n(2),i=n(4),r=o.document,a=i(r)&&i(r.createElement);e.exports=function(e){return a?r.createElement(e):{}}},function(e,t,n){var o=n(1);e.exports=!!Object.getOwnPropertySymbols&&!o((function(){return!String(Symbol())}))},function(e,t,n){var o=n(13),i=n(12),r=n(129),a=function(e){return function(t,n,a){var s,l=o(t),c=i(l.length),d=r(a,c);if(e&&n!=n){for(;c>d;)if((s=l[d++])!=s)return!0}else for(;c>d;d++)if((e||d in l)&&l[d]===n)return e||d||0;return!e&&-1}};e.exports={includes:a(!0),indexOf:a(!1)}},function(e,t){e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,t,n){var o=n(125),i=Function.toString;"function"!=typeof o.inspectSource&&(o.inspectSource=function(e){return i.call(e)}),e.exports=o.inspectSource},function(e,t,n){"use strict";var o={}.propertyIsEnumerable,i=Object.getOwnPropertyDescriptor,r=i&&!o.call({1:2},1);t.f=r?function(e){var t=i(this,e);return!!t&&t.enumerable}:o},function(e,t){t.f=Object.getOwnPropertySymbols},function(e,t,n){var o=n(7),i=n(14),r=n(58),a=n(136),s=r("IE_PROTO"),l=Object.prototype;e.exports=a?Object.getPrototypeOf:function(e){return e=i(e),o(e,s)?e[s]:"function"==typeof e.constructor&&e instanceof e.constructor?e.constructor.prototype:e instanceof Object?l:null}},function(e,t,n){var o=n(6),i=n(202);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,t=!1,n={};try{(e=Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set).call(n,[]),t=n instanceof Array}catch(e){}return function(n,r){return o(n),i(r),t?e.call(n,r):n.__proto__=r,n}}():void 0)},function(e,t,n){var o={};o[n(3)("toStringTag")]="z",e.exports="[object z]"===String(o)},function(e,t,n){var o=n(6),i=n(26),r=n(3)("species");e.exports=function(e,t){var n,a=o(e).constructor;return void 0===a||null==(n=o(a)[r])?t:i(n)}},function(e,t,n){var o=n(21);e.exports=o("navigator","userAgent")||""},function(e,t,n){var o,i,r=n(2),a=n(90),s=r.process,l=s&&s.versions,c=l&&l.v8;c?i=(o=c.split("."))[0]+o[1]:a&&(!(o=a.match(/Edge\/(\d+)/))||o[1]>=74)&&(o=a.match(/Chrome\/(\d+)/))&&(i=o[1]),e.exports=i&&+i},function(e,t,n){n(154)("iterator")},function(e,t,n){var o=n(219),i=n(31),r=Object.prototype,a=r.hasOwnProperty,s=r.propertyIsEnumerable,l=o(function(){return arguments}())?o:function(e){return i(e)&&a.call(e,"callee")&&!s.call(e,"callee")};e.exports=l},function(e,t,n){var o=n(27)(n(19),"Map");e.exports=o},function(e,t){e.exports=function(e){var t=typeof e;return null!=e&&("object"==t||"function"==t)}},function(e,t,n){var o=n(239),i=n(246),r=n(248),a=n(249),s=n(250);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t){e.exports=function(e){var t=-1,n=Array(e.size);return e.forEach((function(e){n[++t]=e})),n}},function(e,t){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,t,n){var o=n(17),i=n(100),r=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,a=/^\w*$/;e.exports=function(e,t){if(o(e))return!1;var n=typeof e;return!("number"!=n&&"symbol"!=n&&"boolean"!=n&&null!=e&&!i(e))||(a.test(e)||!r.test(e)||null!=t&&e in Object(t))}},function(e,t,n){var o=n(37),i=n(31);e.exports=function(e){return"symbol"==typeof e||i(e)&&"[object Symbol]"==o(e)}},function(e,t){e.exports=function(e){return e}},function(e,t,n){var o=n(0),i=n(5);o({target:"Object",stat:!0,forced:!i,sham:!i},{defineProperty:n(9).f})},function(e,t,n){"use strict";n.d(t,"a",(function(){return i}));n(109);n(42),n(63),n(92),n(10),n(28),n(33);var o=n(69);function i(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(e)){var n=[],o=!0,i=!1,r=void 0;try{for(var a,s=e[Symbol.iterator]();!(o=(a=s.next()).done)&&(n.push(a.value),!t||n.length!==t);o=!0);}catch(e){i=!0,r=e}finally{try{o||null==s.return||s.return()}finally{if(i)throw r}}return n}}(e,t)||Object(o.a)(e,t)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,t,n){"use strict";var o=n(107),i=n(117),r=n(6),a=n(20),s=n(89),l=n(119),c=n(12),d=n(108),u=n(62),h=n(1),f=[].push,p=Math.min,m=!h((function(){return!RegExp(4294967295,"y")}));o("split",2,(function(e,t,n){var o;return o="c"=="abbc".split(/(b)*/)[1]||4!="test".split(/(?:)/,-1).length||2!="ab".split(/(?:ab)*/).length||4!=".".split(/(.?)(.?)/).length||".".split(/()()/).length>1||"".split(/.?/).length?function(e,n){var o=String(a(this)),r=void 0===n?4294967295:n>>>0;if(0===r)return[];if(void 0===e)return[o];if(!i(e))return t.call(o,e,r);for(var s,l,c,d=[],h=(e.ignoreCase?"i":"")+(e.multiline?"m":"")+(e.unicode?"u":"")+(e.sticky?"y":""),p=0,m=new RegExp(e.source,h+"g");(s=u.call(m,o))&&!((l=m.lastIndex)>p&&(d.push(o.slice(p,s.index)),s.length>1&&s.index<o.length&&f.apply(d,s.slice(1)),c=s[0].length,p=l,d.length>=r));)m.lastIndex===s.index&&m.lastIndex++;return p===o.length?!c&&m.test("")||d.push(""):d.push(o.slice(p)),d.length>r?d.slice(0,r):d}:"0".split(void 0,0).length?function(e,n){return void 0===e&&0===n?[]:t.call(this,e,n)}:t,[function(t,n){var i=a(this),r=null==t?void 0:t[e];return void 0!==r?r.call(t,i,n):o.call(String(i),t,n)},function(e,i){var a=n(o,e,this,i,o!==t);if(a.done)return a.value;var u=r(e),h=String(this),f=s(u,RegExp),w=u.unicode,g=(u.ignoreCase?"i":"")+(u.multiline?"m":"")+(u.unicode?"u":"")+(m?"y":"g"),y=new f(m?u:"^(?:"+u.source+")",g),v=void 0===i?4294967295:i>>>0;if(0===v)return[];if(0===h.length)return null===d(y,h)?[h]:[];for(var b=0,k=0,x=[];k<h.length;){y.lastIndex=m?k:0;var S,C=d(y,m?h:h.slice(k));if(null===C||(S=p(c(y.lastIndex+(m?0:k)),h.length))===b)k=l(h,k,w);else{if(x.push(h.slice(b,k)),x.length===v)return x;for(var T=1;T<=C.length-1;T++)if(x.push(C[T]),x.length===v)return x;k=b=S}}return x.push(h.slice(b)),x}]}),!m)},function(e,t,n){var o=n(114),i=n(41),r=n(3)("iterator");e.exports=function(e){if(null!=e)return e[r]||e["@@iterator"]||i[o(e)]}},function(e,t,n){var o=function(e){"use strict";var t=Object.prototype,n=t.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},i=o.iterator||"@@iterator",r=o.asyncIterator||"@@asyncIterator",a=o.toStringTag||"@@toStringTag";function s(e,t,n){return Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}),e[t]}try{s({},"")}catch(e){s=function(e,t,n){return e[t]=n}}function l(e,t,n,o){var i=t&&t.prototype instanceof u?t:u,r=Object.create(i.prototype),a=new S(o||[]);return r._invoke=function(e,t,n){var o="suspendedStart";return function(i,r){if("executing"===o)throw new Error("Generator is already running");if("completed"===o){if("throw"===i)throw r;return T()}for(n.method=i,n.arg=r;;){var a=n.delegate;if(a){var s=b(a,n);if(s){if(s===d)continue;return s}}if("next"===n.method)n.sent=n._sent=n.arg;else if("throw"===n.method){if("suspendedStart"===o)throw o="completed",n.arg;n.dispatchException(n.arg)}else"return"===n.method&&n.abrupt("return",n.arg);o="executing";var l=c(e,t,n);if("normal"===l.type){if(o=n.done?"completed":"suspendedYield",l.arg===d)continue;return{value:l.arg,done:n.done}}"throw"===l.type&&(o="completed",n.method="throw",n.arg=l.arg)}}}(e,n,a),r}function c(e,t,n){try{return{type:"normal",arg:e.call(t,n)}}catch(e){return{type:"throw",arg:e}}}e.wrap=l;var d={};function u(){}function h(){}function f(){}var p={};p[i]=function(){return this};var m=Object.getPrototypeOf,w=m&&m(m(C([])));w&&w!==t&&n.call(w,i)&&(p=w);var g=f.prototype=u.prototype=Object.create(p);function y(e){["next","throw","return"].forEach((function(t){s(e,t,(function(e){return this._invoke(t,e)}))}))}function v(e,t){var o;this._invoke=function(i,r){function a(){return new t((function(o,a){!function o(i,r,a,s){var l=c(e[i],e,r);if("throw"!==l.type){var d=l.arg,u=d.value;return u&&"object"==typeof u&&n.call(u,"__await")?t.resolve(u.__await).then((function(e){o("next",e,a,s)}),(function(e){o("throw",e,a,s)})):t.resolve(u).then((function(e){d.value=e,a(d)}),(function(e){return o("throw",e,a,s)}))}s(l.arg)}(i,r,o,a)}))}return o=o?o.then(a,a):a()}}function b(e,t){var n=e.iterator[t.method];if(void 0===n){if(t.delegate=null,"throw"===t.method){if(e.iterator.return&&(t.method="return",t.arg=void 0,b(e,t),"throw"===t.method))return d;t.method="throw",t.arg=new TypeError("The iterator does not provide a 'throw' method")}return d}var o=c(n,e.iterator,t.arg);if("throw"===o.type)return t.method="throw",t.arg=o.arg,t.delegate=null,d;var i=o.arg;return i?i.done?(t[e.resultName]=i.value,t.next=e.nextLoc,"return"!==t.method&&(t.method="next",t.arg=void 0),t.delegate=null,d):i:(t.method="throw",t.arg=new TypeError("iterator result is not an object"),t.delegate=null,d)}function k(e){var t={tryLoc:e[0]};1 in e&&(t.catchLoc=e[1]),2 in e&&(t.finallyLoc=e[2],t.afterLoc=e[3]),this.tryEntries.push(t)}function x(e){var t=e.completion||{};t.type="normal",delete t.arg,e.completion=t}function S(e){this.tryEntries=[{tryLoc:"root"}],e.forEach(k,this),this.reset(!0)}function C(e){if(e){var t=e[i];if(t)return t.call(e);if("function"==typeof e.next)return e;if(!isNaN(e.length)){var o=-1,r=function t(){for(;++o<e.length;)if(n.call(e,o))return t.value=e[o],t.done=!1,t;return t.value=void 0,t.done=!0,t};return r.next=r}}return{next:T}}function T(){return{value:void 0,done:!0}}return h.prototype=g.constructor=f,f.constructor=h,h.displayName=s(f,a,"GeneratorFunction"),e.isGeneratorFunction=function(e){var t="function"==typeof e&&e.constructor;return!!t&&(t===h||"GeneratorFunction"===(t.displayName||t.name))},e.mark=function(e){return Object.setPrototypeOf?Object.setPrototypeOf(e,f):(e.__proto__=f,s(e,a,"GeneratorFunction")),e.prototype=Object.create(g),e},e.awrap=function(e){return{__await:e}},y(v.prototype),v.prototype[r]=function(){return this},e.AsyncIterator=v,e.async=function(t,n,o,i,r){void 0===r&&(r=Promise);var a=new v(l(t,n,o,i),r);return e.isGeneratorFunction(n)?a:a.next().then((function(e){return e.done?e.value:a.next()}))},y(g),s(g,a,"Generator"),g[i]=function(){return this},g.toString=function(){return"[object Generator]"},e.keys=function(e){var t=[];for(var n in e)t.push(n);return t.reverse(),function n(){for(;t.length;){var o=t.pop();if(o in e)return n.value=o,n.done=!1,n}return n.done=!0,n}},e.values=C,S.prototype={constructor:S,reset:function(e){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(x),!e)for(var t in this)"t"===t.charAt(0)&&n.call(this,t)&&!isNaN(+t.slice(1))&&(this[t]=void 0)},stop:function(){this.done=!0;var e=this.tryEntries[0].completion;if("throw"===e.type)throw e.arg;return this.rval},dispatchException:function(e){if(this.done)throw e;var t=this;function o(n,o){return a.type="throw",a.arg=e,t.next=n,o&&(t.method="next",t.arg=void 0),!!o}for(var i=this.tryEntries.length-1;i>=0;--i){var r=this.tryEntries[i],a=r.completion;if("root"===r.tryLoc)return o("end");if(r.tryLoc<=this.prev){var s=n.call(r,"catchLoc"),l=n.call(r,"finallyLoc");if(s&&l){if(this.prev<r.catchLoc)return o(r.catchLoc,!0);if(this.prev<r.finallyLoc)return o(r.finallyLoc)}else if(s){if(this.prev<r.catchLoc)return o(r.catchLoc,!0)}else{if(!l)throw new Error("try statement without catch or finally");if(this.prev<r.finallyLoc)return o(r.finallyLoc)}}}},abrupt:function(e,t){for(var o=this.tryEntries.length-1;o>=0;--o){var i=this.tryEntries[o];if(i.tryLoc<=this.prev&&n.call(i,"finallyLoc")&&this.prev<i.finallyLoc){var r=i;break}}r&&("break"===e||"continue"===e)&&r.tryLoc<=t&&t<=r.finallyLoc&&(r=null);var a=r?r.completion:{};return a.type=e,a.arg=t,r?(this.method="next",this.next=r.finallyLoc,d):this.complete(a)},complete:function(e,t){if("throw"===e.type)throw e.arg;return"break"===e.type||"continue"===e.type?this.next=e.arg:"return"===e.type?(this.rval=this.arg=e.arg,this.method="return",this.next="end"):"normal"===e.type&&t&&(this.next=t),d},finish:function(e){for(var t=this.tryEntries.length-1;t>=0;--t){var n=this.tryEntries[t];if(n.finallyLoc===e)return this.complete(n.completion,n.afterLoc),x(n),d}},catch:function(e){for(var t=this.tryEntries.length-1;t>=0;--t){var n=this.tryEntries[t];if(n.tryLoc===e){var o=n.completion;if("throw"===o.type){var i=o.arg;x(n)}return i}}throw new Error("illegal catch attempt")},delegateYield:function(e,t,n){return this.delegate={iterator:C(e),resultName:t,nextLoc:n},"next"===this.method&&(this.arg=void 0),d}},e}(e.exports);try{regeneratorRuntime=o}catch(e){Function("r","regeneratorRuntime = r")(o)}},function(e,t,n){"use strict";n(23);var o=n(11),i=n(1),r=n(3),a=n(62),s=n(15),l=r("species"),c=!i((function(){var e=/./;return e.exec=function(){var e=[];return e.groups={a:"7"},e},"7"!=="".replace(e,"$<a>")})),d="$0"==="a".replace(/./,"$0"),u=r("replace"),h=!!/./[u]&&""===/./[u]("a","$0"),f=!i((function(){var e=/(?:)/,t=e.exec;e.exec=function(){return t.apply(this,arguments)};var n="ab".split(e);return 2!==n.length||"a"!==n[0]||"b"!==n[1]}));e.exports=function(e,t,n,u){var p=r(e),m=!i((function(){var t={};return t[p]=function(){return 7},7!=""[e](t)})),w=m&&!i((function(){var t=!1,n=/a/;return"split"===e&&((n={}).constructor={},n.constructor[l]=function(){return n},n.flags="",n[p]=/./[p]),n.exec=function(){return t=!0,null},n[p](""),!t}));if(!m||!w||"replace"===e&&(!c||!d||h)||"split"===e&&!f){var g=/./[p],y=n(p,""[e],(function(e,t,n,o,i){return t.exec===a?m&&!i?{done:!0,value:g.call(t,n,o)}:{done:!0,value:e.call(n,t,o)}:{done:!1}}),{REPLACE_KEEPS_$0:d,REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE:h}),v=y[0],b=y[1];o(String.prototype,e,v),o(RegExp.prototype,p,2==t?function(e,t){return b.call(e,this,t)}:function(e){return b.call(e,this)})}u&&s(RegExp.prototype[p],"sham",!0)}},function(e,t,n){var o=n(18),i=n(62);e.exports=function(e,t){var n=e.exec;if("function"==typeof n){var r=n.call(e,t);if("object"!=typeof r)throw TypeError("RegExp exec method returned something other than an Object or null");return r}if("RegExp"!==o(e))throw TypeError("RegExp#exec called on incompatible receiver");return i.call(e,t)}},function(e,t,n){n(0)({target:"Array",stat:!0},{isArray:n(36)})},function(e,t,n){"use strict";var o=n(11),i=n(6),r=n(1),a=n(118),s=RegExp.prototype,l=s.toString,c=r((function(){return"/a/b"!=l.call({source:"a",flags:"b"})})),d="toString"!=l.name;(c||d)&&o(RegExp.prototype,"toString",(function(){var e=i(this),t=String(e.source),n=e.flags;return"/"+t+"/"+String(void 0===n&&e instanceof RegExp&&!("flags"in s)?a.call(e):n)}),{unsafe:!0})},function(e,t,n){"use strict";var o=n(0),i=n(30).find,r=n(75),a=n(16),s=!0,l=a("find");"find"in[]&&Array(1).find((function(){s=!1})),o({target:"Array",proto:!0,forced:s||!l},{find:function(e){return i(this,e,arguments.length>1?arguments[1]:void 0)}}),r("find")},function(e,t,n){"use strict";var o=n(13),i=n(75),r=n(41),a=n(32),s=n(132),l=a.set,c=a.getterFor("Array Iterator");e.exports=s(Array,"Array",(function(e,t){l(this,{type:"Array Iterator",target:o(e),index:0,kind:t})}),(function(){var e=c(this),t=e.target,n=e.kind,o=e.index++;return!t||o>=t.length?(e.target=void 0,{value:void 0,done:!0}):"keys"==n?{value:o,done:!1}:"values"==n?{value:t[o],done:!1}:{value:[o,t[o]],done:!1}}),"values"),r.Arguments=r.Array,i("keys"),i("values"),i("entries")},function(e,t,n){var o=n(5),i=n(9),r=n(6),a=n(57);e.exports=o?Object.defineProperties:function(e,t){r(e);for(var n,o=a(t),s=o.length,l=0;s>l;)i.f(e,n=o[l++],t[n]);return e}},function(e,t,n){var o=n(88),i=n(18),r=n(3)("toStringTag"),a="Arguments"==i(function(){return arguments}());e.exports=o?i:function(e){var t,n,o;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(n=function(e,t){try{return e[t]}catch(e){}}(t=Object(e),r))?n:a?i(t):"Object"==(o=i(t))&&"function"==typeof t.callee?"Arguments":o}},function(e,t,n){var o=n(4),i=n(36),r=n(3)("species");e.exports=function(e,t){var n;return i(e)&&("function"!=typeof(n=e.constructor)||n!==Array&&!i(n.prototype)?o(n)&&null===(n=n[r])&&(n=void 0):n=void 0),new(void 0===n?Array:n)(0===t?0:t)}},function(e,t,n){var o=n(47),i=n(20),r=function(e){return function(t,n){var r,a,s=String(i(t)),l=o(n),c=s.length;return l<0||l>=c?e?"":void 0:(r=s.charCodeAt(l))<55296||r>56319||l+1===c||(a=s.charCodeAt(l+1))<56320||a>57343?e?s.charAt(l):r:e?s.slice(l,l+2):a-56320+(r-55296<<10)+65536}};e.exports={codeAt:r(!1),charAt:r(!0)}},function(e,t,n){var o=n(4),i=n(18),r=n(3)("match");e.exports=function(e){var t;return o(e)&&(void 0!==(t=e[r])?!!t:"RegExp"==i(e))}},function(e,t,n){"use strict";var o=n(6);e.exports=function(){var e=o(this),t="";return e.global&&(t+="g"),e.ignoreCase&&(t+="i"),e.multiline&&(t+="m"),e.dotAll&&(t+="s"),e.unicode&&(t+="u"),e.sticky&&(t+="y"),t}},function(e,t,n){"use strict";var o=n(116).charAt;e.exports=function(e,t,n){return t+(n?o(e,t).length:1)}},function(e,t,n){var o=n(11),i=Date.prototype,r=i.toString,a=i.getTime;new Date(NaN)+""!="Invalid Date"&&o(i,"toString",(function(){var e=a.call(this);return e==e?r.call(this):"Invalid Date"}))},function(e,t){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,t,n){var o=n(20),i="["+n(123)+"]",r=RegExp("^"+i+i+"*"),a=RegExp(i+i+"*$"),s=function(e){return function(t){var n=String(o(t));return 1&e&&(n=n.replace(r,"")),2&e&&(n=n.replace(a,"")),n}};e.exports={start:s(1),end:s(2),trim:s(3)}},function(e,t){e.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(e,t,n){"use strict";n.d(t,"a",(function(){return r}));n(42),n(24),n(51),n(311),n(102),n(312),n(148),n(73),n(52);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}},function(e,t,n){var o=n(2),i=n(78),r=o["__core-js_shared__"]||i("__core-js_shared__",{});e.exports=r},function(e,t,n){var o=n(5),i=n(1),r=n(79);e.exports=!o&&!i((function(){return 7!=Object.defineProperty(r("div"),"a",{get:function(){return 7}}).a}))},function(e,t,n){var o=n(80);e.exports=o&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,t,n){var o=n(7),i=n(13),r=n(81).indexOf,a=n(40);e.exports=function(e,t){var n,s=i(e),l=0,c=[];for(n in s)!o(a,n)&&o(s,n)&&c.push(n);for(;t.length>l;)o(s,n=t[l++])&&(~r(c,n)||c.push(n));return c}},function(e,t,n){var o=n(47),i=Math.max,r=Math.min;e.exports=function(e,t){var n=o(e);return n<0?i(n+t,0):r(n,t)}},function(e,t,n){var o=n(21);e.exports=o("document","documentElement")},function(e,t,n){var o=n(2);e.exports=o},function(e,t,n){"use strict";var o=n(0),i=n(189),r=n(86),a=n(87),s=n(48),l=n(15),c=n(11),d=n(3),u=n(25),h=n(41),f=n(135),p=f.IteratorPrototype,m=f.BUGGY_SAFARI_ITERATORS,w=d("iterator"),g=function(){return this};e.exports=function(e,t,n,d,f,y,v){i(n,t,d);var b,k,x,S=function(e){if(e===f&&_)return _;if(!m&&e in I)return I[e];switch(e){case"keys":case"values":case"entries":return function(){return new n(this,e)}}return function(){return new n(this)}},C=t+" Iterator",T=!1,I=e.prototype,A=I[w]||I["@@iterator"]||f&&I[f],_=!m&&A||S(f),E="Array"==t&&I.entries||A;if(E&&(b=r(E.call(new e)),p!==Object.prototype&&b.next&&(u||r(b)===p||(a?a(b,p):"function"!=typeof b[w]&&l(b,w,g)),s(b,C,!0,!0),u&&(h[C]=g))),"values"==f&&A&&"values"!==A.name&&(T=!0,_=function(){return A.call(this)}),u&&!v||I[w]===_||l(I,w,_),h[t]=_,f)if(k={values:S("values"),keys:y?_:S("keys"),entries:S("entries")},v)for(x in k)(m||T||!(x in I))&&c(I,x,k[x]);else o({target:t,proto:!0,forced:m||T},k);return k}},function(e,t,n){var o=n(7),i=n(134),r=n(22),a=n(9);e.exports=function(e,t){for(var n=i(t),s=a.f,l=r.f,c=0;c<n.length;c++){var d=n[c];o(e,d)||s(e,d,l(t,d))}}},function(e,t,n){var o=n(21),i=n(54),r=n(85),a=n(6);e.exports=o("Reflect","ownKeys")||function(e){var t=i.f(a(e)),n=r.f;return n?t.concat(n(e)):t}},function(e,t,n){"use strict";var o,i,r,a=n(86),s=n(15),l=n(7),c=n(3),d=n(25),u=c("iterator"),h=!1;[].keys&&("next"in(r=[].keys())?(i=a(a(r)))!==Object.prototype&&(o=i):h=!0),null==o&&(o={}),d||l(o,u)||s(o,u,(function(){return this})),e.exports={IteratorPrototype:o,BUGGY_SAFARI_ITERATORS:h}},function(e,t,n){var o=n(1);e.exports=!o((function(){function e(){}return e.prototype.constructor=null,Object.getPrototypeOf(new e)!==e.prototype}))},function(e,t,n){var o=n(2);e.exports=o.Promise},function(e,t,n){var o=n(3),i=n(41),r=o("iterator"),a=Array.prototype;e.exports=function(e){return void 0!==e&&(i.Array===e||a[r]===e)}},function(e,t,n){var o=n(6);e.exports=function(e,t,n,i){try{return i?t(o(n)[0],n[1]):t(n)}catch(t){var r=e.return;throw void 0!==r&&o(r.call(e)),t}}},function(e,t,n){var o=n(3)("iterator"),i=!1;try{var r=0,a={next:function(){return{done:!!r++}},return:function(){i=!0}};a[o]=function(){return this},Array.from(a,(function(){throw 2}))}catch(e){}e.exports=function(e,t){if(!t&&!i)return!1;var n=!1;try{var r={};r[o]=function(){return{next:function(){return{done:n=!0}}}},e(r)}catch(e){}return n}},function(e,t,n){var o,i,r,a=n(2),s=n(1),l=n(18),c=n(49),d=n(130),u=n(79),h=n(142),f=a.location,p=a.setImmediate,m=a.clearImmediate,w=a.process,g=a.MessageChannel,y=a.Dispatch,v=0,b={},k=function(e){if(b.hasOwnProperty(e)){var t=b[e];delete b[e],t()}},x=function(e){return function(){k(e)}},S=function(e){k(e.data)},C=function(e){a.postMessage(e+"",f.protocol+"//"+f.host)};p&&m||(p=function(e){for(var t=[],n=1;arguments.length>n;)t.push(arguments[n++]);return b[++v]=function(){("function"==typeof e?e:Function(e)).apply(void 0,t)},o(v),v},m=function(e){delete b[e]},"process"==l(w)?o=function(e){w.nextTick(x(e))}:y&&y.now?o=function(e){y.now(x(e))}:g&&!h?(r=(i=new g).port2,i.port1.onmessage=S,o=c(r.postMessage,r,1)):!a.addEventListener||"function"!=typeof postMessage||a.importScripts||s(C)||"file:"===f.protocol?o="onreadystatechange"in u("script")?function(e){d.appendChild(u("script")).onreadystatechange=function(){d.removeChild(this),k(e)}}:function(e){setTimeout(x(e),0)}:(o=C,a.addEventListener("message",S,!1))),e.exports={set:p,clear:m}},function(e,t,n){var o=n(90);e.exports=/(iphone|ipod|ipad).*applewebkit/i.test(o)},function(e,t,n){var o=n(6),i=n(4),r=n(144);e.exports=function(e,t){if(o(e),i(t)&&t.constructor===e)return t;var n=r.f(e);return(0,n.resolve)(t),n.promise}},function(e,t,n){"use strict";var o=n(26),i=function(e){var t,n;this.promise=new e((function(e,o){if(void 0!==t||void 0!==n)throw TypeError("Bad Promise constructor");t=e,n=o})),this.resolve=o(t),this.reject=o(n)};e.exports.f=function(e){return new i(e)}},function(e,t){e.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(e,t,n){"use strict";var o=n(30).forEach,i=n(34),r=n(16),a=i("forEach"),s=r("forEach");e.exports=a&&s?[].forEach:function(e){return o(this,e,arguments.length>1?arguments[1]:void 0)}},function(e,t,n){var o=n(1);e.exports=!o((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(e,t,n){var o=n(0),i=n(5),r=n(134),a=n(13),s=n(22),l=n(60);o({target:"Object",stat:!0,sham:!i},{getOwnPropertyDescriptors:function(e){for(var t,n,o=a(e),i=s.f,c=r(o),d={},u=0;c.length>u;)void 0!==(n=i(o,t=c[u++]))&&l(d,t,n);return d}})},function(e,t,n){var o=n(0),i=n(1),r=n(14),a=n(86),s=n(136);o({target:"Object",stat:!0,forced:i((function(){a(1)})),sham:!s},{getPrototypeOf:function(e){return a(r(e))}})},function(e,t,n){var o=n(117);e.exports=function(e){if(o(e))throw TypeError("The method doesn't accept regular expressions");return e}},function(e,t,n){var o=n(3)("match");e.exports=function(e){var t=/./;try{"/./"[e](t)}catch(n){try{return t[o]=!1,"/./"[e](t)}catch(e){}}return!1}},function(e,t,n){n(0)({target:"Object",stat:!0,sham:!n(5)},{create:n(29)})},function(e,t,n){var o=n(3);t.f=o},function(e,t,n){var o=n(131),i=n(7),r=n(153),a=n(9).f;e.exports=function(e){var t=o.Symbol||(o.Symbol={});i(t,e)||a(t,e,{value:r.f(e)})}},function(e,t,n){var o=n(0),i=n(196);o({target:"Array",stat:!0,forced:!n(140)((function(e){Array.from(e)}))},{from:i})},function(e,t){e.exports=function(e,t){for(var n=-1,o=t.length,i=e.length;++n<o;)e[i+n]=t[n];return e}},function(e,t){var n="object"==typeof global&&global&&global.Object===Object&&global;e.exports=n},function(e,t,n){var o=n(64),i=n(230),r=n(231),a=n(232),s=n(233),l=n(234);function c(e){var t=this.__data__=new o(e);this.size=t.size}c.prototype.clear=i,c.prototype.delete=r,c.prototype.get=a,c.prototype.has=s,c.prototype.set=l,e.exports=c},function(e,t){e.exports=function(e,t){return e===t||e!=e&&t!=t}},function(e,t,n){var o=n(37),i=n(95);e.exports=function(e){if(!i(e))return!1;var t=o(e);return"[object Function]"==t||"[object GeneratorFunction]"==t||"[object AsyncFunction]"==t||"[object Proxy]"==t}},function(e,t){var n=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return n.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,t,n){var o=n(251),i=n(31);e.exports=function e(t,n,r,a,s){return t===n||(null==t||null==n||!i(t)&&!i(n)?t!=t&&n!=n:o(t,n,r,a,e,s))}},function(e,t,n){var o=n(164),i=n(254),r=n(165);e.exports=function(e,t,n,a,s,l){var c=1&n,d=e.length,u=t.length;if(d!=u&&!(c&&u>d))return!1;var h=l.get(e),f=l.get(t);if(h&&f)return h==t&&f==e;var p=-1,m=!0,w=2&n?new o:void 0;for(l.set(e,t),l.set(t,e);++p<d;){var g=e[p],y=t[p];if(a)var v=c?a(y,g,p,t,e,l):a(g,y,p,e,t,l);if(void 0!==v){if(v)continue;m=!1;break}if(w){if(!i(t,(function(e,t){if(!r(w,t)&&(g===e||s(g,e,n,a,l)))return w.push(t)}))){m=!1;break}}else if(g!==y&&!s(g,y,n,a,l)){m=!1;break}}return l.delete(e),l.delete(t),m}},function(e,t,n){var o=n(96),i=n(252),r=n(253);function a(e){var t=-1,n=null==e?0:e.length;for(this.__data__=new o;++t<n;)this.add(e[t])}a.prototype.add=a.prototype.push=i,a.prototype.has=r,e.exports=a},function(e,t){e.exports=function(e,t){return e.has(t)}},function(e,t,n){var o=n(264),i=n(270),r=n(170);e.exports=function(e){return r(e)?o(e):i(e)}},function(e,t,n){(function(e){var o=n(19),i=n(266),r=t&&!t.nodeType&&t,a=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=a&&a.exports===r?o.Buffer:void 0,l=(s?s.isBuffer:void 0)||i;e.exports=l}).call(this,n(121)(e))},function(e,t){var n=/^(?:0|[1-9]\d*)$/;e.exports=function(e,t){var o=typeof e;return!!(t=null==t?9007199254740991:t)&&("number"==o||"symbol"!=o&&n.test(e))&&e>-1&&e%1==0&&e<t}},function(e,t,n){var o=n(267),i=n(268),r=n(269),a=r&&r.isTypedArray,s=a?i(a):o;e.exports=s},function(e,t,n){var o=n(160),i=n(98);e.exports=function(e){return null!=e&&i(e.length)&&!o(e)}},function(e,t,n){var o=n(27)(n(19),"Set");e.exports=o},function(e,t,n){var o=n(95);e.exports=function(e){return e==e&&!o(e)}},function(e,t){e.exports=function(e,t){return function(n){return null!=n&&(n[e]===t&&(void 0!==t||e in Object(n)))}}},function(e,t,n){var o=n(175),i=n(68);e.exports=function(e,t){for(var n=0,r=(t=o(t,e)).length;null!=e&&n<r;)e=e[i(t[n++])];return n&&n==r?e:void 0}},function(e,t,n){var o=n(17),i=n(99),r=n(281),a=n(284);e.exports=function(e,t){return o(e)?e:i(e,t)?[e]:r(a(e))}},function(e,t,n){var o=n(0),i=n(2),r=n(90),a=[].slice,s=function(e){return function(t,n){var o=arguments.length>2,i=o?a.call(arguments,2):void 0;return e(o?function(){("function"==typeof t?t:Function(t)).apply(this,i)}:t,n)}};o({global:!0,bind:!0,forced:/MSIE .\./.test(r)},{setTimeout:s(i.setTimeout),setInterval:s(i.setInterval)})},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){n(0)({target:"Object",stat:!0},{setPrototypeOf:n(87)})},function(e,t,n){var o=n(0),i=n(21),r=n(26),a=n(6),s=n(4),l=n(29),c=n(323),d=n(1),u=i("Reflect","construct"),h=d((function(){function e(){}return!(u((function(){}),[],e)instanceof e)})),f=!d((function(){u((function(){}))})),p=h||f;o({target:"Reflect",stat:!0,forced:p,sham:p},{construct:function(e,t){r(e),a(t);var n=arguments.length<3?e:r(arguments[2]);if(f&&!h)return u(e,t,n);if(e==n){switch(t.length){case 0:return new e;case 1:return new e(t[0]);case 2:return new e(t[0],t[1]);case 3:return new e(t[0],t[1],t[2]);case 4:return new e(t[0],t[1],t[2],t[3])}var o=[null];return o.push.apply(o,t),new(c.apply(e,o))}var i=n.prototype,d=l(s(i)?i:Object.prototype),p=Function.apply.call(e,d,t);return s(p)?p:d}})},function(e,t,n){},function(e,t,n){},function(e,t,n){var o=n(217),i=n(222),r=n(293),a=n(301),s=n(310),l=n(197),c=r((function(e){var t=l(e);return s(t)&&(t=void 0),a(o(e,1,s,!0),i(t,2))}));e.exports=c},function(e,t,n){"use strict";var o=n(0),i=n(30).some,r=n(34),a=n(16),s=r("some"),l=a("some");o({target:"Array",proto:!0,forced:!s||!l},{some:function(e){return i(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,t,n){"use strict";var o=n(0),i=n(81).indexOf,r=n(34),a=n(16),s=[].indexOf,l=!!s&&1/[1].indexOf(1,-0)<0,c=r("indexOf"),d=a("indexOf",{ACCESSORS:!0,1:0});o({target:"Array",proto:!0,forced:l||!c||!d},{indexOf:function(e){return l?s.apply(this,arguments)||0:i(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,t){e.exports=function(e,t,n){if(!(e instanceof t))throw TypeError("Incorrect "+(n?n+" ":"")+"invocation");return e}},function(e,t,n){"use strict";var o=n(135).IteratorPrototype,i=n(29),r=n(35),a=n(48),s=n(41),l=function(){return this};e.exports=function(e,t,n){var c=t+" Iterator";return e.prototype=i(o,{next:r(1,n)}),a(e,c,!1,!0),s[c]=l,e}},function(e,t,n){var o=n(11);e.exports=function(e,t,n){for(var i in t)o(e,i,t[i],n);return e}},function(e,t,n){"use strict";var o=n(21),i=n(9),r=n(3),a=n(5),s=r("species");e.exports=function(e){var t=o(e),n=i.f;a&&t&&!t[s]&&n(t,s,{configurable:!0,get:function(){return this}})}},function(e,t,n){"use strict";var o=n(5),i=n(1),r=n(57),a=n(85),s=n(84),l=n(14),c=n(38),d=Object.assign,u=Object.defineProperty;e.exports=!d||i((function(){if(o&&1!==d({b:1},d(u({},"a",{enumerable:!0,get:function(){u(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var e={},t={},n=Symbol();return e[n]=7,"abcdefghijklmnopqrst".split("").forEach((function(e){t[e]=e})),7!=d({},e)[n]||"abcdefghijklmnopqrst"!=r(d({},t)).join("")}))?function(e,t){for(var n=l(e),i=arguments.length,d=1,u=a.f,h=s.f;i>d;)for(var f,p=c(arguments[d++]),m=u?r(p).concat(u(p)):r(p),w=m.length,g=0;w>g;)f=m[g++],o&&!h.call(p,f)||(n[f]=p[f]);return n}:d},function(e,t,n){"use strict";var o=n(0),i=n(81).includes,r=n(75);o({target:"Array",proto:!0,forced:!n(16)("indexOf",{ACCESSORS:!0,1:0})},{includes:function(e){return i(this,e,arguments.length>1?arguments[1]:void 0)}}),r("includes")},function(e,t,n){"use strict";var o=n(1);function i(e,t){return RegExp(e,t)}t.UNSUPPORTED_Y=o((function(){var e=i("a","y");return e.lastIndex=2,null!=e.exec("abcd")})),t.BROKEN_CARET=o((function(){var e=i("^r","gy");return e.lastIndex=2,null!=e.exec("str")}))},function(e,t,n){"use strict";var o=n(0),i=n(150),r=n(20);o({target:"String",proto:!0,forced:!n(151)("includes")},{includes:function(e){return!!~String(r(this)).indexOf(i(e),arguments.length>1?arguments[1]:void 0)}})},function(e,t,n){"use strict";var o=n(49),i=n(14),r=n(139),a=n(138),s=n(12),l=n(60),c=n(105);e.exports=function(e){var t,n,d,u,h,f,p=i(e),m="function"==typeof this?this:Array,w=arguments.length,g=w>1?arguments[1]:void 0,y=void 0!==g,v=c(p),b=0;if(y&&(g=o(g,w>2?arguments[2]:void 0,2)),null==v||m==Array&&a(v))for(n=new m(t=s(p.length));t>b;b++)f=y?g(p[b],b):p[b],l(n,b,f);else for(h=(u=v.call(p)).next,n=new m;!(d=h.call(u)).done;b++)f=y?r(u,g,[d.value,b],!0):d.value,l(n,b,f);return n.length=b,n}},function(e,t){e.exports=function(e){var t=null==e?0:e.length;return t?e[t-1]:void 0}},function(e,t,n){var o=n(0),i=n(313);o({global:!0,forced:parseInt!=i},{parseInt:i})},function(e,t,n){var o=n(4),i=n(87);e.exports=function(e,t,n){var r,a;return i&&"function"==typeof(r=t.constructor)&&r!==n&&o(a=r.prototype)&&a!==n.prototype&&i(e,a),e}},function(e,t,n){e.exports=n(326)},function(e,t,n){var o=n(2),i=n(83),r=o.WeakMap;e.exports="function"==typeof r&&/native code/.test(i(r))},function(e,t,n){var o=n(4);e.exports=function(e){if(!o(e)&&null!==e)throw TypeError("Can't set "+String(e)+" as a prototype");return e}},function(e,t,n){"use strict";var o,i,r,a,s=n(0),l=n(25),c=n(2),d=n(21),u=n(137),h=n(11),f=n(190),p=n(48),m=n(191),w=n(4),g=n(26),y=n(188),v=n(18),b=n(83),k=n(204),x=n(140),S=n(89),C=n(141).set,T=n(205),I=n(143),A=n(206),_=n(144),E=n(207),j=n(32),O=n(76),W=n(3),P=n(91),q=W("species"),R="Promise",D=j.get,N=j.set,z=j.getterFor(R),F=u,L=c.TypeError,M=c.document,G=c.process,H=d("fetch"),$=_.f,U=$,B="process"==v(G),V=!!(M&&M.createEvent&&c.dispatchEvent),Q=O(R,(function(){if(!(b(F)!==String(F))){if(66===P)return!0;if(!B&&"function"!=typeof PromiseRejectionEvent)return!0}if(l&&!F.prototype.finally)return!0;if(P>=51&&/native code/.test(F))return!1;var e=F.resolve(1),t=function(e){e((function(){}),(function(){}))};return(e.constructor={})[q]=t,!(e.then((function(){}))instanceof t)})),Y=Q||!x((function(e){F.all(e).catch((function(){}))})),K=function(e){var t;return!(!w(e)||"function"!=typeof(t=e.then))&&t},J=function(e,t,n){if(!t.notified){t.notified=!0;var o=t.reactions;T((function(){for(var i=t.value,r=1==t.state,a=0;o.length>a;){var s,l,c,d=o[a++],u=r?d.ok:d.fail,h=d.resolve,f=d.reject,p=d.domain;try{u?(r||(2===t.rejection&&te(e,t),t.rejection=1),!0===u?s=i:(p&&p.enter(),s=u(i),p&&(p.exit(),c=!0)),s===d.promise?f(L("Promise-chain cycle")):(l=K(s))?l.call(s,h,f):h(s)):f(i)}catch(e){p&&!c&&p.exit(),f(e)}}t.reactions=[],t.notified=!1,n&&!t.rejection&&Z(e,t)}))}},X=function(e,t,n){var o,i;V?((o=M.createEvent("Event")).promise=t,o.reason=n,o.initEvent(e,!1,!0),c.dispatchEvent(o)):o={promise:t,reason:n},(i=c["on"+e])?i(o):"unhandledrejection"===e&&A("Unhandled promise rejection",n)},Z=function(e,t){C.call(c,(function(){var n,o=t.value;if(ee(t)&&(n=E((function(){B?G.emit("unhandledRejection",o,e):X("unhandledrejection",e,o)})),t.rejection=B||ee(t)?2:1,n.error))throw n.value}))},ee=function(e){return 1!==e.rejection&&!e.parent},te=function(e,t){C.call(c,(function(){B?G.emit("rejectionHandled",e):X("rejectionhandled",e,t.value)}))},ne=function(e,t,n,o){return function(i){e(t,n,i,o)}},oe=function(e,t,n,o){t.done||(t.done=!0,o&&(t=o),t.value=n,t.state=2,J(e,t,!0))},ie=function(e,t,n,o){if(!t.done){t.done=!0,o&&(t=o);try{if(e===n)throw L("Promise can't be resolved itself");var i=K(n);i?T((function(){var o={done:!1};try{i.call(n,ne(ie,e,o,t),ne(oe,e,o,t))}catch(n){oe(e,o,n,t)}})):(t.value=n,t.state=1,J(e,t,!1))}catch(n){oe(e,{done:!1},n,t)}}};Q&&(F=function(e){y(this,F,R),g(e),o.call(this);var t=D(this);try{e(ne(ie,this,t),ne(oe,this,t))}catch(e){oe(this,t,e)}},(o=function(e){N(this,{type:R,done:!1,notified:!1,parent:!1,reactions:[],rejection:!1,state:0,value:void 0})}).prototype=f(F.prototype,{then:function(e,t){var n=z(this),o=$(S(this,F));return o.ok="function"!=typeof e||e,o.fail="function"==typeof t&&t,o.domain=B?G.domain:void 0,n.parent=!0,n.reactions.push(o),0!=n.state&&J(this,n,!1),o.promise},catch:function(e){return this.then(void 0,e)}}),i=function(){var e=new o,t=D(e);this.promise=e,this.resolve=ne(ie,e,t),this.reject=ne(oe,e,t)},_.f=$=function(e){return e===F||e===r?new i(e):U(e)},l||"function"!=typeof u||(a=u.prototype.then,h(u.prototype,"then",(function(e,t){var n=this;return new F((function(e,t){a.call(n,e,t)})).then(e,t)}),{unsafe:!0}),"function"==typeof H&&s({global:!0,enumerable:!0,forced:!0},{fetch:function(e){return I(F,H.apply(c,arguments))}}))),s({global:!0,wrap:!0,forced:Q},{Promise:F}),p(F,R,!1,!0),m(R),r=d(R),s({target:R,stat:!0,forced:Q},{reject:function(e){var t=$(this);return t.reject.call(void 0,e),t.promise}}),s({target:R,stat:!0,forced:l||Q},{resolve:function(e){return I(l&&this===r?F:this,e)}}),s({target:R,stat:!0,forced:Y},{all:function(e){var t=this,n=$(t),o=n.resolve,i=n.reject,r=E((function(){var n=g(t.resolve),r=[],a=0,s=1;k(e,(function(e){var l=a++,c=!1;r.push(void 0),s++,n.call(t,e).then((function(e){c||(c=!0,r[l]=e,--s||o(r))}),i)})),--s||o(r)}));return r.error&&i(r.value),n.promise},race:function(e){var t=this,n=$(t),o=n.reject,i=E((function(){var i=g(t.resolve);k(e,(function(e){i.call(t,e).then(n.resolve,o)}))}));return i.error&&o(i.value),n.promise}})},function(e,t,n){var o=n(6),i=n(138),r=n(12),a=n(49),s=n(105),l=n(139),c=function(e,t){this.stopped=e,this.result=t};(e.exports=function(e,t,n,d,u){var h,f,p,m,w,g,y,v=a(t,n,d?2:1);if(u)h=e;else{if("function"!=typeof(f=s(e)))throw TypeError("Target is not iterable");if(i(f)){for(p=0,m=r(e.length);m>p;p++)if((w=d?v(o(y=e[p])[0],y[1]):v(e[p]))&&w instanceof c)return w;return new c(!1)}h=f.call(e)}for(g=h.next;!(y=g.call(h)).done;)if("object"==typeof(w=l(h,v,y.value,d))&&w&&w instanceof c)return w;return new c(!1)}).stop=function(e){return new c(!0,e)}},function(e,t,n){var o,i,r,a,s,l,c,d,u=n(2),h=n(22).f,f=n(18),p=n(141).set,m=n(142),w=u.MutationObserver||u.WebKitMutationObserver,g=u.process,y=u.Promise,v="process"==f(g),b=h(u,"queueMicrotask"),k=b&&b.value;k||(o=function(){var e,t;for(v&&(e=g.domain)&&e.exit();i;){t=i.fn,i=i.next;try{t()}catch(e){throw i?a():r=void 0,e}}r=void 0,e&&e.enter()},v?a=function(){g.nextTick(o)}:w&&!m?(s=!0,l=document.createTextNode(""),new w(o).observe(l,{characterData:!0}),a=function(){l.data=s=!s}):y&&y.resolve?(c=y.resolve(void 0),d=c.then,a=function(){d.call(c,o)}):a=function(){p.call(u,o)}),e.exports=k||function(e){var t={fn:e,next:void 0};r&&(r.next=t),i||(i=t,a()),r=t}},function(e,t,n){var o=n(2);e.exports=function(e,t){var n=o.console;n&&n.error&&(1===arguments.length?n.error(e):n.error(e,t))}},function(e,t){e.exports=function(e){try{return{error:!1,value:e()}}catch(e){return{error:!0,value:e}}}},function(e,t,n){var o=n(0),i=n(192);o({target:"Object",stat:!0,forced:Object.assign!==i},{assign:i})},function(e,t,n){"use strict";var o=n(0),i=n(25),r=n(137),a=n(1),s=n(21),l=n(89),c=n(143),d=n(11);o({target:"Promise",proto:!0,real:!0,forced:!!r&&a((function(){r.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(e){var t=l(this,s("Promise")),n="function"==typeof e;return this.then(n?function(n){return c(t,e()).then((function(){return n}))}:e,n?function(n){return c(t,e()).then((function(){throw n}))}:e)}}),i||"function"!=typeof r||r.prototype.finally||d(r.prototype,"finally",s("Promise").prototype.finally)},function(e,t,n){"use strict";var o=n(88),i=n(114);e.exports=o?{}.toString:function(){return"[object "+i(this)+"]"}},function(e,t,n){"use strict";var o=n(0),i=n(212).left,r=n(34),a=n(16),s=r("reduce"),l=a("reduce",{1:0});o({target:"Array",proto:!0,forced:!s||!l},{reduce:function(e){return i(this,e,arguments.length,arguments.length>1?arguments[1]:void 0)}})},function(e,t,n){var o=n(26),i=n(14),r=n(38),a=n(12),s=function(e){return function(t,n,s,l){o(n);var c=i(t),d=r(c),u=a(c.length),h=e?u-1:0,f=e?-1:1;if(s<2)for(;;){if(h in d){l=d[h],h+=f;break}if(h+=f,e?h<0:u<=h)throw TypeError("Reduce of empty array with no initial value")}for(;e?h>=0:u>h;h+=f)h in d&&(l=n(l,d[h],h,c));return l}};e.exports={left:s(!1),right:s(!0)}},function(e,t,n){var o=n(0),i=n(147),r=n(1),a=n(4),s=n(214).onFreeze,l=Object.freeze;o({target:"Object",stat:!0,forced:r((function(){l(1)})),sham:!i},{freeze:function(e){return l&&a(e)?l(s(e)):e}})},function(e,t,n){var o=n(40),i=n(4),r=n(7),a=n(9).f,s=n(56),l=n(147),c=s("meta"),d=0,u=Object.isExtensible||function(){return!0},h=function(e){a(e,c,{value:{objectID:"O"+ ++d,weakData:{}}})},f=e.exports={REQUIRED:!1,fastKey:function(e,t){if(!i(e))return"symbol"==typeof e?e:("string"==typeof e?"S":"P")+e;if(!r(e,c)){if(!u(e))return"F";if(!t)return"E";h(e)}return e[c].objectID},getWeakData:function(e,t){if(!r(e,c)){if(!u(e))return!0;if(!t)return!1;h(e)}return e[c].weakData},onFreeze:function(e){return l&&f.REQUIRED&&u(e)&&!r(e,c)&&h(e),e}};o[c]=!0},function(e,t,n){"use strict";var o,i=n(0),r=n(22).f,a=n(12),s=n(150),l=n(20),c=n(151),d=n(25),u="".startsWith,h=Math.min,f=c("startsWith");i({target:"String",proto:!0,forced:!!(d||f||(o=r(String.prototype,"startsWith"),!o||o.writable))&&!f},{startsWith:function(e){var t=String(l(this));s(e);var n=a(h(arguments.length>1?arguments[1]:void 0,t.length)),o=String(e);return u?u.call(t,o,n):t.slice(n,n+o.length)===o}})},function(e,t,n){var o=n(13),i=n(54).f,r={}.toString,a="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];e.exports.f=function(e){return a&&"[object Window]"==r.call(e)?function(e){try{return i(e)}catch(e){return a.slice()}}(e):i(o(e))}},function(e,t,n){var o=n(156),i=n(218);e.exports=function e(t,n,r,a,s){var l=-1,c=t.length;for(r||(r=i),s||(s=[]);++l<c;){var d=t[l];n>0&&r(d)?n>1?e(d,n-1,r,a,s):o(s,d):a||(s[s.length]=d)}return s}},function(e,t,n){var o=n(43),i=n(93),r=n(17),a=o?o.isConcatSpreadable:void 0;e.exports=function(e){return r(e)||i(e)||!!(a&&e&&e[a])}},function(e,t,n){var o=n(37),i=n(31);e.exports=function(e){return i(e)&&"[object Arguments]"==o(e)}},function(e,t,n){var o=n(43),i=Object.prototype,r=i.hasOwnProperty,a=i.toString,s=o?o.toStringTag:void 0;e.exports=function(e){var t=r.call(e,s),n=e[s];try{e[s]=void 0;var o=!0}catch(e){}var i=a.call(e);return o&&(t?e[s]=n:delete e[s]),i}},function(e,t){var n=Object.prototype.toString;e.exports=function(e){return n.call(e)}},function(e,t,n){var o=n(223),i=n(279),r=n(101),a=n(17),s=n(290);e.exports=function(e){return"function"==typeof e?e:null==e?r:"object"==typeof e?a(e)?i(e[0],e[1]):o(e):s(e)}},function(e,t,n){var o=n(224),i=n(278),r=n(173);e.exports=function(e){var t=i(e);return 1==t.length&&t[0][2]?r(t[0][0],t[0][1]):function(n){return n===e||o(n,e,t)}}},function(e,t,n){var o=n(158),i=n(162);e.exports=function(e,t,n,r){var a=n.length,s=a,l=!r;if(null==e)return!s;for(e=Object(e);a--;){var c=n[a];if(l&&c[2]?c[1]!==e[c[0]]:!(c[0]in e))return!1}for(;++a<s;){var d=(c=n[a])[0],u=e[d],h=c[1];if(l&&c[2]){if(void 0===u&&!(d in e))return!1}else{var f=new o;if(r)var p=r(u,h,d,e,t,f);if(!(void 0===p?i(h,u,3,r,f):p))return!1}}return!0}},function(e,t){e.exports=function(){this.__data__=[],this.size=0}},function(e,t,n){var o=n(65),i=Array.prototype.splice;e.exports=function(e){var t=this.__data__,n=o(t,e);return!(n<0)&&(n==t.length-1?t.pop():i.call(t,n,1),--this.size,!0)}},function(e,t,n){var o=n(65);e.exports=function(e){var t=this.__data__,n=o(t,e);return n<0?void 0:t[n][1]}},function(e,t,n){var o=n(65);e.exports=function(e){return o(this.__data__,e)>-1}},function(e,t,n){var o=n(65);e.exports=function(e,t){var n=this.__data__,i=o(n,e);return i<0?(++this.size,n.push([e,t])):n[i][1]=t,this}},function(e,t,n){var o=n(64);e.exports=function(){this.__data__=new o,this.size=0}},function(e,t){e.exports=function(e){var t=this.__data__,n=t.delete(e);return this.size=t.size,n}},function(e,t){e.exports=function(e){return this.__data__.get(e)}},function(e,t){e.exports=function(e){return this.__data__.has(e)}},function(e,t,n){var o=n(64),i=n(94),r=n(96);e.exports=function(e,t){var n=this.__data__;if(n instanceof o){var a=n.__data__;if(!i||a.length<199)return a.push([e,t]),this.size=++n.size,this;n=this.__data__=new r(a)}return n.set(e,t),this.size=n.size,this}},function(e,t,n){var o=n(160),i=n(236),r=n(95),a=n(161),s=/^\[object .+?Constructor\]$/,l=Function.prototype,c=Object.prototype,d=l.toString,u=c.hasOwnProperty,h=RegExp("^"+d.call(u).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!r(e)||i(e))&&(o(e)?h:s).test(a(e))}},function(e,t,n){var o,i=n(237),r=(o=/[^.]+$/.exec(i&&i.keys&&i.keys.IE_PROTO||""))?"Symbol(src)_1."+o:"";e.exports=function(e){return!!r&&r in e}},function(e,t,n){var o=n(19)["__core-js_shared__"];e.exports=o},function(e,t){e.exports=function(e,t){return null==e?void 0:e[t]}},function(e,t,n){var o=n(240),i=n(64),r=n(94);e.exports=function(){this.size=0,this.__data__={hash:new o,map:new(r||i),string:new o}}},function(e,t,n){var o=n(241),i=n(242),r=n(243),a=n(244),s=n(245);function l(e){var t=-1,n=null==e?0:e.length;for(this.clear();++t<n;){var o=e[t];this.set(o[0],o[1])}}l.prototype.clear=o,l.prototype.delete=i,l.prototype.get=r,l.prototype.has=a,l.prototype.set=s,e.exports=l},function(e,t,n){var o=n(66);e.exports=function(){this.__data__=o?o(null):{},this.size=0}},function(e,t){e.exports=function(e){var t=this.has(e)&&delete this.__data__[e];return this.size-=t?1:0,t}},function(e,t,n){var o=n(66),i=Object.prototype.hasOwnProperty;e.exports=function(e){var t=this.__data__;if(o){var n=t[e];return"__lodash_hash_undefined__"===n?void 0:n}return i.call(t,e)?t[e]:void 0}},function(e,t,n){var o=n(66),i=Object.prototype.hasOwnProperty;e.exports=function(e){var t=this.__data__;return o?void 0!==t[e]:i.call(t,e)}},function(e,t,n){var o=n(66);e.exports=function(e,t){var n=this.__data__;return this.size+=this.has(e)?0:1,n[e]=o&&void 0===t?"__lodash_hash_undefined__":t,this}},function(e,t,n){var o=n(67);e.exports=function(e){var t=o(this,e).delete(e);return this.size-=t?1:0,t}},function(e,t){e.exports=function(e){var t=typeof e;return"string"==t||"number"==t||"symbol"==t||"boolean"==t?"__proto__"!==e:null===e}},function(e,t,n){var o=n(67);e.exports=function(e){return o(this,e).get(e)}},function(e,t,n){var o=n(67);e.exports=function(e){return o(this,e).has(e)}},function(e,t,n){var o=n(67);e.exports=function(e,t){var n=o(this,e),i=n.size;return n.set(e,t),this.size+=n.size==i?0:1,this}},function(e,t,n){var o=n(158),i=n(163),r=n(255),a=n(258),s=n(274),l=n(17),c=n(167),d=n(169),u="[object Object]",h=Object.prototype.hasOwnProperty;e.exports=function(e,t,n,f,p,m){var w=l(e),g=l(t),y=w?"[object Array]":s(e),v=g?"[object Array]":s(t),b=(y="[object Arguments]"==y?u:y)==u,k=(v="[object Arguments]"==v?u:v)==u,x=y==v;if(x&&c(e)){if(!c(t))return!1;w=!0,b=!1}if(x&&!b)return m||(m=new o),w||d(e)?i(e,t,n,f,p,m):r(e,t,y,n,f,p,m);if(!(1&n)){var S=b&&h.call(e,"__wrapped__"),C=k&&h.call(t,"__wrapped__");if(S||C){var T=S?e.value():e,I=C?t.value():t;return m||(m=new o),p(T,I,n,f,m)}}return!!x&&(m||(m=new o),a(e,t,n,f,p,m))}},function(e,t){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,t){e.exports=function(e){return this.__data__.has(e)}},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length;++n<o;)if(t(e[n],n,e))return!0;return!1}},function(e,t,n){var o=n(43),i=n(256),r=n(159),a=n(163),s=n(257),l=n(97),c=o?o.prototype:void 0,d=c?c.valueOf:void 0;e.exports=function(e,t,n,o,c,u,h){switch(n){case"[object DataView]":if(e.byteLength!=t.byteLength||e.byteOffset!=t.byteOffset)return!1;e=e.buffer,t=t.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=t.byteLength||!u(new i(e),new i(t)));case"[object Boolean]":case"[object Date]":case"[object Number]":return r(+e,+t);case"[object Error]":return e.name==t.name&&e.message==t.message;case"[object RegExp]":case"[object String]":return e==t+"";case"[object Map]":var f=s;case"[object Set]":var p=1&o;if(f||(f=l),e.size!=t.size&&!p)return!1;var m=h.get(e);if(m)return m==t;o|=2,h.set(e,t);var w=a(f(e),f(t),o,c,u,h);return h.delete(e),w;case"[object Symbol]":if(d)return d.call(e)==d.call(t)}return!1}},function(e,t,n){var o=n(19).Uint8Array;e.exports=o},function(e,t){e.exports=function(e){var t=-1,n=Array(e.size);return e.forEach((function(e,o){n[++t]=[o,e]})),n}},function(e,t,n){var o=n(259),i=Object.prototype.hasOwnProperty;e.exports=function(e,t,n,r,a,s){var l=1&n,c=o(e),d=c.length;if(d!=o(t).length&&!l)return!1;for(var u=d;u--;){var h=c[u];if(!(l?h in t:i.call(t,h)))return!1}var f=s.get(e),p=s.get(t);if(f&&p)return f==t&&p==e;var m=!0;s.set(e,t),s.set(t,e);for(var w=l;++u<d;){var g=e[h=c[u]],y=t[h];if(r)var v=l?r(y,g,h,t,e,s):r(g,y,h,e,t,s);if(!(void 0===v?g===y||a(g,y,n,r,s):v)){m=!1;break}w||(w="constructor"==h)}if(m&&!w){var b=e.constructor,k=t.constructor;b==k||!("constructor"in e)||!("constructor"in t)||"function"==typeof b&&b instanceof b&&"function"==typeof k&&k instanceof k||(m=!1)}return s.delete(e),s.delete(t),m}},function(e,t,n){var o=n(260),i=n(261),r=n(166);e.exports=function(e){return o(e,r,i)}},function(e,t,n){var o=n(156),i=n(17);e.exports=function(e,t,n){var r=t(e);return i(e)?r:o(r,n(e))}},function(e,t,n){var o=n(262),i=n(263),r=Object.prototype.propertyIsEnumerable,a=Object.getOwnPropertySymbols,s=a?function(e){return null==e?[]:(e=Object(e),o(a(e),(function(t){return r.call(e,t)})))}:i;e.exports=s},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length,i=0,r=[];++n<o;){var a=e[n];t(a,n,e)&&(r[i++]=a)}return r}},function(e,t){e.exports=function(){return[]}},function(e,t,n){var o=n(265),i=n(93),r=n(17),a=n(167),s=n(168),l=n(169),c=Object.prototype.hasOwnProperty;e.exports=function(e,t){var n=r(e),d=!n&&i(e),u=!n&&!d&&a(e),h=!n&&!d&&!u&&l(e),f=n||d||u||h,p=f?o(e.length,String):[],m=p.length;for(var w in e)!t&&!c.call(e,w)||f&&("length"==w||u&&("offset"==w||"parent"==w)||h&&("buffer"==w||"byteLength"==w||"byteOffset"==w)||s(w,m))||p.push(w);return p}},function(e,t){e.exports=function(e,t){for(var n=-1,o=Array(e);++n<e;)o[n]=t(n);return o}},function(e,t){e.exports=function(){return!1}},function(e,t,n){var o=n(37),i=n(98),r=n(31),a={};a["[object Float32Array]"]=a["[object Float64Array]"]=a["[object Int8Array]"]=a["[object Int16Array]"]=a["[object Int32Array]"]=a["[object Uint8Array]"]=a["[object Uint8ClampedArray]"]=a["[object Uint16Array]"]=a["[object Uint32Array]"]=!0,a["[object Arguments]"]=a["[object Array]"]=a["[object ArrayBuffer]"]=a["[object Boolean]"]=a["[object DataView]"]=a["[object Date]"]=a["[object Error]"]=a["[object Function]"]=a["[object Map]"]=a["[object Number]"]=a["[object Object]"]=a["[object RegExp]"]=a["[object Set]"]=a["[object String]"]=a["[object WeakMap]"]=!1,e.exports=function(e){return r(e)&&i(e.length)&&!!a[o(e)]}},function(e,t){e.exports=function(e){return function(t){return e(t)}}},function(e,t,n){(function(e){var o=n(157),i=t&&!t.nodeType&&t,r=i&&"object"==typeof e&&e&&!e.nodeType&&e,a=r&&r.exports===i&&o.process,s=function(){try{var e=r&&r.require&&r.require("util").types;return e||a&&a.binding&&a.binding("util")}catch(e){}}();e.exports=s}).call(this,n(121)(e))},function(e,t,n){var o=n(271),i=n(272),r=Object.prototype.hasOwnProperty;e.exports=function(e){if(!o(e))return i(e);var t=[];for(var n in Object(e))r.call(e,n)&&"constructor"!=n&&t.push(n);return t}},function(e,t){var n=Object.prototype;e.exports=function(e){var t=e&&e.constructor;return e===("function"==typeof t&&t.prototype||n)}},function(e,t,n){var o=n(273)(Object.keys,Object);e.exports=o},function(e,t){e.exports=function(e,t){return function(n){return e(t(n))}}},function(e,t,n){var o=n(275),i=n(94),r=n(276),a=n(171),s=n(277),l=n(37),c=n(161),d=c(o),u=c(i),h=c(r),f=c(a),p=c(s),m=l;(o&&"[object DataView]"!=m(new o(new ArrayBuffer(1)))||i&&"[object Map]"!=m(new i)||r&&"[object Promise]"!=m(r.resolve())||a&&"[object Set]"!=m(new a)||s&&"[object WeakMap]"!=m(new s))&&(m=function(e){var t=l(e),n="[object Object]"==t?e.constructor:void 0,o=n?c(n):"";if(o)switch(o){case d:return"[object DataView]";case u:return"[object Map]";case h:return"[object Promise]";case f:return"[object Set]";case p:return"[object WeakMap]"}return t}),e.exports=m},function(e,t,n){var o=n(27)(n(19),"DataView");e.exports=o},function(e,t,n){var o=n(27)(n(19),"Promise");e.exports=o},function(e,t,n){var o=n(27)(n(19),"WeakMap");e.exports=o},function(e,t,n){var o=n(172),i=n(166);e.exports=function(e){for(var t=i(e),n=t.length;n--;){var r=t[n],a=e[r];t[n]=[r,a,o(a)]}return t}},function(e,t,n){var o=n(162),i=n(280),r=n(287),a=n(99),s=n(172),l=n(173),c=n(68);e.exports=function(e,t){return a(e)&&s(t)?l(c(e),t):function(n){var a=i(n,e);return void 0===a&&a===t?r(n,e):o(t,a,3)}}},function(e,t,n){var o=n(174);e.exports=function(e,t,n){var i=null==e?void 0:o(e,t);return void 0===i?n:i}},function(e,t,n){var o=n(282),i=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,r=/\\(\\)?/g,a=o((function(e){var t=[];return 46===e.charCodeAt(0)&&t.push(""),e.replace(i,(function(e,n,o,i){t.push(o?i.replace(r,"$1"):n||e)})),t}));e.exports=a},function(e,t,n){var o=n(283);e.exports=function(e){var t=o(e,(function(e){return 500===n.size&&n.clear(),e})),n=t.cache;return t}},function(e,t,n){var o=n(96);function i(e,t){if("function"!=typeof e||null!=t&&"function"!=typeof t)throw new TypeError("Expected a function");var n=function(){var o=arguments,i=t?t.apply(this,o):o[0],r=n.cache;if(r.has(i))return r.get(i);var a=e.apply(this,o);return n.cache=r.set(i,a)||r,a};return n.cache=new(i.Cache||o),n}i.Cache=o,e.exports=i},function(e,t,n){var o=n(285);e.exports=function(e){return null==e?"":o(e)}},function(e,t,n){var o=n(43),i=n(286),r=n(17),a=n(100),s=o?o.prototype:void 0,l=s?s.toString:void 0;e.exports=function e(t){if("string"==typeof t)return t;if(r(t))return i(t,e)+"";if(a(t))return l?l.call(t):"";var n=t+"";return"0"==n&&1/t==-1/0?"-0":n}},function(e,t){e.exports=function(e,t){for(var n=-1,o=null==e?0:e.length,i=Array(o);++n<o;)i[n]=t(e[n],n,e);return i}},function(e,t,n){var o=n(288),i=n(289);e.exports=function(e,t){return null!=e&&i(e,t,o)}},function(e,t){e.exports=function(e,t){return null!=e&&t in Object(e)}},function(e,t,n){var o=n(175),i=n(93),r=n(17),a=n(168),s=n(98),l=n(68);e.exports=function(e,t,n){for(var c=-1,d=(t=o(t,e)).length,u=!1;++c<d;){var h=l(t[c]);if(!(u=null!=e&&n(e,h)))break;e=e[h]}return u||++c!=d?u:!!(d=null==e?0:e.length)&&s(d)&&a(h,d)&&(r(e)||i(e))}},function(e,t,n){var o=n(291),i=n(292),r=n(99),a=n(68);e.exports=function(e){return r(e)?o(a(e)):i(e)}},function(e,t){e.exports=function(e){return function(t){return null==t?void 0:t[e]}}},function(e,t,n){var o=n(174);e.exports=function(e){return function(t){return o(t,e)}}},function(e,t,n){var o=n(101),i=n(294),r=n(296);e.exports=function(e,t){return r(i(e,t,o),e+"")}},function(e,t,n){var o=n(295),i=Math.max;e.exports=function(e,t,n){return t=i(void 0===t?e.length-1:t,0),function(){for(var r=arguments,a=-1,s=i(r.length-t,0),l=Array(s);++a<s;)l[a]=r[t+a];a=-1;for(var c=Array(t+1);++a<t;)c[a]=r[a];return c[t]=n(l),o(e,this,c)}}},function(e,t){e.exports=function(e,t,n){switch(n.length){case 0:return e.call(t);case 1:return e.call(t,n[0]);case 2:return e.call(t,n[0],n[1]);case 3:return e.call(t,n[0],n[1],n[2])}return e.apply(t,n)}},function(e,t,n){var o=n(297),i=n(300)(o);e.exports=i},function(e,t,n){var o=n(298),i=n(299),r=n(101),a=i?function(e,t){return i(e,"toString",{configurable:!0,enumerable:!1,value:o(t),writable:!0})}:r;e.exports=a},function(e,t){e.exports=function(e){return function(){return e}}},function(e,t,n){var o=n(27),i=function(){try{var e=o(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=i},function(e,t){var n=Date.now;e.exports=function(e){var t=0,o=0;return function(){var i=n(),r=16-(i-o);if(o=i,r>0){if(++t>=800)return arguments[0]}else t=0;return e.apply(void 0,arguments)}}},function(e,t,n){var o=n(164),i=n(302),r=n(307),a=n(165),s=n(308),l=n(97);e.exports=function(e,t,n){var c=-1,d=i,u=e.length,h=!0,f=[],p=f;if(n)h=!1,d=r;else if(u>=200){var m=t?null:s(e);if(m)return l(m);h=!1,d=a,p=new o}else p=t?[]:f;e:for(;++c<u;){var w=e[c],g=t?t(w):w;if(w=n||0!==w?w:0,h&&g==g){for(var y=p.length;y--;)if(p[y]===g)continue e;t&&p.push(g),f.push(w)}else d(p,g,n)||(p!==f&&p.push(g),f.push(w))}return f}},function(e,t,n){var o=n(303);e.exports=function(e,t){return!!(null==e?0:e.length)&&o(e,t,0)>-1}},function(e,t,n){var o=n(304),i=n(305),r=n(306);e.exports=function(e,t,n){return t==t?r(e,t,n):o(e,i,n)}},function(e,t){e.exports=function(e,t,n,o){for(var i=e.length,r=n+(o?1:-1);o?r--:++r<i;)if(t(e[r],r,e))return r;return-1}},function(e,t){e.exports=function(e){return e!=e}},function(e,t){e.exports=function(e,t,n){for(var o=n-1,i=e.length;++o<i;)if(e[o]===t)return o;return-1}},function(e,t){e.exports=function(e,t,n){for(var o=-1,i=null==e?0:e.length;++o<i;)if(n(t,e[o]))return!0;return!1}},function(e,t,n){var o=n(171),i=n(309),r=n(97),a=o&&1/r(new o([,-0]))[1]==1/0?function(e){return new o(e)}:i;e.exports=a},function(e,t){e.exports=function(){}},function(e,t,n){var o=n(170),i=n(31);e.exports=function(e){return i(e)&&o(e)}},function(e,t,n){var o=n(0),i=n(5);o({target:"Object",stat:!0,forced:!i,sham:!i},{defineProperties:n(113)})},function(e,t,n){var o=n(0),i=n(1),r=n(13),a=n(22).f,s=n(5),l=i((function(){a(1)}));o({target:"Object",stat:!0,forced:!s||l,sham:!s},{getOwnPropertyDescriptor:function(e,t){return a(r(e),t)}})},function(e,t,n){var o=n(2),i=n(122).trim,r=n(123),a=o.parseInt,s=/^[+-]?0[Xx]/,l=8!==a(r+"08")||22!==a(r+"0x16");e.exports=l?function(e,t){var n=i(String(e));return a(n,t>>>0||(s.test(n)?16:10))}:a},function(e,t,n){"use strict";n(177)},function(e,t,n){},function(e,t,n){},function(e,t,n){},function(e,t,n){"use strict";var o=n(5),i=n(2),r=n(76),a=n(11),s=n(7),l=n(18),c=n(199),d=n(39),u=n(1),h=n(29),f=n(54).f,p=n(22).f,m=n(9).f,w=n(122).trim,g=i.Number,y=g.prototype,v="Number"==l(h(y)),b=function(e){var t,n,o,i,r,a,s,l,c=d(e,!1);if("string"==typeof c&&c.length>2)if(43===(t=(c=w(c)).charCodeAt(0))||45===t){if(88===(n=c.charCodeAt(2))||120===n)return NaN}else if(48===t){switch(c.charCodeAt(1)){case 66:case 98:o=2,i=49;break;case 79:case 111:o=8,i=55;break;default:return+c}for(a=(r=c.slice(2)).length,s=0;s<a;s++)if((l=r.charCodeAt(s))<48||l>i)return NaN;return parseInt(r,o)}return+c};if(r("Number",!g(" 0o1")||!g("0b1")||g("+0x1"))){for(var k,x=function(e){var t=arguments.length<1?0:e,n=this;return n instanceof x&&(v?u((function(){y.valueOf.call(n)})):"Number"!=l(n))?c(new g(b(t)),n,x):b(t)},S=o?f(g):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger".split(","),C=0;S.length>C;C++)s(g,k=S[C])&&!s(x,k)&&m(x,k,p(g,k));x.prototype=y,y.constructor=x,a(i,"Number",x)}},function(e,t,n){"use strict";n(178)},function(e,t,n){},function(e,t,n){"use strict";n(179)},function(e,t,n){"use strict";n(180)},function(e,t,n){"use strict";var o=n(26),i=n(4),r=[].slice,a={},s=function(e,t,n){if(!(t in a)){for(var o=[],i=0;i<t;i++)o[i]="a["+i+"]";a[t]=Function("C,a","return new C("+o.join(",")+")")}return a[t](e,n)};e.exports=Function.bind||function(e){var t=o(this),n=r.call(arguments,1),a=function(){var o=n.concat(r.call(arguments));return this instanceof a?s(t,o.length,o):t.apply(e,o)};return i(t.prototype)&&(a.prototype=t.prototype),a}},function(e,t,n){"use strict";n(183)},function(e,t,n){"use strict";n(184)},function(e,t,n){"use strict";n.r(t);n(112),n(203),n(208),n(209),n(24),n(72),n(50),n(10),n(28),n(33),n(106);var o=n(61),i=Object.freeze({});function r(e){return null==e}function a(e){return null!=e}function s(e){return!0===e}function l(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function c(e){return null!==e&&"object"==typeof e}var d=Object.prototype.toString;function u(e){return"[object Object]"===d.call(e)}function h(e){return"[object RegExp]"===d.call(e)}function f(e){var t=parseFloat(String(e));return t>=0&&Math.floor(t)===t&&isFinite(e)}function p(e){return a(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function m(e){return null==e?"":Array.isArray(e)||u(e)&&e.toString===d?JSON.stringify(e,null,2):String(e)}function w(e){var t=parseFloat(e);return isNaN(t)?e:t}function g(e,t){for(var n=Object.create(null),o=e.split(","),i=0;i<o.length;i++)n[o[i]]=!0;return t?function(e){return n[e.toLowerCase()]}:function(e){return n[e]}}g("slot,component",!0);var y=g("key,ref,slot,slot-scope,is");function v(e,t){if(e.length){var n=e.indexOf(t);if(n>-1)return e.splice(n,1)}}var b=Object.prototype.hasOwnProperty;function k(e,t){return b.call(e,t)}function x(e){var t=Object.create(null);return function(n){return t[n]||(t[n]=e(n))}}var S=/-(\w)/g,C=x((function(e){return e.replace(S,(function(e,t){return t?t.toUpperCase():""}))})),T=x((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),I=/\B([A-Z])/g,A=x((function(e){return e.replace(I,"-$1").toLowerCase()}));var _=Function.prototype.bind?function(e,t){return e.bind(t)}:function(e,t){function n(n){var o=arguments.length;return o?o>1?e.apply(t,arguments):e.call(t,n):e.call(t)}return n._length=e.length,n};function E(e,t){t=t||0;for(var n=e.length-t,o=new Array(n);n--;)o[n]=e[n+t];return o}function j(e,t){for(var n in t)e[n]=t[n];return e}function O(e){for(var t={},n=0;n<e.length;n++)e[n]&&j(t,e[n]);return t}function W(e,t,n){}var P=function(e,t,n){return!1},q=function(e){return e};function R(e,t){if(e===t)return!0;var n=c(e),o=c(t);if(!n||!o)return!n&&!o&&String(e)===String(t);try{var i=Array.isArray(e),r=Array.isArray(t);if(i&&r)return e.length===t.length&&e.every((function(e,n){return R(e,t[n])}));if(e instanceof Date&&t instanceof Date)return e.getTime()===t.getTime();if(i||r)return!1;var a=Object.keys(e),s=Object.keys(t);return a.length===s.length&&a.every((function(n){return R(e[n],t[n])}))}catch(e){return!1}}function D(e,t){for(var n=0;n<e.length;n++)if(R(e[n],t))return n;return-1}function N(e){var t=!1;return function(){t||(t=!0,e.apply(this,arguments))}}var z=["component","directive","filter"],F=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],L={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:P,isReservedAttr:P,isUnknownElement:P,getTagNamespace:W,parsePlatformTagName:q,mustUseProp:P,async:!0,_lifecycleHooks:F},M=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function G(e,t,n,o){Object.defineProperty(e,t,{value:n,enumerable:!!o,writable:!0,configurable:!0})}var H=new RegExp("[^"+M.source+".$_\\d]");var $,U="__proto__"in{},B="undefined"!=typeof window,V="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,Q=V&&WXEnvironment.platform.toLowerCase(),Y=B&&window.navigator.userAgent.toLowerCase(),K=Y&&/msie|trident/.test(Y),J=Y&&Y.indexOf("msie 9.0")>0,X=Y&&Y.indexOf("edge/")>0,Z=(Y&&Y.indexOf("android"),Y&&/iphone|ipad|ipod|ios/.test(Y)||"ios"===Q),ee=(Y&&/chrome\/\d+/.test(Y),Y&&/phantomjs/.test(Y),Y&&Y.match(/firefox\/(\d+)/)),te={}.watch,ne=!1;if(B)try{var oe={};Object.defineProperty(oe,"passive",{get:function(){ne=!0}}),window.addEventListener("test-passive",null,oe)}catch(e){}var ie=function(){return void 0===$&&($=!B&&!V&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),$},re=B&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function ae(e){return"function"==typeof e&&/native code/.test(e.toString())}var se,le="undefined"!=typeof Symbol&&ae(Symbol)&&"undefined"!=typeof Reflect&&ae(Reflect.ownKeys);se="undefined"!=typeof Set&&ae(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var ce=W,de=0,ue=function(){this.id=de++,this.subs=[]};ue.prototype.addSub=function(e){this.subs.push(e)},ue.prototype.removeSub=function(e){v(this.subs,e)},ue.prototype.depend=function(){ue.target&&ue.target.addDep(this)},ue.prototype.notify=function(){var e=this.subs.slice();for(var t=0,n=e.length;t<n;t++)e[t].update()},ue.target=null;var he=[];function fe(e){he.push(e),ue.target=e}function pe(){he.pop(),ue.target=he[he.length-1]}var me=function(e,t,n,o,i,r,a,s){this.tag=e,this.data=t,this.children=n,this.text=o,this.elm=i,this.ns=void 0,this.context=r,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=t&&t.key,this.componentOptions=a,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=s,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},we={child:{configurable:!0}};we.child.get=function(){return this.componentInstance},Object.defineProperties(me.prototype,we);var ge=function(e){void 0===e&&(e="");var t=new me;return t.text=e,t.isComment=!0,t};function ye(e){return new me(void 0,void 0,void 0,String(e))}function ve(e){var t=new me(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return t.ns=e.ns,t.isStatic=e.isStatic,t.key=e.key,t.isComment=e.isComment,t.fnContext=e.fnContext,t.fnOptions=e.fnOptions,t.fnScopeId=e.fnScopeId,t.asyncMeta=e.asyncMeta,t.isCloned=!0,t}var be=Array.prototype,ke=Object.create(be);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var t=be[e];G(ke,e,(function(){for(var n=[],o=arguments.length;o--;)n[o]=arguments[o];var i,r=t.apply(this,n),a=this.__ob__;switch(e){case"push":case"unshift":i=n;break;case"splice":i=n.slice(2)}return i&&a.observeArray(i),a.dep.notify(),r}))}));var xe=Object.getOwnPropertyNames(ke),Se=!0;function Ce(e){Se=e}var Te=function(e){this.value=e,this.dep=new ue,this.vmCount=0,G(e,"__ob__",this),Array.isArray(e)?(U?function(e,t){e.__proto__=t}(e,ke):function(e,t,n){for(var o=0,i=n.length;o<i;o++){var r=n[o];G(e,r,t[r])}}(e,ke,xe),this.observeArray(e)):this.walk(e)};function Ie(e,t){var n;if(c(e)&&!(e instanceof me))return k(e,"__ob__")&&e.__ob__ instanceof Te?n=e.__ob__:Se&&!ie()&&(Array.isArray(e)||u(e))&&Object.isExtensible(e)&&!e._isVue&&(n=new Te(e)),t&&n&&n.vmCount++,n}function Ae(e,t,n,o,i){var r=new ue,a=Object.getOwnPropertyDescriptor(e,t);if(!a||!1!==a.configurable){var s=a&&a.get,l=a&&a.set;s&&!l||2!==arguments.length||(n=e[t]);var c=!i&&Ie(n);Object.defineProperty(e,t,{enumerable:!0,configurable:!0,get:function(){var t=s?s.call(e):n;return ue.target&&(r.depend(),c&&(c.dep.depend(),Array.isArray(t)&&je(t))),t},set:function(t){var o=s?s.call(e):n;t===o||t!=t&&o!=o||s&&!l||(l?l.call(e,t):n=t,c=!i&&Ie(t),r.notify())}})}}function _e(e,t,n){if(Array.isArray(e)&&f(t))return e.length=Math.max(e.length,t),e.splice(t,1,n),n;if(t in e&&!(t in Object.prototype))return e[t]=n,n;var o=e.__ob__;return e._isVue||o&&o.vmCount?n:o?(Ae(o.value,t,n),o.dep.notify(),n):(e[t]=n,n)}function Ee(e,t){if(Array.isArray(e)&&f(t))e.splice(t,1);else{var n=e.__ob__;e._isVue||n&&n.vmCount||k(e,t)&&(delete e[t],n&&n.dep.notify())}}function je(e){for(var t=void 0,n=0,o=e.length;n<o;n++)(t=e[n])&&t.__ob__&&t.__ob__.dep.depend(),Array.isArray(t)&&je(t)}Te.prototype.walk=function(e){for(var t=Object.keys(e),n=0;n<t.length;n++)Ae(e,t[n])},Te.prototype.observeArray=function(e){for(var t=0,n=e.length;t<n;t++)Ie(e[t])};var Oe=L.optionMergeStrategies;function We(e,t){if(!t)return e;for(var n,o,i,r=le?Reflect.ownKeys(t):Object.keys(t),a=0;a<r.length;a++)"__ob__"!==(n=r[a])&&(o=e[n],i=t[n],k(e,n)?o!==i&&u(o)&&u(i)&&We(o,i):_e(e,n,i));return e}function Pe(e,t,n){return n?function(){var o="function"==typeof t?t.call(n,n):t,i="function"==typeof e?e.call(n,n):e;return o?We(o,i):i}:t?e?function(){return We("function"==typeof t?t.call(this,this):t,"function"==typeof e?e.call(this,this):e)}:t:e}function qe(e,t){var n=t?e?e.concat(t):Array.isArray(t)?t:[t]:e;return n?function(e){for(var t=[],n=0;n<e.length;n++)-1===t.indexOf(e[n])&&t.push(e[n]);return t}(n):n}function Re(e,t,n,o){var i=Object.create(e||null);return t?j(i,t):i}Oe.data=function(e,t,n){return n?Pe(e,t,n):t&&"function"!=typeof t?e:Pe(e,t)},F.forEach((function(e){Oe[e]=qe})),z.forEach((function(e){Oe[e+"s"]=Re})),Oe.watch=function(e,t,n,o){if(e===te&&(e=void 0),t===te&&(t=void 0),!t)return Object.create(e||null);if(!e)return t;var i={};for(var r in j(i,e),t){var a=i[r],s=t[r];a&&!Array.isArray(a)&&(a=[a]),i[r]=a?a.concat(s):Array.isArray(s)?s:[s]}return i},Oe.props=Oe.methods=Oe.inject=Oe.computed=function(e,t,n,o){if(!e)return t;var i=Object.create(null);return j(i,e),t&&j(i,t),i},Oe.provide=Pe;var De=function(e,t){return void 0===t?e:t};function Ne(e,t,n){if("function"==typeof t&&(t=t.options),function(e,t){var n=e.props;if(n){var o,i,r={};if(Array.isArray(n))for(o=n.length;o--;)"string"==typeof(i=n[o])&&(r[C(i)]={type:null});else if(u(n))for(var a in n)i=n[a],r[C(a)]=u(i)?i:{type:i};else 0;e.props=r}}(t),function(e,t){var n=e.inject;if(n){var o=e.inject={};if(Array.isArray(n))for(var i=0;i<n.length;i++)o[n[i]]={from:n[i]};else if(u(n))for(var r in n){var a=n[r];o[r]=u(a)?j({from:r},a):{from:a}}else 0}}(t),function(e){var t=e.directives;if(t)for(var n in t){var o=t[n];"function"==typeof o&&(t[n]={bind:o,update:o})}}(t),!t._base&&(t.extends&&(e=Ne(e,t.extends,n)),t.mixins))for(var o=0,i=t.mixins.length;o<i;o++)e=Ne(e,t.mixins[o],n);var r,a={};for(r in e)s(r);for(r in t)k(e,r)||s(r);function s(o){var i=Oe[o]||De;a[o]=i(e[o],t[o],n,o)}return a}function ze(e,t,n,o){if("string"==typeof n){var i=e[t];if(k(i,n))return i[n];var r=C(n);if(k(i,r))return i[r];var a=T(r);return k(i,a)?i[a]:i[n]||i[r]||i[a]}}function Fe(e,t,n,o){var i=t[e],r=!k(n,e),a=n[e],s=Ge(Boolean,i.type);if(s>-1)if(r&&!k(i,"default"))a=!1;else if(""===a||a===A(e)){var l=Ge(String,i.type);(l<0||s<l)&&(a=!0)}if(void 0===a){a=function(e,t,n){if(!k(t,"default"))return;var o=t.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[n]&&void 0!==e._props[n])return e._props[n];return"function"==typeof o&&"Function"!==Le(t.type)?o.call(e):o}(o,i,e);var c=Se;Ce(!0),Ie(a),Ce(c)}return a}function Le(e){var t=e&&e.toString().match(/^\s*function (\w+)/);return t?t[1]:""}function Me(e,t){return Le(e)===Le(t)}function Ge(e,t){if(!Array.isArray(t))return Me(t,e)?0:-1;for(var n=0,o=t.length;n<o;n++)if(Me(t[n],e))return n;return-1}function He(e,t,n){fe();try{if(t)for(var o=t;o=o.$parent;){var i=o.$options.errorCaptured;if(i)for(var r=0;r<i.length;r++)try{if(!1===i[r].call(o,e,t,n))return}catch(e){Ue(e,o,"errorCaptured hook")}}Ue(e,t,n)}finally{pe()}}function $e(e,t,n,o,i){var r;try{(r=n?e.apply(t,n):e.call(t))&&!r._isVue&&p(r)&&!r._handled&&(r.catch((function(e){return He(e,o,i+" (Promise/async)")})),r._handled=!0)}catch(e){He(e,o,i)}return r}function Ue(e,t,n){if(L.errorHandler)try{return L.errorHandler.call(null,e,t,n)}catch(t){t!==e&&Be(t,null,"config.errorHandler")}Be(e,t,n)}function Be(e,t,n){if(!B&&!V||"undefined"==typeof console)throw e;console.error(e)}var Ve,Qe=!1,Ye=[],Ke=!1;function Je(){Ke=!1;var e=Ye.slice(0);Ye.length=0;for(var t=0;t<e.length;t++)e[t]()}if("undefined"!=typeof Promise&&ae(Promise)){var Xe=Promise.resolve();Ve=function(){Xe.then(Je),Z&&setTimeout(W)},Qe=!0}else if(K||"undefined"==typeof MutationObserver||!ae(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())Ve="undefined"!=typeof setImmediate&&ae(setImmediate)?function(){setImmediate(Je)}:function(){setTimeout(Je,0)};else{var Ze=1,et=new MutationObserver(Je),tt=document.createTextNode(String(Ze));et.observe(tt,{characterData:!0}),Ve=function(){Ze=(Ze+1)%2,tt.data=String(Ze)},Qe=!0}function nt(e,t){var n;if(Ye.push((function(){if(e)try{e.call(t)}catch(e){He(e,t,"nextTick")}else n&&n(t)})),Ke||(Ke=!0,Ve()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){n=e}))}var ot=new se;function it(e){!function e(t,n){var o,i,r=Array.isArray(t);if(!r&&!c(t)||Object.isFrozen(t)||t instanceof me)return;if(t.__ob__){var a=t.__ob__.dep.id;if(n.has(a))return;n.add(a)}if(r)for(o=t.length;o--;)e(t[o],n);else for(i=Object.keys(t),o=i.length;o--;)e(t[i[o]],n)}(e,ot),ot.clear()}var rt=x((function(e){var t="&"===e.charAt(0),n="~"===(e=t?e.slice(1):e).charAt(0),o="!"===(e=n?e.slice(1):e).charAt(0);return{name:e=o?e.slice(1):e,once:n,capture:o,passive:t}}));function at(e,t){function n(){var e=arguments,o=n.fns;if(!Array.isArray(o))return $e(o,null,arguments,t,"v-on handler");for(var i=o.slice(),r=0;r<i.length;r++)$e(i[r],null,e,t,"v-on handler")}return n.fns=e,n}function st(e,t,n,o,i,a){var l,c,d,u;for(l in e)c=e[l],d=t[l],u=rt(l),r(c)||(r(d)?(r(c.fns)&&(c=e[l]=at(c,a)),s(u.once)&&(c=e[l]=i(u.name,c,u.capture)),n(u.name,c,u.capture,u.passive,u.params)):c!==d&&(d.fns=c,e[l]=d));for(l in t)r(e[l])&&o((u=rt(l)).name,t[l],u.capture)}function lt(e,t,n){var o;e instanceof me&&(e=e.data.hook||(e.data.hook={}));var i=e[t];function l(){n.apply(this,arguments),v(o.fns,l)}r(i)?o=at([l]):a(i.fns)&&s(i.merged)?(o=i).fns.push(l):o=at([i,l]),o.merged=!0,e[t]=o}function ct(e,t,n,o,i){if(a(t)){if(k(t,n))return e[n]=t[n],i||delete t[n],!0;if(k(t,o))return e[n]=t[o],i||delete t[o],!0}return!1}function dt(e){return l(e)?[ye(e)]:Array.isArray(e)?function e(t,n){var o,i,c,d,u=[];for(o=0;o<t.length;o++)r(i=t[o])||"boolean"==typeof i||(c=u.length-1,d=u[c],Array.isArray(i)?i.length>0&&(ut((i=e(i,(n||"")+"_"+o))[0])&&ut(d)&&(u[c]=ye(d.text+i[0].text),i.shift()),u.push.apply(u,i)):l(i)?ut(d)?u[c]=ye(d.text+i):""!==i&&u.push(ye(i)):ut(i)&&ut(d)?u[c]=ye(d.text+i.text):(s(t._isVList)&&a(i.tag)&&r(i.key)&&a(n)&&(i.key="__vlist"+n+"_"+o+"__"),u.push(i)));return u}(e):void 0}function ut(e){return a(e)&&a(e.text)&&!1===e.isComment}function ht(e,t){if(e){for(var n=Object.create(null),o=le?Reflect.ownKeys(e):Object.keys(e),i=0;i<o.length;i++){var r=o[i];if("__ob__"!==r){for(var a=e[r].from,s=t;s;){if(s._provided&&k(s._provided,a)){n[r]=s._provided[a];break}s=s.$parent}if(!s)if("default"in e[r]){var l=e[r].default;n[r]="function"==typeof l?l.call(t):l}else 0}}return n}}function ft(e,t){if(!e||!e.length)return{};for(var n={},o=0,i=e.length;o<i;o++){var r=e[o],a=r.data;if(a&&a.attrs&&a.attrs.slot&&delete a.attrs.slot,r.context!==t&&r.fnContext!==t||!a||null==a.slot)(n.default||(n.default=[])).push(r);else{var s=a.slot,l=n[s]||(n[s]=[]);"template"===r.tag?l.push.apply(l,r.children||[]):l.push(r)}}for(var c in n)n[c].every(pt)&&delete n[c];return n}function pt(e){return e.isComment&&!e.asyncFactory||" "===e.text}function mt(e,t,n){var o,r=Object.keys(t).length>0,a=e?!!e.$stable:!r,s=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(a&&n&&n!==i&&s===n.$key&&!r&&!n.$hasNormal)return n;for(var l in o={},e)e[l]&&"$"!==l[0]&&(o[l]=wt(t,l,e[l]))}else o={};for(var c in t)c in o||(o[c]=gt(t,c));return e&&Object.isExtensible(e)&&(e._normalized=o),G(o,"$stable",a),G(o,"$key",s),G(o,"$hasNormal",r),o}function wt(e,t,n){var o=function(){var e=arguments.length?n.apply(null,arguments):n({});return(e=e&&"object"==typeof e&&!Array.isArray(e)?[e]:dt(e))&&(0===e.length||1===e.length&&e[0].isComment)?void 0:e};return n.proxy&&Object.defineProperty(e,t,{get:o,enumerable:!0,configurable:!0}),o}function gt(e,t){return function(){return e[t]}}function yt(e,t){var n,o,i,r,s;if(Array.isArray(e)||"string"==typeof e)for(n=new Array(e.length),o=0,i=e.length;o<i;o++)n[o]=t(e[o],o);else if("number"==typeof e)for(n=new Array(e),o=0;o<e;o++)n[o]=t(o+1,o);else if(c(e))if(le&&e[Symbol.iterator]){n=[];for(var l=e[Symbol.iterator](),d=l.next();!d.done;)n.push(t(d.value,n.length)),d=l.next()}else for(r=Object.keys(e),n=new Array(r.length),o=0,i=r.length;o<i;o++)s=r[o],n[o]=t(e[s],s,o);return a(n)||(n=[]),n._isVList=!0,n}function vt(e,t,n,o){var i,r=this.$scopedSlots[e];r?(n=n||{},o&&(n=j(j({},o),n)),i=r(n)||t):i=this.$slots[e]||t;var a=n&&n.slot;return a?this.$createElement("template",{slot:a},i):i}function bt(e){return ze(this.$options,"filters",e)||q}function kt(e,t){return Array.isArray(e)?-1===e.indexOf(t):e!==t}function xt(e,t,n,o,i){var r=L.keyCodes[t]||n;return i&&o&&!L.keyCodes[t]?kt(i,o):r?kt(r,e):o?A(o)!==t:void 0}function St(e,t,n,o,i){if(n)if(c(n)){var r;Array.isArray(n)&&(n=O(n));var a=function(a){if("class"===a||"style"===a||y(a))r=e;else{var s=e.attrs&&e.attrs.type;r=o||L.mustUseProp(t,s,a)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var l=C(a),c=A(a);l in r||c in r||(r[a]=n[a],i&&((e.on||(e.on={}))["update:"+a]=function(e){n[a]=e}))};for(var s in n)a(s)}else;return e}function Ct(e,t){var n=this._staticTrees||(this._staticTrees=[]),o=n[e];return o&&!t||It(o=n[e]=this.$options.staticRenderFns[e].call(this._renderProxy,null,this),"__static__"+e,!1),o}function Tt(e,t,n){return It(e,"__once__"+t+(n?"_"+n:""),!0),e}function It(e,t,n){if(Array.isArray(e))for(var o=0;o<e.length;o++)e[o]&&"string"!=typeof e[o]&&At(e[o],t+"_"+o,n);else At(e,t,n)}function At(e,t,n){e.isStatic=!0,e.key=t,e.isOnce=n}function _t(e,t){if(t)if(u(t)){var n=e.on=e.on?j({},e.on):{};for(var o in t){var i=n[o],r=t[o];n[o]=i?[].concat(i,r):r}}else;return e}function Et(e,t,n,o){t=t||{$stable:!n};for(var i=0;i<e.length;i++){var r=e[i];Array.isArray(r)?Et(r,t,n):r&&(r.proxy&&(r.fn.proxy=!0),t[r.key]=r.fn)}return o&&(t.$key=o),t}function jt(e,t){for(var n=0;n<t.length;n+=2){var o=t[n];"string"==typeof o&&o&&(e[t[n]]=t[n+1])}return e}function Ot(e,t){return"string"==typeof e?t+e:e}function Wt(e){e._o=Tt,e._n=w,e._s=m,e._l=yt,e._t=vt,e._q=R,e._i=D,e._m=Ct,e._f=bt,e._k=xt,e._b=St,e._v=ye,e._e=ge,e._u=Et,e._g=_t,e._d=jt,e._p=Ot}function Pt(e,t,n,o,r){var a,l=this,c=r.options;k(o,"_uid")?(a=Object.create(o))._original=o:(a=o,o=o._original);var d=s(c._compiled),u=!d;this.data=e,this.props=t,this.children=n,this.parent=o,this.listeners=e.on||i,this.injections=ht(c.inject,o),this.slots=function(){return l.$slots||mt(e.scopedSlots,l.$slots=ft(n,o)),l.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return mt(e.scopedSlots,this.slots())}}),d&&(this.$options=c,this.$slots=this.slots(),this.$scopedSlots=mt(e.scopedSlots,this.$slots)),c._scopeId?this._c=function(e,t,n,i){var r=Lt(a,e,t,n,i,u);return r&&!Array.isArray(r)&&(r.fnScopeId=c._scopeId,r.fnContext=o),r}:this._c=function(e,t,n,o){return Lt(a,e,t,n,o,u)}}function qt(e,t,n,o,i){var r=ve(e);return r.fnContext=n,r.fnOptions=o,t.slot&&((r.data||(r.data={})).slot=t.slot),r}function Rt(e,t){for(var n in t)e[C(n)]=t[n]}Wt(Pt.prototype);var Dt={init:function(e,t){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var n=e;Dt.prepatch(n,n)}else{(e.componentInstance=function(e,t){var n={_isComponent:!0,_parentVnode:e,parent:t},o=e.data.inlineTemplate;a(o)&&(n.render=o.render,n.staticRenderFns=o.staticRenderFns);return new e.componentOptions.Ctor(n)}(e,Kt)).$mount(t?e.elm:void 0,t)}},prepatch:function(e,t){var n=t.componentOptions;!function(e,t,n,o,r){0;var a=o.data.scopedSlots,s=e.$scopedSlots,l=!!(a&&!a.$stable||s!==i&&!s.$stable||a&&e.$scopedSlots.$key!==a.$key),c=!!(r||e.$options._renderChildren||l);e.$options._parentVnode=o,e.$vnode=o,e._vnode&&(e._vnode.parent=o);if(e.$options._renderChildren=r,e.$attrs=o.data.attrs||i,e.$listeners=n||i,t&&e.$options.props){Ce(!1);for(var d=e._props,u=e.$options._propKeys||[],h=0;h<u.length;h++){var f=u[h],p=e.$options.props;d[f]=Fe(f,p,t,e)}Ce(!0),e.$options.propsData=t}n=n||i;var m=e.$options._parentListeners;e.$options._parentListeners=n,Yt(e,n,m),c&&(e.$slots=ft(r,o.context),e.$forceUpdate());0}(t.componentInstance=e.componentInstance,n.propsData,n.listeners,t,n.children)},insert:function(e){var t,n=e.context,o=e.componentInstance;o._isMounted||(o._isMounted=!0,en(o,"mounted")),e.data.keepAlive&&(n._isMounted?((t=o)._inactive=!1,nn.push(t)):Zt(o,!0))},destroy:function(e){var t=e.componentInstance;t._isDestroyed||(e.data.keepAlive?function e(t,n){if(n&&(t._directInactive=!0,Xt(t)))return;if(!t._inactive){t._inactive=!0;for(var o=0;o<t.$children.length;o++)e(t.$children[o]);en(t,"deactivated")}}(t,!0):t.$destroy())}},Nt=Object.keys(Dt);function zt(e,t,n,o,l){if(!r(e)){var d=n.$options._base;if(c(e)&&(e=d.extend(e)),"function"==typeof e){var u;if(r(e.cid)&&void 0===(e=function(e,t){if(s(e.error)&&a(e.errorComp))return e.errorComp;if(a(e.resolved))return e.resolved;var n=Gt;n&&a(e.owners)&&-1===e.owners.indexOf(n)&&e.owners.push(n);if(s(e.loading)&&a(e.loadingComp))return e.loadingComp;if(n&&!a(e.owners)){var o=e.owners=[n],i=!0,l=null,d=null;n.$on("hook:destroyed",(function(){return v(o,n)}));var u=function(e){for(var t=0,n=o.length;t<n;t++)o[t].$forceUpdate();e&&(o.length=0,null!==l&&(clearTimeout(l),l=null),null!==d&&(clearTimeout(d),d=null))},h=N((function(n){e.resolved=Ht(n,t),i?o.length=0:u(!0)})),f=N((function(t){a(e.errorComp)&&(e.error=!0,u(!0))})),m=e(h,f);return c(m)&&(p(m)?r(e.resolved)&&m.then(h,f):p(m.component)&&(m.component.then(h,f),a(m.error)&&(e.errorComp=Ht(m.error,t)),a(m.loading)&&(e.loadingComp=Ht(m.loading,t),0===m.delay?e.loading=!0:l=setTimeout((function(){l=null,r(e.resolved)&&r(e.error)&&(e.loading=!0,u(!1))}),m.delay||200)),a(m.timeout)&&(d=setTimeout((function(){d=null,r(e.resolved)&&f(null)}),m.timeout)))),i=!1,e.loading?e.loadingComp:e.resolved}}(u=e,d)))return function(e,t,n,o,i){var r=ge();return r.asyncFactory=e,r.asyncMeta={data:t,context:n,children:o,tag:i},r}(u,t,n,o,l);t=t||{},Sn(e),a(t.model)&&function(e,t){var n=e.model&&e.model.prop||"value",o=e.model&&e.model.event||"input";(t.attrs||(t.attrs={}))[n]=t.model.value;var i=t.on||(t.on={}),r=i[o],s=t.model.callback;a(r)?(Array.isArray(r)?-1===r.indexOf(s):r!==s)&&(i[o]=[s].concat(r)):i[o]=s}(e.options,t);var h=function(e,t,n){var o=t.options.props;if(!r(o)){var i={},s=e.attrs,l=e.props;if(a(s)||a(l))for(var c in o){var d=A(c);ct(i,l,c,d,!0)||ct(i,s,c,d,!1)}return i}}(t,e);if(s(e.options.functional))return function(e,t,n,o,r){var s=e.options,l={},c=s.props;if(a(c))for(var d in c)l[d]=Fe(d,c,t||i);else a(n.attrs)&&Rt(l,n.attrs),a(n.props)&&Rt(l,n.props);var u=new Pt(n,l,r,o,e),h=s.render.call(null,u._c,u);if(h instanceof me)return qt(h,n,u.parent,s,u);if(Array.isArray(h)){for(var f=dt(h)||[],p=new Array(f.length),m=0;m<f.length;m++)p[m]=qt(f[m],n,u.parent,s,u);return p}}(e,h,t,n,o);var f=t.on;if(t.on=t.nativeOn,s(e.options.abstract)){var m=t.slot;t={},m&&(t.slot=m)}!function(e){for(var t=e.hook||(e.hook={}),n=0;n<Nt.length;n++){var o=Nt[n],i=t[o],r=Dt[o];i===r||i&&i._merged||(t[o]=i?Ft(r,i):r)}}(t);var w=e.options.name||l;return new me("vue-component-"+e.cid+(w?"-"+w:""),t,void 0,void 0,void 0,n,{Ctor:e,propsData:h,listeners:f,tag:l,children:o},u)}}}function Ft(e,t){var n=function(n,o){e(n,o),t(n,o)};return n._merged=!0,n}function Lt(e,t,n,o,i,d){return(Array.isArray(n)||l(n))&&(i=o,o=n,n=void 0),s(d)&&(i=2),function(e,t,n,o,i){if(a(n)&&a(n.__ob__))return ge();a(n)&&a(n.is)&&(t=n.is);if(!t)return ge();0;Array.isArray(o)&&"function"==typeof o[0]&&((n=n||{}).scopedSlots={default:o[0]},o.length=0);2===i?o=dt(o):1===i&&(o=function(e){for(var t=0;t<e.length;t++)if(Array.isArray(e[t]))return Array.prototype.concat.apply([],e);return e}(o));var l,d;if("string"==typeof t){var u;d=e.$vnode&&e.$vnode.ns||L.getTagNamespace(t),l=L.isReservedTag(t)?new me(L.parsePlatformTagName(t),n,o,void 0,void 0,e):n&&n.pre||!a(u=ze(e.$options,"components",t))?new me(t,n,o,void 0,void 0,e):zt(u,n,e,o,t)}else l=zt(t,n,e,o);return Array.isArray(l)?l:a(l)?(a(d)&&function e(t,n,o){t.ns=n,"foreignObject"===t.tag&&(n=void 0,o=!0);if(a(t.children))for(var i=0,l=t.children.length;i<l;i++){var c=t.children[i];a(c.tag)&&(r(c.ns)||s(o)&&"svg"!==c.tag)&&e(c,n,o)}}(l,d),a(n)&&function(e){c(e.style)&&it(e.style);c(e.class)&&it(e.class)}(n),l):ge()}(e,t,n,o,i)}var Mt,Gt=null;function Ht(e,t){return(e.__esModule||le&&"Module"===e[Symbol.toStringTag])&&(e=e.default),c(e)?t.extend(e):e}function $t(e){return e.isComment&&e.asyncFactory}function Ut(e){if(Array.isArray(e))for(var t=0;t<e.length;t++){var n=e[t];if(a(n)&&(a(n.componentOptions)||$t(n)))return n}}function Bt(e,t){Mt.$on(e,t)}function Vt(e,t){Mt.$off(e,t)}function Qt(e,t){var n=Mt;return function o(){var i=t.apply(null,arguments);null!==i&&n.$off(e,o)}}function Yt(e,t,n){Mt=e,st(t,n||{},Bt,Vt,Qt,e),Mt=void 0}var Kt=null;function Jt(e){var t=Kt;return Kt=e,function(){Kt=t}}function Xt(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function Zt(e,t){if(t){if(e._directInactive=!1,Xt(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var n=0;n<e.$children.length;n++)Zt(e.$children[n]);en(e,"activated")}}function en(e,t){fe();var n=e.$options[t],o=t+" hook";if(n)for(var i=0,r=n.length;i<r;i++)$e(n[i],e,null,e,o);e._hasHookEvent&&e.$emit("hook:"+t),pe()}var tn=[],nn=[],on={},rn=!1,an=!1,sn=0;var ln=0,cn=Date.now;if(B&&!K){var dn=window.performance;dn&&"function"==typeof dn.now&&cn()>document.createEvent("Event").timeStamp&&(cn=function(){return dn.now()})}function un(){var e,t;for(ln=cn(),an=!0,tn.sort((function(e,t){return e.id-t.id})),sn=0;sn<tn.length;sn++)(e=tn[sn]).before&&e.before(),t=e.id,on[t]=null,e.run();var n=nn.slice(),o=tn.slice();sn=tn.length=nn.length=0,on={},rn=an=!1,function(e){for(var t=0;t<e.length;t++)e[t]._inactive=!0,Zt(e[t],!0)}(n),function(e){var t=e.length;for(;t--;){var n=e[t],o=n.vm;o._watcher===n&&o._isMounted&&!o._isDestroyed&&en(o,"updated")}}(o),re&&L.devtools&&re.emit("flush")}var hn=0,fn=function(e,t,n,o,i){this.vm=e,i&&(e._watcher=this),e._watchers.push(this),o?(this.deep=!!o.deep,this.user=!!o.user,this.lazy=!!o.lazy,this.sync=!!o.sync,this.before=o.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=n,this.id=++hn,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new se,this.newDepIds=new se,this.expression="","function"==typeof t?this.getter=t:(this.getter=function(e){if(!H.test(e)){var t=e.split(".");return function(e){for(var n=0;n<t.length;n++){if(!e)return;e=e[t[n]]}return e}}}(t),this.getter||(this.getter=W)),this.value=this.lazy?void 0:this.get()};fn.prototype.get=function(){var e;fe(this);var t=this.vm;try{e=this.getter.call(t,t)}catch(e){if(!this.user)throw e;He(e,t,'getter for watcher "'+this.expression+'"')}finally{this.deep&&it(e),pe(),this.cleanupDeps()}return e},fn.prototype.addDep=function(e){var t=e.id;this.newDepIds.has(t)||(this.newDepIds.add(t),this.newDeps.push(e),this.depIds.has(t)||e.addSub(this))},fn.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var t=this.deps[e];this.newDepIds.has(t.id)||t.removeSub(this)}var n=this.depIds;this.depIds=this.newDepIds,this.newDepIds=n,this.newDepIds.clear(),n=this.deps,this.deps=this.newDeps,this.newDeps=n,this.newDeps.length=0},fn.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(e){var t=e.id;if(null==on[t]){if(on[t]=!0,an){for(var n=tn.length-1;n>sn&&tn[n].id>e.id;)n--;tn.splice(n+1,0,e)}else tn.push(e);rn||(rn=!0,nt(un))}}(this)},fn.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||c(e)||this.deep){var t=this.value;if(this.value=e,this.user)try{this.cb.call(this.vm,e,t)}catch(e){He(e,this.vm,'callback for watcher "'+this.expression+'"')}else this.cb.call(this.vm,e,t)}}},fn.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},fn.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},fn.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||v(this.vm._watchers,this);for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1}};var pn={enumerable:!0,configurable:!0,get:W,set:W};function mn(e,t,n){pn.get=function(){return this[t][n]},pn.set=function(e){this[t][n]=e},Object.defineProperty(e,n,pn)}function wn(e){e._watchers=[];var t=e.$options;t.props&&function(e,t){var n=e.$options.propsData||{},o=e._props={},i=e.$options._propKeys=[];e.$parent&&Ce(!1);var r=function(r){i.push(r);var a=Fe(r,t,n,e);Ae(o,r,a),r in e||mn(e,"_props",r)};for(var a in t)r(a);Ce(!0)}(e,t.props),t.methods&&function(e,t){e.$options.props;for(var n in t)e[n]="function"!=typeof t[n]?W:_(t[n],e)}(e,t.methods),t.data?function(e){var t=e.$options.data;u(t=e._data="function"==typeof t?function(e,t){fe();try{return e.call(t,t)}catch(e){return He(e,t,"data()"),{}}finally{pe()}}(t,e):t||{})||(t={});var n=Object.keys(t),o=e.$options.props,i=(e.$options.methods,n.length);for(;i--;){var r=n[i];0,o&&k(o,r)||(a=void 0,36!==(a=(r+"").charCodeAt(0))&&95!==a&&mn(e,"_data",r))}var a;Ie(t,!0)}(e):Ie(e._data={},!0),t.computed&&function(e,t){var n=e._computedWatchers=Object.create(null),o=ie();for(var i in t){var r=t[i],a="function"==typeof r?r:r.get;0,o||(n[i]=new fn(e,a||W,W,gn)),i in e||yn(e,i,r)}}(e,t.computed),t.watch&&t.watch!==te&&function(e,t){for(var n in t){var o=t[n];if(Array.isArray(o))for(var i=0;i<o.length;i++)kn(e,n,o[i]);else kn(e,n,o)}}(e,t.watch)}var gn={lazy:!0};function yn(e,t,n){var o=!ie();"function"==typeof n?(pn.get=o?vn(t):bn(n),pn.set=W):(pn.get=n.get?o&&!1!==n.cache?vn(t):bn(n.get):W,pn.set=n.set||W),Object.defineProperty(e,t,pn)}function vn(e){return function(){var t=this._computedWatchers&&this._computedWatchers[e];if(t)return t.dirty&&t.evaluate(),ue.target&&t.depend(),t.value}}function bn(e){return function(){return e.call(this,this)}}function kn(e,t,n,o){return u(n)&&(o=n,n=n.handler),"string"==typeof n&&(n=e[n]),e.$watch(t,n,o)}var xn=0;function Sn(e){var t=e.options;if(e.super){var n=Sn(e.super);if(n!==e.superOptions){e.superOptions=n;var o=function(e){var t,n=e.options,o=e.sealedOptions;for(var i in n)n[i]!==o[i]&&(t||(t={}),t[i]=n[i]);return t}(e);o&&j(e.extendOptions,o),(t=e.options=Ne(n,e.extendOptions)).name&&(t.components[t.name]=e)}}return t}function Cn(e){this._init(e)}function Tn(e){e.cid=0;var t=1;e.extend=function(e){e=e||{};var n=this,o=n.cid,i=e._Ctor||(e._Ctor={});if(i[o])return i[o];var r=e.name||n.options.name;var a=function(e){this._init(e)};return(a.prototype=Object.create(n.prototype)).constructor=a,a.cid=t++,a.options=Ne(n.options,e),a.super=n,a.options.props&&function(e){var t=e.options.props;for(var n in t)mn(e.prototype,"_props",n)}(a),a.options.computed&&function(e){var t=e.options.computed;for(var n in t)yn(e.prototype,n,t[n])}(a),a.extend=n.extend,a.mixin=n.mixin,a.use=n.use,z.forEach((function(e){a[e]=n[e]})),r&&(a.options.components[r]=a),a.superOptions=n.options,a.extendOptions=e,a.sealedOptions=j({},a.options),i[o]=a,a}}function In(e){return e&&(e.Ctor.options.name||e.tag)}function An(e,t){return Array.isArray(e)?e.indexOf(t)>-1:"string"==typeof e?e.split(",").indexOf(t)>-1:!!h(e)&&e.test(t)}function _n(e,t){var n=e.cache,o=e.keys,i=e._vnode;for(var r in n){var a=n[r];if(a){var s=In(a.componentOptions);s&&!t(s)&&En(n,r,o,i)}}}function En(e,t,n,o){var i=e[t];!i||o&&i.tag===o.tag||i.componentInstance.$destroy(),e[t]=null,v(n,t)}Cn.prototype._init=function(e){var t=this;t._uid=xn++,t._isVue=!0,e&&e._isComponent?function(e,t){var n=e.$options=Object.create(e.constructor.options),o=t._parentVnode;n.parent=t.parent,n._parentVnode=o;var i=o.componentOptions;n.propsData=i.propsData,n._parentListeners=i.listeners,n._renderChildren=i.children,n._componentTag=i.tag,t.render&&(n.render=t.render,n.staticRenderFns=t.staticRenderFns)}(t,e):t.$options=Ne(Sn(t.constructor),e||{},t),t._renderProxy=t,t._self=t,function(e){var t=e.$options,n=t.parent;if(n&&!t.abstract){for(;n.$options.abstract&&n.$parent;)n=n.$parent;n.$children.push(e)}e.$parent=n,e.$root=n?n.$root:e,e.$children=[],e.$refs={},e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(t),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var t=e.$options._parentListeners;t&&Yt(e,t)}(t),function(e){e._vnode=null,e._staticTrees=null;var t=e.$options,n=e.$vnode=t._parentVnode,o=n&&n.context;e.$slots=ft(t._renderChildren,o),e.$scopedSlots=i,e._c=function(t,n,o,i){return Lt(e,t,n,o,i,!1)},e.$createElement=function(t,n,o,i){return Lt(e,t,n,o,i,!0)};var r=n&&n.data;Ae(e,"$attrs",r&&r.attrs||i,null,!0),Ae(e,"$listeners",t._parentListeners||i,null,!0)}(t),en(t,"beforeCreate"),function(e){var t=ht(e.$options.inject,e);t&&(Ce(!1),Object.keys(t).forEach((function(n){Ae(e,n,t[n])})),Ce(!0))}(t),wn(t),function(e){var t=e.$options.provide;t&&(e._provided="function"==typeof t?t.call(e):t)}(t),en(t,"created"),t.$options.el&&t.$mount(t.$options.el)},function(e){var t={get:function(){return this._data}},n={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",t),Object.defineProperty(e.prototype,"$props",n),e.prototype.$set=_e,e.prototype.$delete=Ee,e.prototype.$watch=function(e,t,n){if(u(t))return kn(this,e,t,n);(n=n||{}).user=!0;var o=new fn(this,e,t,n);if(n.immediate)try{t.call(this,o.value)}catch(e){He(e,this,'callback for immediate watcher "'+o.expression+'"')}return function(){o.teardown()}}}(Cn),function(e){var t=/^hook:/;e.prototype.$on=function(e,n){var o=this;if(Array.isArray(e))for(var i=0,r=e.length;i<r;i++)o.$on(e[i],n);else(o._events[e]||(o._events[e]=[])).push(n),t.test(e)&&(o._hasHookEvent=!0);return o},e.prototype.$once=function(e,t){var n=this;function o(){n.$off(e,o),t.apply(n,arguments)}return o.fn=t,n.$on(e,o),n},e.prototype.$off=function(e,t){var n=this;if(!arguments.length)return n._events=Object.create(null),n;if(Array.isArray(e)){for(var o=0,i=e.length;o<i;o++)n.$off(e[o],t);return n}var r,a=n._events[e];if(!a)return n;if(!t)return n._events[e]=null,n;for(var s=a.length;s--;)if((r=a[s])===t||r.fn===t){a.splice(s,1);break}return n},e.prototype.$emit=function(e){var t=this,n=t._events[e];if(n){n=n.length>1?E(n):n;for(var o=E(arguments,1),i='event handler for "'+e+'"',r=0,a=n.length;r<a;r++)$e(n[r],t,o,t,i)}return t}}(Cn),function(e){e.prototype._update=function(e,t){var n=this,o=n.$el,i=n._vnode,r=Jt(n);n._vnode=e,n.$el=i?n.__patch__(i,e):n.__patch__(n.$el,e,t,!1),r(),o&&(o.__vue__=null),n.$el&&(n.$el.__vue__=n),n.$vnode&&n.$parent&&n.$vnode===n.$parent._vnode&&(n.$parent.$el=n.$el)},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){en(e,"beforeDestroy"),e._isBeingDestroyed=!0;var t=e.$parent;!t||t._isBeingDestroyed||e.$options.abstract||v(t.$children,e),e._watcher&&e._watcher.teardown();for(var n=e._watchers.length;n--;)e._watchers[n].teardown();e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),en(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Cn),function(e){Wt(e.prototype),e.prototype.$nextTick=function(e){return nt(e,this)},e.prototype._render=function(){var e,t=this,n=t.$options,o=n.render,i=n._parentVnode;i&&(t.$scopedSlots=mt(i.data.scopedSlots,t.$slots,t.$scopedSlots)),t.$vnode=i;try{Gt=t,e=o.call(t._renderProxy,t.$createElement)}catch(n){He(n,t,"render"),e=t._vnode}finally{Gt=null}return Array.isArray(e)&&1===e.length&&(e=e[0]),e instanceof me||(e=ge()),e.parent=i,e}}(Cn);var jn=[String,RegExp,Array],On={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:jn,exclude:jn,max:[String,Number]},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)En(this.cache,e,this.keys)},mounted:function(){var e=this;this.$watch("include",(function(t){_n(e,(function(e){return An(t,e)}))})),this.$watch("exclude",(function(t){_n(e,(function(e){return!An(t,e)}))}))},render:function(){var e=this.$slots.default,t=Ut(e),n=t&&t.componentOptions;if(n){var o=In(n),i=this.include,r=this.exclude;if(i&&(!o||!An(i,o))||r&&o&&An(r,o))return t;var a=this.cache,s=this.keys,l=null==t.key?n.Ctor.cid+(n.tag?"::"+n.tag:""):t.key;a[l]?(t.componentInstance=a[l].componentInstance,v(s,l),s.push(l)):(a[l]=t,s.push(l),this.max&&s.length>parseInt(this.max)&&En(a,s[0],s,this._vnode)),t.data.keepAlive=!0}return t||e&&e[0]}}};!function(e){var t={get:function(){return L}};Object.defineProperty(e,"config",t),e.util={warn:ce,extend:j,mergeOptions:Ne,defineReactive:Ae},e.set=_e,e.delete=Ee,e.nextTick=nt,e.observable=function(e){return Ie(e),e},e.options=Object.create(null),z.forEach((function(t){e.options[t+"s"]=Object.create(null)})),e.options._base=e,j(e.options.components,On),function(e){e.use=function(e){var t=this._installedPlugins||(this._installedPlugins=[]);if(t.indexOf(e)>-1)return this;var n=E(arguments,1);return n.unshift(this),"function"==typeof e.install?e.install.apply(e,n):"function"==typeof e&&e.apply(null,n),t.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=Ne(this.options,e),this}}(e),Tn(e),function(e){z.forEach((function(t){e[t]=function(e,n){return n?("component"===t&&u(n)&&(n.name=n.name||e,n=this.options._base.extend(n)),"directive"===t&&"function"==typeof n&&(n={bind:n,update:n}),this.options[t+"s"][e]=n,n):this.options[t+"s"][e]}}))}(e)}(Cn),Object.defineProperty(Cn.prototype,"$isServer",{get:ie}),Object.defineProperty(Cn.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Cn,"FunctionalRenderContext",{value:Pt}),Cn.version="2.6.12";var Wn=g("style,class"),Pn=g("input,textarea,option,select,progress"),qn=g("contenteditable,draggable,spellcheck"),Rn=g("events,caret,typing,plaintext-only"),Dn=g("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,translate,truespeed,typemustmatch,visible"),Nn="http://www.w3.org/1999/xlink",zn=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},Fn=function(e){return zn(e)?e.slice(6,e.length):""},Ln=function(e){return null==e||!1===e};function Mn(e){for(var t=e.data,n=e,o=e;a(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(t=Gn(o.data,t));for(;a(n=n.parent);)n&&n.data&&(t=Gn(t,n.data));return function(e,t){if(a(e)||a(t))return Hn(e,$n(t));return""}(t.staticClass,t.class)}function Gn(e,t){return{staticClass:Hn(e.staticClass,t.staticClass),class:a(e.class)?[e.class,t.class]:t.class}}function Hn(e,t){return e?t?e+" "+t:e:t||""}function $n(e){return Array.isArray(e)?function(e){for(var t,n="",o=0,i=e.length;o<i;o++)a(t=$n(e[o]))&&""!==t&&(n&&(n+=" "),n+=t);return n}(e):c(e)?function(e){var t="";for(var n in e)e[n]&&(t&&(t+=" "),t+=n);return t}(e):"string"==typeof e?e:""}var Un={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Bn=g("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Vn=g("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignObject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Qn=function(e){return Bn(e)||Vn(e)};var Yn=Object.create(null);var Kn=g("text,number,password,search,email,tel,url");var Jn=Object.freeze({createElement:function(e,t){var n=document.createElement(e);return"select"!==e||t.data&&t.data.attrs&&void 0!==t.data.attrs.multiple&&n.setAttribute("multiple","multiple"),n},createElementNS:function(e,t){return document.createElementNS(Un[e],t)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,t,n){e.insertBefore(t,n)},removeChild:function(e,t){e.removeChild(t)},appendChild:function(e,t){e.appendChild(t)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,t){e.textContent=t},setStyleScope:function(e,t){e.setAttribute(t,"")}}),Xn={create:function(e,t){Zn(t)},update:function(e,t){e.data.ref!==t.data.ref&&(Zn(e,!0),Zn(t))},destroy:function(e){Zn(e,!0)}};function Zn(e,t){var n=e.data.ref;if(a(n)){var o=e.context,i=e.componentInstance||e.elm,r=o.$refs;t?Array.isArray(r[n])?v(r[n],i):r[n]===i&&(r[n]=void 0):e.data.refInFor?Array.isArray(r[n])?r[n].indexOf(i)<0&&r[n].push(i):r[n]=[i]:r[n]=i}}var eo=new me("",{},[]),to=["create","activate","update","remove","destroy"];function no(e,t){return e.key===t.key&&(e.tag===t.tag&&e.isComment===t.isComment&&a(e.data)===a(t.data)&&function(e,t){if("input"!==e.tag)return!0;var n,o=a(n=e.data)&&a(n=n.attrs)&&n.type,i=a(n=t.data)&&a(n=n.attrs)&&n.type;return o===i||Kn(o)&&Kn(i)}(e,t)||s(e.isAsyncPlaceholder)&&e.asyncFactory===t.asyncFactory&&r(t.asyncFactory.error))}function oo(e,t,n){var o,i,r={};for(o=t;o<=n;++o)a(i=e[o].key)&&(r[i]=o);return r}var io={create:ro,update:ro,destroy:function(e){ro(e,eo)}};function ro(e,t){(e.data.directives||t.data.directives)&&function(e,t){var n,o,i,r=e===eo,a=t===eo,s=so(e.data.directives,e.context),l=so(t.data.directives,t.context),c=[],d=[];for(n in l)o=s[n],i=l[n],o?(i.oldValue=o.value,i.oldArg=o.arg,co(i,"update",t,e),i.def&&i.def.componentUpdated&&d.push(i)):(co(i,"bind",t,e),i.def&&i.def.inserted&&c.push(i));if(c.length){var u=function(){for(var n=0;n<c.length;n++)co(c[n],"inserted",t,e)};r?lt(t,"insert",u):u()}d.length&&lt(t,"postpatch",(function(){for(var n=0;n<d.length;n++)co(d[n],"componentUpdated",t,e)}));if(!r)for(n in s)l[n]||co(s[n],"unbind",e,e,a)}(e,t)}var ao=Object.create(null);function so(e,t){var n,o,i=Object.create(null);if(!e)return i;for(n=0;n<e.length;n++)(o=e[n]).modifiers||(o.modifiers=ao),i[lo(o)]=o,o.def=ze(t.$options,"directives",o.name);return i}function lo(e){return e.rawName||e.name+"."+Object.keys(e.modifiers||{}).join(".")}function co(e,t,n,o,i){var r=e.def&&e.def[t];if(r)try{r(n.elm,e,n,o,i)}catch(o){He(o,n.context,"directive "+e.name+" "+t+" hook")}}var uo=[Xn,io];function ho(e,t){var n=t.componentOptions;if(!(a(n)&&!1===n.Ctor.options.inheritAttrs||r(e.data.attrs)&&r(t.data.attrs))){var o,i,s=t.elm,l=e.data.attrs||{},c=t.data.attrs||{};for(o in a(c.__ob__)&&(c=t.data.attrs=j({},c)),c)i=c[o],l[o]!==i&&fo(s,o,i);for(o in(K||X)&&c.value!==l.value&&fo(s,"value",c.value),l)r(c[o])&&(zn(o)?s.removeAttributeNS(Nn,Fn(o)):qn(o)||s.removeAttribute(o))}}function fo(e,t,n){e.tagName.indexOf("-")>-1?po(e,t,n):Dn(t)?Ln(n)?e.removeAttribute(t):(n="allowfullscreen"===t&&"EMBED"===e.tagName?"true":t,e.setAttribute(t,n)):qn(t)?e.setAttribute(t,function(e,t){return Ln(t)||"false"===t?"false":"contenteditable"===e&&Rn(t)?t:"true"}(t,n)):zn(t)?Ln(n)?e.removeAttributeNS(Nn,Fn(t)):e.setAttributeNS(Nn,t,n):po(e,t,n)}function po(e,t,n){if(Ln(n))e.removeAttribute(t);else{if(K&&!J&&"TEXTAREA"===e.tagName&&"placeholder"===t&&""!==n&&!e.__ieph){var o=function(t){t.stopImmediatePropagation(),e.removeEventListener("input",o)};e.addEventListener("input",o),e.__ieph=!0}e.setAttribute(t,n)}}var mo={create:ho,update:ho};function wo(e,t){var n=t.elm,o=t.data,i=e.data;if(!(r(o.staticClass)&&r(o.class)&&(r(i)||r(i.staticClass)&&r(i.class)))){var s=Mn(t),l=n._transitionClasses;a(l)&&(s=Hn(s,$n(l))),s!==n._prevClass&&(n.setAttribute("class",s),n._prevClass=s)}}var go,yo={create:wo,update:wo};function vo(e,t,n){var o=go;return function i(){var r=t.apply(null,arguments);null!==r&&xo(e,i,n,o)}}var bo=Qe&&!(ee&&Number(ee[1])<=53);function ko(e,t,n,o){if(bo){var i=ln,r=t;t=r._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=i||e.timeStamp<=0||e.target.ownerDocument!==document)return r.apply(this,arguments)}}go.addEventListener(e,t,ne?{capture:n,passive:o}:n)}function xo(e,t,n,o){(o||go).removeEventListener(e,t._wrapper||t,n)}function So(e,t){if(!r(e.data.on)||!r(t.data.on)){var n=t.data.on||{},o=e.data.on||{};go=t.elm,function(e){if(a(e.__r)){var t=K?"change":"input";e[t]=[].concat(e.__r,e[t]||[]),delete e.__r}a(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(n),st(n,o,ko,xo,vo,t.context),go=void 0}}var Co,To={create:So,update:So};function Io(e,t){if(!r(e.data.domProps)||!r(t.data.domProps)){var n,o,i=t.elm,s=e.data.domProps||{},l=t.data.domProps||{};for(n in a(l.__ob__)&&(l=t.data.domProps=j({},l)),s)n in l||(i[n]="");for(n in l){if(o=l[n],"textContent"===n||"innerHTML"===n){if(t.children&&(t.children.length=0),o===s[n])continue;1===i.childNodes.length&&i.removeChild(i.childNodes[0])}if("value"===n&&"PROGRESS"!==i.tagName){i._value=o;var c=r(o)?"":String(o);Ao(i,c)&&(i.value=c)}else if("innerHTML"===n&&Vn(i.tagName)&&r(i.innerHTML)){(Co=Co||document.createElement("div")).innerHTML="<svg>"+o+"</svg>";for(var d=Co.firstChild;i.firstChild;)i.removeChild(i.firstChild);for(;d.firstChild;)i.appendChild(d.firstChild)}else if(o!==s[n])try{i[n]=o}catch(e){}}}}function Ao(e,t){return!e.composing&&("OPTION"===e.tagName||function(e,t){var n=!0;try{n=document.activeElement!==e}catch(e){}return n&&e.value!==t}(e,t)||function(e,t){var n=e.value,o=e._vModifiers;if(a(o)){if(o.number)return w(n)!==w(t);if(o.trim)return n.trim()!==t.trim()}return n!==t}(e,t))}var _o={create:Io,update:Io},Eo=x((function(e){var t={},n=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var o=e.split(n);o.length>1&&(t[o[0].trim()]=o[1].trim())}})),t}));function jo(e){var t=Oo(e.style);return e.staticStyle?j(e.staticStyle,t):t}function Oo(e){return Array.isArray(e)?O(e):"string"==typeof e?Eo(e):e}var Wo,Po=/^--/,qo=/\s*!important$/,Ro=function(e,t,n){if(Po.test(t))e.style.setProperty(t,n);else if(qo.test(n))e.style.setProperty(A(t),n.replace(qo,""),"important");else{var o=No(t);if(Array.isArray(n))for(var i=0,r=n.length;i<r;i++)e.style[o]=n[i];else e.style[o]=n}},Do=["Webkit","Moz","ms"],No=x((function(e){if(Wo=Wo||document.createElement("div").style,"filter"!==(e=C(e))&&e in Wo)return e;for(var t=e.charAt(0).toUpperCase()+e.slice(1),n=0;n<Do.length;n++){var o=Do[n]+t;if(o in Wo)return o}}));function zo(e,t){var n=t.data,o=e.data;if(!(r(n.staticStyle)&&r(n.style)&&r(o.staticStyle)&&r(o.style))){var i,s,l=t.elm,c=o.staticStyle,d=o.normalizedStyle||o.style||{},u=c||d,h=Oo(t.data.style)||{};t.data.normalizedStyle=a(h.__ob__)?j({},h):h;var f=function(e,t){var n,o={};if(t)for(var i=e;i.componentInstance;)(i=i.componentInstance._vnode)&&i.data&&(n=jo(i.data))&&j(o,n);(n=jo(e.data))&&j(o,n);for(var r=e;r=r.parent;)r.data&&(n=jo(r.data))&&j(o,n);return o}(t,!0);for(s in u)r(f[s])&&Ro(l,s,"");for(s in f)(i=f[s])!==u[s]&&Ro(l,s,null==i?"":i)}}var Fo={create:zo,update:zo},Lo=/\s+/;function Mo(e,t){if(t&&(t=t.trim()))if(e.classList)t.indexOf(" ")>-1?t.split(Lo).forEach((function(t){return e.classList.add(t)})):e.classList.add(t);else{var n=" "+(e.getAttribute("class")||"")+" ";n.indexOf(" "+t+" ")<0&&e.setAttribute("class",(n+t).trim())}}function Go(e,t){if(t&&(t=t.trim()))if(e.classList)t.indexOf(" ")>-1?t.split(Lo).forEach((function(t){return e.classList.remove(t)})):e.classList.remove(t),e.classList.length||e.removeAttribute("class");else{for(var n=" "+(e.getAttribute("class")||"")+" ",o=" "+t+" ";n.indexOf(o)>=0;)n=n.replace(o," ");(n=n.trim())?e.setAttribute("class",n):e.removeAttribute("class")}}function Ho(e){if(e){if("object"==typeof e){var t={};return!1!==e.css&&j(t,$o(e.name||"v")),j(t,e),t}return"string"==typeof e?$o(e):void 0}}var $o=x((function(e){return{enterClass:e+"-enter",enterToClass:e+"-enter-to",enterActiveClass:e+"-enter-active",leaveClass:e+"-leave",leaveToClass:e+"-leave-to",leaveActiveClass:e+"-leave-active"}})),Uo=B&&!J,Bo="transition",Vo="transitionend",Qo="animation",Yo="animationend";Uo&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Bo="WebkitTransition",Vo="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Qo="WebkitAnimation",Yo="webkitAnimationEnd"));var Ko=B?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function Jo(e){Ko((function(){Ko(e)}))}function Xo(e,t){var n=e._transitionClasses||(e._transitionClasses=[]);n.indexOf(t)<0&&(n.push(t),Mo(e,t))}function Zo(e,t){e._transitionClasses&&v(e._transitionClasses,t),Go(e,t)}function ei(e,t,n){var o=ni(e,t),i=o.type,r=o.timeout,a=o.propCount;if(!i)return n();var s="transition"===i?Vo:Yo,l=0,c=function(){e.removeEventListener(s,d),n()},d=function(t){t.target===e&&++l>=a&&c()};setTimeout((function(){l<a&&c()}),r+1),e.addEventListener(s,d)}var ti=/\b(transform|all)(,|$)/;function ni(e,t){var n,o=window.getComputedStyle(e),i=(o[Bo+"Delay"]||"").split(", "),r=(o[Bo+"Duration"]||"").split(", "),a=oi(i,r),s=(o[Qo+"Delay"]||"").split(", "),l=(o[Qo+"Duration"]||"").split(", "),c=oi(s,l),d=0,u=0;return"transition"===t?a>0&&(n="transition",d=a,u=r.length):"animation"===t?c>0&&(n="animation",d=c,u=l.length):u=(n=(d=Math.max(a,c))>0?a>c?"transition":"animation":null)?"transition"===n?r.length:l.length:0,{type:n,timeout:d,propCount:u,hasTransform:"transition"===n&&ti.test(o[Bo+"Property"])}}function oi(e,t){for(;e.length<t.length;)e=e.concat(e);return Math.max.apply(null,t.map((function(t,n){return ii(t)+ii(e[n])})))}function ii(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function ri(e,t){var n=e.elm;a(n._leaveCb)&&(n._leaveCb.cancelled=!0,n._leaveCb());var o=Ho(e.data.transition);if(!r(o)&&!a(n._enterCb)&&1===n.nodeType){for(var i=o.css,s=o.type,l=o.enterClass,d=o.enterToClass,u=o.enterActiveClass,h=o.appearClass,f=o.appearToClass,p=o.appearActiveClass,m=o.beforeEnter,g=o.enter,y=o.afterEnter,v=o.enterCancelled,b=o.beforeAppear,k=o.appear,x=o.afterAppear,S=o.appearCancelled,C=o.duration,T=Kt,I=Kt.$vnode;I&&I.parent;)T=I.context,I=I.parent;var A=!T._isMounted||!e.isRootInsert;if(!A||k||""===k){var _=A&&h?h:l,E=A&&p?p:u,j=A&&f?f:d,O=A&&b||m,W=A&&"function"==typeof k?k:g,P=A&&x||y,q=A&&S||v,R=w(c(C)?C.enter:C);0;var D=!1!==i&&!J,z=li(W),F=n._enterCb=N((function(){D&&(Zo(n,j),Zo(n,E)),F.cancelled?(D&&Zo(n,_),q&&q(n)):P&&P(n),n._enterCb=null}));e.data.show||lt(e,"insert",(function(){var t=n.parentNode,o=t&&t._pending&&t._pending[e.key];o&&o.tag===e.tag&&o.elm._leaveCb&&o.elm._leaveCb(),W&&W(n,F)})),O&&O(n),D&&(Xo(n,_),Xo(n,E),Jo((function(){Zo(n,_),F.cancelled||(Xo(n,j),z||(si(R)?setTimeout(F,R):ei(n,s,F)))}))),e.data.show&&(t&&t(),W&&W(n,F)),D||z||F()}}}function ai(e,t){var n=e.elm;a(n._enterCb)&&(n._enterCb.cancelled=!0,n._enterCb());var o=Ho(e.data.transition);if(r(o)||1!==n.nodeType)return t();if(!a(n._leaveCb)){var i=o.css,s=o.type,l=o.leaveClass,d=o.leaveToClass,u=o.leaveActiveClass,h=o.beforeLeave,f=o.leave,p=o.afterLeave,m=o.leaveCancelled,g=o.delayLeave,y=o.duration,v=!1!==i&&!J,b=li(f),k=w(c(y)?y.leave:y);0;var x=n._leaveCb=N((function(){n.parentNode&&n.parentNode._pending&&(n.parentNode._pending[e.key]=null),v&&(Zo(n,d),Zo(n,u)),x.cancelled?(v&&Zo(n,l),m&&m(n)):(t(),p&&p(n)),n._leaveCb=null}));g?g(S):S()}function S(){x.cancelled||(!e.data.show&&n.parentNode&&((n.parentNode._pending||(n.parentNode._pending={}))[e.key]=e),h&&h(n),v&&(Xo(n,l),Xo(n,u),Jo((function(){Zo(n,l),x.cancelled||(Xo(n,d),b||(si(k)?setTimeout(x,k):ei(n,s,x)))}))),f&&f(n,x),v||b||x())}}function si(e){return"number"==typeof e&&!isNaN(e)}function li(e){if(r(e))return!1;var t=e.fns;return a(t)?li(Array.isArray(t)?t[0]:t):(e._length||e.length)>1}function ci(e,t){!0!==t.data.show&&ri(t)}var di=function(e){var t,n,o={},i=e.modules,c=e.nodeOps;for(t=0;t<to.length;++t)for(o[to[t]]=[],n=0;n<i.length;++n)a(i[n][to[t]])&&o[to[t]].push(i[n][to[t]]);function d(e){var t=c.parentNode(e);a(t)&&c.removeChild(t,e)}function u(e,t,n,i,r,l,d){if(a(e.elm)&&a(l)&&(e=l[d]=ve(e)),e.isRootInsert=!r,!function(e,t,n,i){var r=e.data;if(a(r)){var l=a(e.componentInstance)&&r.keepAlive;if(a(r=r.hook)&&a(r=r.init)&&r(e,!1),a(e.componentInstance))return h(e,t),f(n,e.elm,i),s(l)&&function(e,t,n,i){var r,s=e;for(;s.componentInstance;)if(s=s.componentInstance._vnode,a(r=s.data)&&a(r=r.transition)){for(r=0;r<o.activate.length;++r)o.activate[r](eo,s);t.push(s);break}f(n,e.elm,i)}(e,t,n,i),!0}}(e,t,n,i)){var u=e.data,m=e.children,g=e.tag;a(g)?(e.elm=e.ns?c.createElementNS(e.ns,g):c.createElement(g,e),y(e),p(e,m,t),a(u)&&w(e,t),f(n,e.elm,i)):s(e.isComment)?(e.elm=c.createComment(e.text),f(n,e.elm,i)):(e.elm=c.createTextNode(e.text),f(n,e.elm,i))}}function h(e,t){a(e.data.pendingInsert)&&(t.push.apply(t,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,m(e)?(w(e,t),y(e)):(Zn(e),t.push(e))}function f(e,t,n){a(e)&&(a(n)?c.parentNode(n)===e&&c.insertBefore(e,t,n):c.appendChild(e,t))}function p(e,t,n){if(Array.isArray(t)){0;for(var o=0;o<t.length;++o)u(t[o],n,e.elm,null,!0,t,o)}else l(e.text)&&c.appendChild(e.elm,c.createTextNode(String(e.text)))}function m(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return a(e.tag)}function w(e,n){for(var i=0;i<o.create.length;++i)o.create[i](eo,e);a(t=e.data.hook)&&(a(t.create)&&t.create(eo,e),a(t.insert)&&n.push(e))}function y(e){var t;if(a(t=e.fnScopeId))c.setStyleScope(e.elm,t);else for(var n=e;n;)a(t=n.context)&&a(t=t.$options._scopeId)&&c.setStyleScope(e.elm,t),n=n.parent;a(t=Kt)&&t!==e.context&&t!==e.fnContext&&a(t=t.$options._scopeId)&&c.setStyleScope(e.elm,t)}function v(e,t,n,o,i,r){for(;o<=i;++o)u(n[o],r,e,t,!1,n,o)}function b(e){var t,n,i=e.data;if(a(i))for(a(t=i.hook)&&a(t=t.destroy)&&t(e),t=0;t<o.destroy.length;++t)o.destroy[t](e);if(a(t=e.children))for(n=0;n<e.children.length;++n)b(e.children[n])}function k(e,t,n){for(;t<=n;++t){var o=e[t];a(o)&&(a(o.tag)?(x(o),b(o)):d(o.elm))}}function x(e,t){if(a(t)||a(e.data)){var n,i=o.remove.length+1;for(a(t)?t.listeners+=i:t=function(e,t){function n(){0==--n.listeners&&d(e)}return n.listeners=t,n}(e.elm,i),a(n=e.componentInstance)&&a(n=n._vnode)&&a(n.data)&&x(n,t),n=0;n<o.remove.length;++n)o.remove[n](e,t);a(n=e.data.hook)&&a(n=n.remove)?n(e,t):t()}else d(e.elm)}function S(e,t,n,o){for(var i=n;i<o;i++){var r=t[i];if(a(r)&&no(e,r))return i}}function C(e,t,n,i,l,d){if(e!==t){a(t.elm)&&a(i)&&(t=i[l]=ve(t));var h=t.elm=e.elm;if(s(e.isAsyncPlaceholder))a(t.asyncFactory.resolved)?A(e.elm,t,n):t.isAsyncPlaceholder=!0;else if(s(t.isStatic)&&s(e.isStatic)&&t.key===e.key&&(s(t.isCloned)||s(t.isOnce)))t.componentInstance=e.componentInstance;else{var f,p=t.data;a(p)&&a(f=p.hook)&&a(f=f.prepatch)&&f(e,t);var w=e.children,g=t.children;if(a(p)&&m(t)){for(f=0;f<o.update.length;++f)o.update[f](e,t);a(f=p.hook)&&a(f=f.update)&&f(e,t)}r(t.text)?a(w)&&a(g)?w!==g&&function(e,t,n,o,i){var s,l,d,h=0,f=0,p=t.length-1,m=t[0],w=t[p],g=n.length-1,y=n[0],b=n[g],x=!i;for(0;h<=p&&f<=g;)r(m)?m=t[++h]:r(w)?w=t[--p]:no(m,y)?(C(m,y,o,n,f),m=t[++h],y=n[++f]):no(w,b)?(C(w,b,o,n,g),w=t[--p],b=n[--g]):no(m,b)?(C(m,b,o,n,g),x&&c.insertBefore(e,m.elm,c.nextSibling(w.elm)),m=t[++h],b=n[--g]):no(w,y)?(C(w,y,o,n,f),x&&c.insertBefore(e,w.elm,m.elm),w=t[--p],y=n[++f]):(r(s)&&(s=oo(t,h,p)),r(l=a(y.key)?s[y.key]:S(y,t,h,p))?u(y,o,e,m.elm,!1,n,f):no(d=t[l],y)?(C(d,y,o,n,f),t[l]=void 0,x&&c.insertBefore(e,d.elm,m.elm)):u(y,o,e,m.elm,!1,n,f),y=n[++f]);h>p?v(e,r(n[g+1])?null:n[g+1].elm,n,f,g,o):f>g&&k(t,h,p)}(h,w,g,n,d):a(g)?(a(e.text)&&c.setTextContent(h,""),v(h,null,g,0,g.length-1,n)):a(w)?k(w,0,w.length-1):a(e.text)&&c.setTextContent(h,""):e.text!==t.text&&c.setTextContent(h,t.text),a(p)&&a(f=p.hook)&&a(f=f.postpatch)&&f(e,t)}}}function T(e,t,n){if(s(n)&&a(e.parent))e.parent.data.pendingInsert=t;else for(var o=0;o<t.length;++o)t[o].data.hook.insert(t[o])}var I=g("attrs,class,staticClass,staticStyle,key");function A(e,t,n,o){var i,r=t.tag,l=t.data,c=t.children;if(o=o||l&&l.pre,t.elm=e,s(t.isComment)&&a(t.asyncFactory))return t.isAsyncPlaceholder=!0,!0;if(a(l)&&(a(i=l.hook)&&a(i=i.init)&&i(t,!0),a(i=t.componentInstance)))return h(t,n),!0;if(a(r)){if(a(c))if(e.hasChildNodes())if(a(i=l)&&a(i=i.domProps)&&a(i=i.innerHTML)){if(i!==e.innerHTML)return!1}else{for(var d=!0,u=e.firstChild,f=0;f<c.length;f++){if(!u||!A(u,c[f],n,o)){d=!1;break}u=u.nextSibling}if(!d||u)return!1}else p(t,c,n);if(a(l)){var m=!1;for(var g in l)if(!I(g)){m=!0,w(t,n);break}!m&&l.class&&it(l.class)}}else e.data!==t.text&&(e.data=t.text);return!0}return function(e,t,n,i){if(!r(t)){var l,d=!1,h=[];if(r(e))d=!0,u(t,h);else{var f=a(e.nodeType);if(!f&&no(e,t))C(e,t,h,null,null,i);else{if(f){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),n=!0),s(n)&&A(e,t,h))return T(t,h,!0),e;l=e,e=new me(c.tagName(l).toLowerCase(),{},[],void 0,l)}var p=e.elm,w=c.parentNode(p);if(u(t,h,p._leaveCb?null:w,c.nextSibling(p)),a(t.parent))for(var g=t.parent,y=m(t);g;){for(var v=0;v<o.destroy.length;++v)o.destroy[v](g);if(g.elm=t.elm,y){for(var x=0;x<o.create.length;++x)o.create[x](eo,g);var S=g.data.hook.insert;if(S.merged)for(var I=1;I<S.fns.length;I++)S.fns[I]()}else Zn(g);g=g.parent}a(w)?k([e],0,0):a(e.tag)&&b(e)}}return T(t,h,d),t.elm}a(e)&&b(e)}}({nodeOps:Jn,modules:[mo,yo,To,_o,Fo,B?{create:ci,activate:ci,remove:function(e,t){!0!==e.data.show?ai(e,t):t()}}:{}].concat(uo)});J&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&yi(e,"input")}));var ui={inserted:function(e,t,n,o){"select"===n.tag?(o.elm&&!o.elm._vOptions?lt(n,"postpatch",(function(){ui.componentUpdated(e,t,n)})):hi(e,t,n.context),e._vOptions=[].map.call(e.options,mi)):("textarea"===n.tag||Kn(e.type))&&(e._vModifiers=t.modifiers,t.modifiers.lazy||(e.addEventListener("compositionstart",wi),e.addEventListener("compositionend",gi),e.addEventListener("change",gi),J&&(e.vmodel=!0)))},componentUpdated:function(e,t,n){if("select"===n.tag){hi(e,t,n.context);var o=e._vOptions,i=e._vOptions=[].map.call(e.options,mi);if(i.some((function(e,t){return!R(e,o[t])})))(e.multiple?t.value.some((function(e){return pi(e,i)})):t.value!==t.oldValue&&pi(t.value,i))&&yi(e,"change")}}};function hi(e,t,n){fi(e,t,n),(K||X)&&setTimeout((function(){fi(e,t,n)}),0)}function fi(e,t,n){var o=t.value,i=e.multiple;if(!i||Array.isArray(o)){for(var r,a,s=0,l=e.options.length;s<l;s++)if(a=e.options[s],i)r=D(o,mi(a))>-1,a.selected!==r&&(a.selected=r);else if(R(mi(a),o))return void(e.selectedIndex!==s&&(e.selectedIndex=s));i||(e.selectedIndex=-1)}}function pi(e,t){return t.every((function(t){return!R(t,e)}))}function mi(e){return"_value"in e?e._value:e.value}function wi(e){e.target.composing=!0}function gi(e){e.target.composing&&(e.target.composing=!1,yi(e.target,"input"))}function yi(e,t){var n=document.createEvent("HTMLEvents");n.initEvent(t,!0,!0),e.dispatchEvent(n)}function vi(e){return!e.componentInstance||e.data&&e.data.transition?e:vi(e.componentInstance._vnode)}var bi={model:ui,show:{bind:function(e,t,n){var o=t.value,i=(n=vi(n)).data&&n.data.transition,r=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;o&&i?(n.data.show=!0,ri(n,(function(){e.style.display=r}))):e.style.display=o?r:"none"},update:function(e,t,n){var o=t.value;!o!=!t.oldValue&&((n=vi(n)).data&&n.data.transition?(n.data.show=!0,o?ri(n,(function(){e.style.display=e.__vOriginalDisplay})):ai(n,(function(){e.style.display="none"}))):e.style.display=o?e.__vOriginalDisplay:"none")},unbind:function(e,t,n,o,i){i||(e.style.display=e.__vOriginalDisplay)}}},ki={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function xi(e){var t=e&&e.componentOptions;return t&&t.Ctor.options.abstract?xi(Ut(t.children)):e}function Si(e){var t={},n=e.$options;for(var o in n.propsData)t[o]=e[o];var i=n._parentListeners;for(var r in i)t[C(r)]=i[r];return t}function Ci(e,t){if(/\d-keep-alive$/.test(t.tag))return e("keep-alive",{props:t.componentOptions.propsData})}var Ti=function(e){return e.tag||$t(e)},Ii=function(e){return"show"===e.name},Ai={name:"transition",props:ki,abstract:!0,render:function(e){var t=this,n=this.$slots.default;if(n&&(n=n.filter(Ti)).length){0;var o=this.mode;0;var i=n[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return i;var r=xi(i);if(!r)return i;if(this._leaving)return Ci(e,i);var a="__transition-"+this._uid+"-";r.key=null==r.key?r.isComment?a+"comment":a+r.tag:l(r.key)?0===String(r.key).indexOf(a)?r.key:a+r.key:r.key;var s=(r.data||(r.data={})).transition=Si(this),c=this._vnode,d=xi(c);if(r.data.directives&&r.data.directives.some(Ii)&&(r.data.show=!0),d&&d.data&&!function(e,t){return t.key===e.key&&t.tag===e.tag}(r,d)&&!$t(d)&&(!d.componentInstance||!d.componentInstance._vnode.isComment)){var u=d.data.transition=j({},s);if("out-in"===o)return this._leaving=!0,lt(u,"afterLeave",(function(){t._leaving=!1,t.$forceUpdate()})),Ci(e,i);if("in-out"===o){if($t(r))return c;var h,f=function(){h()};lt(s,"afterEnter",f),lt(s,"enterCancelled",f),lt(u,"delayLeave",(function(e){h=e}))}}return i}}},_i=j({tag:String,moveClass:String},ki);function Ei(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function ji(e){e.data.newPos=e.elm.getBoundingClientRect()}function Oi(e){var t=e.data.pos,n=e.data.newPos,o=t.left-n.left,i=t.top-n.top;if(o||i){e.data.moved=!0;var r=e.elm.style;r.transform=r.WebkitTransform="translate("+o+"px,"+i+"px)",r.transitionDuration="0s"}}delete _i.mode;var Wi={Transition:Ai,TransitionGroup:{props:_i,beforeMount:function(){var e=this,t=this._update;this._update=function(n,o){var i=Jt(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,i(),t.call(e,n,o)}},render:function(e){for(var t=this.tag||this.$vnode.data.tag||"span",n=Object.create(null),o=this.prevChildren=this.children,i=this.$slots.default||[],r=this.children=[],a=Si(this),s=0;s<i.length;s++){var l=i[s];if(l.tag)if(null!=l.key&&0!==String(l.key).indexOf("__vlist"))r.push(l),n[l.key]=l,(l.data||(l.data={})).transition=a;else;}if(o){for(var c=[],d=[],u=0;u<o.length;u++){var h=o[u];h.data.transition=a,h.data.pos=h.elm.getBoundingClientRect(),n[h.key]?c.push(h):d.push(h)}this.kept=e(t,null,c),this.removed=d}return e(t,null,r)},updated:function(){var e=this.prevChildren,t=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,t)&&(e.forEach(Ei),e.forEach(ji),e.forEach(Oi),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var n=e.elm,o=n.style;Xo(n,t),o.transform=o.WebkitTransform=o.transitionDuration="",n.addEventListener(Vo,n._moveCb=function e(o){o&&o.target!==n||o&&!/transform$/.test(o.propertyName)||(n.removeEventListener(Vo,e),n._moveCb=null,Zo(n,t))})}})))},methods:{hasMove:function(e,t){if(!Uo)return!1;if(this._hasMove)return this._hasMove;var n=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){Go(n,e)})),Mo(n,t),n.style.display="none",this.$el.appendChild(n);var o=ni(n);return this.$el.removeChild(n),this._hasMove=o.hasTransform}}}};Cn.config.mustUseProp=function(e,t,n){return"value"===n&&Pn(e)&&"button"!==t||"selected"===n&&"option"===e||"checked"===n&&"input"===e||"muted"===n&&"video"===e},Cn.config.isReservedTag=Qn,Cn.config.isReservedAttr=Wn,Cn.config.getTagNamespace=function(e){return Vn(e)?"svg":"math"===e?"math":void 0},Cn.config.isUnknownElement=function(e){if(!B)return!0;if(Qn(e))return!1;if(e=e.toLowerCase(),null!=Yn[e])return Yn[e];var t=document.createElement(e);return e.indexOf("-")>-1?Yn[e]=t.constructor===window.HTMLUnknownElement||t.constructor===window.HTMLElement:Yn[e]=/HTMLUnknownElement/.test(t.toString())},j(Cn.options.directives,bi),j(Cn.options.components,Wi),Cn.prototype.__patch__=B?di:W,Cn.prototype.$mount=function(e,t){return function(e,t,n){var o;return e.$el=t,e.$options.render||(e.$options.render=ge),en(e,"beforeMount"),o=function(){e._update(e._render(),n)},new fn(e,o,W,{before:function(){e._isMounted&&!e._isDestroyed&&en(e,"beforeUpdate")}},!0),n=!1,null==e.$vnode&&(e._isMounted=!0,en(e,"mounted")),e}(this,e=e&&B?function(e){if("string"==typeof e){var t=document.querySelector(e);return t||document.createElement("div")}return e}(e):void 0,t)},B&&setTimeout((function(){L.devtools&&re&&re.emit("init",Cn)}),0);var Pi=Cn;
/*!
  * vue-router v3.4.8
  * (c) 2020 Evan You
  * @license MIT
  */function qi(e,t){for(var n in t)e[n]=t[n];return e}var Ri=/[!'()*]/g,Di=function(e){return"%"+e.charCodeAt(0).toString(16)},Ni=/%2C/g,zi=function(e){return encodeURIComponent(e).replace(Ri,Di).replace(Ni,",")};function Fi(e){try{return decodeURIComponent(e)}catch(e){0}return e}var Li=function(e){return null==e||"object"==typeof e?e:String(e)};function Mi(e){var t={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var n=e.replace(/\+/g," ").split("="),o=Fi(n.shift()),i=n.length>0?Fi(n.join("=")):null;void 0===t[o]?t[o]=i:Array.isArray(t[o])?t[o].push(i):t[o]=[t[o],i]})),t):t}function Gi(e){var t=e?Object.keys(e).map((function(t){var n=e[t];if(void 0===n)return"";if(null===n)return zi(t);if(Array.isArray(n)){var o=[];return n.forEach((function(e){void 0!==e&&(null===e?o.push(zi(t)):o.push(zi(t)+"="+zi(e)))})),o.join("&")}return zi(t)+"="+zi(n)})).filter((function(e){return e.length>0})).join("&"):null;return t?"?"+t:""}var Hi=/\/?$/;function $i(e,t,n,o){var i=o&&o.options.stringifyQuery,r=t.query||{};try{r=Ui(r)}catch(e){}var a={name:t.name||e&&e.name,meta:e&&e.meta||{},path:t.path||"/",hash:t.hash||"",query:r,params:t.params||{},fullPath:Qi(t,i),matched:e?Vi(e):[]};return n&&(a.redirectedFrom=Qi(n,i)),Object.freeze(a)}function Ui(e){if(Array.isArray(e))return e.map(Ui);if(e&&"object"==typeof e){var t={};for(var n in e)t[n]=Ui(e[n]);return t}return e}var Bi=$i(null,{path:"/"});function Vi(e){for(var t=[];e;)t.unshift(e),e=e.parent;return t}function Qi(e,t){var n=e.path,o=e.query;void 0===o&&(o={});var i=e.hash;return void 0===i&&(i=""),(n||"/")+(t||Gi)(o)+i}function Yi(e,t){return t===Bi?e===t:!!t&&(e.path&&t.path?e.path.replace(Hi,"")===t.path.replace(Hi,"")&&e.hash===t.hash&&Ki(e.query,t.query):!(!e.name||!t.name)&&(e.name===t.name&&e.hash===t.hash&&Ki(e.query,t.query)&&Ki(e.params,t.params)))}function Ki(e,t){if(void 0===e&&(e={}),void 0===t&&(t={}),!e||!t)return e===t;var n=Object.keys(e).sort(),o=Object.keys(t).sort();return n.length===o.length&&n.every((function(n,i){var r=e[n];if(o[i]!==n)return!1;var a=t[n];return null==r||null==a?r===a:"object"==typeof r&&"object"==typeof a?Ki(r,a):String(r)===String(a)}))}function Ji(e){for(var t=0;t<e.matched.length;t++){var n=e.matched[t];for(var o in n.instances){var i=n.instances[o],r=n.enteredCbs[o];if(i&&r){delete n.enteredCbs[o];for(var a=0;a<r.length;a++)i._isBeingDestroyed||r[a](i)}}}}var Xi={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,t){var n=t.props,o=t.children,i=t.parent,r=t.data;r.routerView=!0;for(var a=i.$createElement,s=n.name,l=i.$route,c=i._routerViewCache||(i._routerViewCache={}),d=0,u=!1;i&&i._routerRoot!==i;){var h=i.$vnode?i.$vnode.data:{};h.routerView&&d++,h.keepAlive&&i._directInactive&&i._inactive&&(u=!0),i=i.$parent}if(r.routerViewDepth=d,u){var f=c[s],p=f&&f.component;return p?(f.configProps&&Zi(p,r,f.route,f.configProps),a(p,r,o)):a()}var m=l.matched[d],w=m&&m.components[s];if(!m||!w)return c[s]=null,a();c[s]={component:w},r.registerRouteInstance=function(e,t){var n=m.instances[s];(t&&n!==e||!t&&n===e)&&(m.instances[s]=t)},(r.hook||(r.hook={})).prepatch=function(e,t){m.instances[s]=t.componentInstance},r.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==m.instances[s]&&(m.instances[s]=e.componentInstance),Ji(l)};var g=m.props&&m.props[s];return g&&(qi(c[s],{route:l,configProps:g}),Zi(w,r,l,g)),a(w,r,o)}};function Zi(e,t,n,o){var i=t.props=function(e,t){switch(typeof t){case"undefined":return;case"object":return t;case"function":return t(e);case"boolean":return t?e.params:void 0;default:0}}(n,o);if(i){i=t.props=qi({},i);var r=t.attrs=t.attrs||{};for(var a in i)e.props&&a in e.props||(r[a]=i[a],delete i[a])}}function er(e,t,n){var o=e.charAt(0);if("/"===o)return e;if("?"===o||"#"===o)return t+e;var i=t.split("/");n&&i[i.length-1]||i.pop();for(var r=e.replace(/^\//,"").split("/"),a=0;a<r.length;a++){var s=r[a];".."===s?i.pop():"."!==s&&i.push(s)}return""!==i[0]&&i.unshift(""),i.join("/")}function tr(e){return e.replace(/\/\//g,"/")}var nr=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},or=gr,ir=cr,rr=function(e,t){return ur(cr(e,t),t)},ar=ur,sr=wr,lr=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function cr(e,t){for(var n,o=[],i=0,r=0,a="",s=t&&t.delimiter||"/";null!=(n=lr.exec(e));){var l=n[0],c=n[1],d=n.index;if(a+=e.slice(r,d),r=d+l.length,c)a+=c[1];else{var u=e[r],h=n[2],f=n[3],p=n[4],m=n[5],w=n[6],g=n[7];a&&(o.push(a),a="");var y=null!=h&&null!=u&&u!==h,v="+"===w||"*"===w,b="?"===w||"*"===w,k=n[2]||s,x=p||m;o.push({name:f||i++,prefix:h||"",delimiter:k,optional:b,repeat:v,partial:y,asterisk:!!g,pattern:x?fr(x):g?".*":"[^"+hr(k)+"]+?"})}}return r<e.length&&(a+=e.substr(r)),a&&o.push(a),o}function dr(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function ur(e,t){for(var n=new Array(e.length),o=0;o<e.length;o++)"object"==typeof e[o]&&(n[o]=new RegExp("^(?:"+e[o].pattern+")$",mr(t)));return function(t,o){for(var i="",r=t||{},a=(o||{}).pretty?dr:encodeURIComponent,s=0;s<e.length;s++){var l=e[s];if("string"!=typeof l){var c,d=r[l.name];if(null==d){if(l.optional){l.partial&&(i+=l.prefix);continue}throw new TypeError('Expected "'+l.name+'" to be defined')}if(nr(d)){if(!l.repeat)throw new TypeError('Expected "'+l.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(l.optional)continue;throw new TypeError('Expected "'+l.name+'" to not be empty')}for(var u=0;u<d.length;u++){if(c=a(d[u]),!n[s].test(c))throw new TypeError('Expected all "'+l.name+'" to match "'+l.pattern+'", but received `'+JSON.stringify(c)+"`");i+=(0===u?l.prefix:l.delimiter)+c}}else{if(c=l.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):a(d),!n[s].test(c))throw new TypeError('Expected "'+l.name+'" to match "'+l.pattern+'", but received "'+c+'"');i+=l.prefix+c}}else i+=l}return i}}function hr(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function fr(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function pr(e,t){return e.keys=t,e}function mr(e){return e&&e.sensitive?"":"i"}function wr(e,t,n){nr(t)||(n=t||n,t=[]);for(var o=(n=n||{}).strict,i=!1!==n.end,r="",a=0;a<e.length;a++){var s=e[a];if("string"==typeof s)r+=hr(s);else{var l=hr(s.prefix),c="(?:"+s.pattern+")";t.push(s),s.repeat&&(c+="(?:"+l+c+")*"),r+=c=s.optional?s.partial?l+"("+c+")?":"(?:"+l+"("+c+"))?":l+"("+c+")"}}var d=hr(n.delimiter||"/"),u=r.slice(-d.length)===d;return o||(r=(u?r.slice(0,-d.length):r)+"(?:"+d+"(?=$))?"),r+=i?"$":o&&u?"":"(?="+d+"|$)",pr(new RegExp("^"+r,mr(n)),t)}function gr(e,t,n){return nr(t)||(n=t||n,t=[]),n=n||{},e instanceof RegExp?function(e,t){var n=e.source.match(/\((?!\?)/g);if(n)for(var o=0;o<n.length;o++)t.push({name:o,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return pr(e,t)}(e,t):nr(e)?function(e,t,n){for(var o=[],i=0;i<e.length;i++)o.push(gr(e[i],t,n).source);return pr(new RegExp("(?:"+o.join("|")+")",mr(n)),t)}(e,t,n):function(e,t,n){return wr(cr(e,n),t,n)}(e,t,n)}or.parse=ir,or.compile=rr,or.tokensToFunction=ar,or.tokensToRegExp=sr;var yr=Object.create(null);function vr(e,t,n){t=t||{};try{var o=yr[e]||(yr[e]=or.compile(e));return"string"==typeof t.pathMatch&&(t[0]=t.pathMatch),o(t,{pretty:!0})}catch(e){return""}finally{delete t[0]}}function br(e,t,n,o){var i="string"==typeof e?{path:e}:e;if(i._normalized)return i;if(i.name){var r=(i=qi({},e)).params;return r&&"object"==typeof r&&(i.params=qi({},r)),i}if(!i.path&&i.params&&t){(i=qi({},i))._normalized=!0;var a=qi(qi({},t.params),i.params);if(t.name)i.name=t.name,i.params=a;else if(t.matched.length){var s=t.matched[t.matched.length-1].path;i.path=vr(s,a,t.path)}else 0;return i}var l=function(e){var t="",n="",o=e.indexOf("#");o>=0&&(t=e.slice(o),e=e.slice(0,o));var i=e.indexOf("?");return i>=0&&(n=e.slice(i+1),e=e.slice(0,i)),{path:e,query:n,hash:t}}(i.path||""),c=t&&t.path||"/",d=l.path?er(l.path,c,n||i.append):c,u=function(e,t,n){void 0===t&&(t={});var o,i=n||Mi;try{o=i(e||"")}catch(e){o={}}for(var r in t){var a=t[r];o[r]=Array.isArray(a)?a.map(Li):Li(a)}return o}(l.query,i.query,o&&o.options.parseQuery),h=i.hash||l.hash;return h&&"#"!==h.charAt(0)&&(h="#"+h),{_normalized:!0,path:d,query:u,hash:h}}var kr,xr=function(){},Sr={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},exact:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var t=this,n=this.$router,o=this.$route,i=n.resolve(this.to,o,this.append),r=i.location,a=i.route,s=i.href,l={},c=n.options.linkActiveClass,d=n.options.linkExactActiveClass,u=null==c?"router-link-active":c,h=null==d?"router-link-exact-active":d,f=null==this.activeClass?u:this.activeClass,p=null==this.exactActiveClass?h:this.exactActiveClass,m=a.redirectedFrom?$i(null,br(a.redirectedFrom),null,n):a;l[p]=Yi(o,m),l[f]=this.exact?l[p]:function(e,t){return 0===e.path.replace(Hi,"/").indexOf(t.path.replace(Hi,"/"))&&(!t.hash||e.hash===t.hash)&&function(e,t){for(var n in t)if(!(n in e))return!1;return!0}(e.query,t.query)}(o,m);var w=l[p]?this.ariaCurrentValue:null,g=function(e){Cr(e)&&(t.replace?n.replace(r,xr):n.push(r,xr))},y={click:Cr};Array.isArray(this.event)?this.event.forEach((function(e){y[e]=g})):y[this.event]=g;var v={class:l},b=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:s,route:a,navigate:g,isActive:l[f],isExactActive:l[p]});if(b){if(1===b.length)return b[0];if(b.length>1||!b.length)return 0===b.length?e():e("span",{},b)}if("a"===this.tag)v.on=y,v.attrs={href:s,"aria-current":w};else{var k=function e(t){var n;if(t)for(var o=0;o<t.length;o++){if("a"===(n=t[o]).tag)return n;if(n.children&&(n=e(n.children)))return n}}(this.$slots.default);if(k){k.isStatic=!1;var x=k.data=qi({},k.data);for(var S in x.on=x.on||{},x.on){var C=x.on[S];S in y&&(x.on[S]=Array.isArray(C)?C:[C])}for(var T in y)T in x.on?x.on[T].push(y[T]):x.on[T]=g;var I=k.data.attrs=qi({},k.data.attrs);I.href=s,I["aria-current"]=w}else v.on=y}return e(this.tag,v,this.$slots.default)}};function Cr(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var t=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(t))return}return e.preventDefault&&e.preventDefault(),!0}}var Tr="undefined"!=typeof window;function Ir(e,t,n,o){var i=t||[],r=n||Object.create(null),a=o||Object.create(null);e.forEach((function(e){!function e(t,n,o,i,r,a){var s=i.path,l=i.name;0;var c=i.pathToRegexpOptions||{},d=function(e,t,n){n||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==t)return e;return tr(t.path+"/"+e)}(s,r,c.strict);"boolean"==typeof i.caseSensitive&&(c.sensitive=i.caseSensitive);var u={path:d,regex:Ar(d,c),components:i.components||{default:i.component},instances:{},enteredCbs:{},name:l,parent:r,matchAs:a,redirect:i.redirect,beforeEnter:i.beforeEnter,meta:i.meta||{},props:null==i.props?{}:i.components?i.props:{default:i.props}};i.children&&i.children.forEach((function(i){var r=a?tr(a+"/"+i.path):void 0;e(t,n,o,i,u,r)}));n[u.path]||(t.push(u.path),n[u.path]=u);if(void 0!==i.alias)for(var h=Array.isArray(i.alias)?i.alias:[i.alias],f=0;f<h.length;++f){0;var p={path:h[f],children:i.children};e(t,n,o,p,r,u.path||"/")}l&&(o[l]||(o[l]=u))}(i,r,a,e)}));for(var s=0,l=i.length;s<l;s++)"*"===i[s]&&(i.push(i.splice(s,1)[0]),l--,s--);return{pathList:i,pathMap:r,nameMap:a}}function Ar(e,t){return or(e,[],t)}function _r(e,t){var n=Ir(e),o=n.pathList,i=n.pathMap,r=n.nameMap;function a(e,n,a){var s=br(e,n,!1,t),c=s.name;if(c){var d=r[c];if(!d)return l(null,s);var u=d.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof s.params&&(s.params={}),n&&"object"==typeof n.params)for(var h in n.params)!(h in s.params)&&u.indexOf(h)>-1&&(s.params[h]=n.params[h]);return s.path=vr(d.path,s.params),l(d,s,a)}if(s.path){s.params={};for(var f=0;f<o.length;f++){var p=o[f],m=i[p];if(Er(m.regex,s.path,s.params))return l(m,s,a)}}return l(null,s)}function s(e,n){var o=e.redirect,i="function"==typeof o?o($i(e,n,null,t)):o;if("string"==typeof i&&(i={path:i}),!i||"object"!=typeof i)return l(null,n);var s=i,c=s.name,d=s.path,u=n.query,h=n.hash,f=n.params;if(u=s.hasOwnProperty("query")?s.query:u,h=s.hasOwnProperty("hash")?s.hash:h,f=s.hasOwnProperty("params")?s.params:f,c){r[c];return a({_normalized:!0,name:c,query:u,hash:h,params:f},void 0,n)}if(d){var p=function(e,t){return er(e,t.parent?t.parent.path:"/",!0)}(d,e);return a({_normalized:!0,path:vr(p,f),query:u,hash:h},void 0,n)}return l(null,n)}function l(e,n,o){return e&&e.redirect?s(e,o||n):e&&e.matchAs?function(e,t,n){var o=a({_normalized:!0,path:vr(n,t.params)});if(o){var i=o.matched,r=i[i.length-1];return t.params=o.params,l(r,t)}return l(null,t)}(0,n,e.matchAs):$i(e,n,o,t)}return{match:a,addRoutes:function(e){Ir(e,o,i,r)}}}function Er(e,t,n){try{t=decodeURI(t)}catch(e){0}var o=t.match(e);if(!o)return!1;if(!n)return!0;for(var i=1,r=o.length;i<r;++i){var a=e.keys[i-1];a&&(n[a.name||"pathMatch"]=o[i])}return!0}var jr=Tr&&window.performance&&window.performance.now?window.performance:Date;function Or(){return jr.now().toFixed(3)}var Wr=Or();function Pr(){return Wr}function qr(e){return Wr=e}var Rr=Object.create(null);function Dr(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,t=window.location.href.replace(e,""),n=qi({},window.history.state);return n.key=Pr(),window.history.replaceState(n,"",t),window.addEventListener("popstate",Fr),function(){window.removeEventListener("popstate",Fr)}}function Nr(e,t,n,o){if(e.app){var i=e.options.scrollBehavior;i&&e.app.$nextTick((function(){var r=function(){var e=Pr();if(e)return Rr[e]}(),a=i.call(e,t,n,o?r:null);a&&("function"==typeof a.then?a.then((function(e){$r(e,r)})).catch((function(e){0})):$r(a,r))}))}}function zr(){var e=Pr();e&&(Rr[e]={x:window.pageXOffset,y:window.pageYOffset})}function Fr(e){zr(),e.state&&e.state.key&&qr(e.state.key)}function Lr(e){return Gr(e.x)||Gr(e.y)}function Mr(e){return{x:Gr(e.x)?e.x:window.pageXOffset,y:Gr(e.y)?e.y:window.pageYOffset}}function Gr(e){return"number"==typeof e}var Hr=/^#\d/;function $r(e,t){var n,o="object"==typeof e;if(o&&"string"==typeof e.selector){var i=Hr.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(i){var r=e.offset&&"object"==typeof e.offset?e.offset:{};t=function(e,t){var n=document.documentElement.getBoundingClientRect(),o=e.getBoundingClientRect();return{x:o.left-n.left-t.x,y:o.top-n.top-t.y}}(i,r={x:Gr((n=r).x)?n.x:0,y:Gr(n.y)?n.y:0})}else Lr(e)&&(t=Mr(e))}else o&&Lr(e)&&(t=Mr(e));t&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:t.x,top:t.y,behavior:e.behavior}):window.scrollTo(t.x,t.y))}var Ur,Br=Tr&&((-1===(Ur=window.navigator.userAgent).indexOf("Android 2.")&&-1===Ur.indexOf("Android 4.0")||-1===Ur.indexOf("Mobile Safari")||-1!==Ur.indexOf("Chrome")||-1!==Ur.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Vr(e,t){zr();var n=window.history;try{if(t){var o=qi({},n.state);o.key=Pr(),n.replaceState(o,"",e)}else n.pushState({key:qr(Or())},"",e)}catch(n){window.location[t?"replace":"assign"](e)}}function Qr(e){Vr(e,!0)}function Yr(e,t,n){var o=function(i){i>=e.length?n():e[i]?t(e[i],(function(){o(i+1)})):o(i+1)};o(0)}var Kr={redirected:2,aborted:4,cancelled:8,duplicated:16};function Jr(e,t){return Zr(e,t,Kr.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var t={};return ea.forEach((function(n){n in e&&(t[n]=e[n])})),JSON.stringify(t,null,2)}(t)+'" via a navigation guard.')}function Xr(e,t){return Zr(e,t,Kr.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+t.fullPath+'" with a new navigation.')}function Zr(e,t,n,o){var i=new Error(o);return i._isRouter=!0,i.from=e,i.to=t,i.type=n,i}var ea=["params","query","hash"];function ta(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function na(e,t){return ta(e)&&e._isRouter&&(null==t||e.type===t)}function oa(e){return function(t,n,o){var i=!1,r=0,a=null;ia(e,(function(e,t,n,s){if("function"==typeof e&&void 0===e.cid){i=!0,r++;var l,c=sa((function(t){var i;((i=t).__esModule||aa&&"Module"===i[Symbol.toStringTag])&&(t=t.default),e.resolved="function"==typeof t?t:kr.extend(t),n.components[s]=t,--r<=0&&o()})),d=sa((function(e){var t="Failed to resolve async component "+s+": "+e;a||(a=ta(e)?e:new Error(t),o(a))}));try{l=e(c,d)}catch(e){d(e)}if(l)if("function"==typeof l.then)l.then(c,d);else{var u=l.component;u&&"function"==typeof u.then&&u.then(c,d)}}})),i||o()}}function ia(e,t){return ra(e.map((function(e){return Object.keys(e.components).map((function(n){return t(e.components[n],e.instances[n],e,n)}))})))}function ra(e){return Array.prototype.concat.apply([],e)}var aa="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function sa(e){var t=!1;return function(){for(var n=[],o=arguments.length;o--;)n[o]=arguments[o];if(!t)return t=!0,e.apply(this,n)}}var la=function(e,t){this.router=e,this.base=function(e){if(!e)if(Tr){var t=document.querySelector("base");e=(e=t&&t.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(t),this.current=Bi,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function ca(e,t,n,o){var i=ia(e,(function(e,o,i,r){var a=function(e,t){"function"!=typeof e&&(e=kr.extend(e));return e.options[t]}(e,t);if(a)return Array.isArray(a)?a.map((function(e){return n(e,o,i,r)})):n(a,o,i,r)}));return ra(o?i.reverse():i)}function da(e,t){if(t)return function(){return e.apply(t,arguments)}}la.prototype.listen=function(e){this.cb=e},la.prototype.onReady=function(e,t){this.ready?e():(this.readyCbs.push(e),t&&this.readyErrorCbs.push(t))},la.prototype.onError=function(e){this.errorCbs.push(e)},la.prototype.transitionTo=function(e,t,n){var o,i=this;try{o=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(t){t(e)})),e}var r=this.current;this.confirmTransition(o,(function(){i.updateRoute(o),t&&t(o),i.ensureURL(),i.router.afterHooks.forEach((function(e){e&&e(o,r)})),i.ready||(i.ready=!0,i.readyCbs.forEach((function(e){e(o)})))}),(function(e){n&&n(e),e&&!i.ready&&(na(e,Kr.redirected)&&r===Bi||(i.ready=!0,i.readyErrorCbs.forEach((function(t){t(e)}))))}))},la.prototype.confirmTransition=function(e,t,n){var o=this,i=this.current;this.pending=e;var r,a,s=function(e){!na(e)&&ta(e)&&(o.errorCbs.length?o.errorCbs.forEach((function(t){t(e)})):console.error(e)),n&&n(e)},l=e.matched.length-1,c=i.matched.length-1;if(Yi(e,i)&&l===c&&e.matched[l]===i.matched[c])return this.ensureURL(),s(((a=Zr(r=i,e,Kr.duplicated,'Avoided redundant navigation to current location: "'+r.fullPath+'".')).name="NavigationDuplicated",a));var d=function(e,t){var n,o=Math.max(e.length,t.length);for(n=0;n<o&&e[n]===t[n];n++);return{updated:t.slice(0,n),activated:t.slice(n),deactivated:e.slice(n)}}(this.current.matched,e.matched),u=d.updated,h=d.deactivated,f=d.activated,p=[].concat(function(e){return ca(e,"beforeRouteLeave",da,!0)}(h),this.router.beforeHooks,function(e){return ca(e,"beforeRouteUpdate",da)}(u),f.map((function(e){return e.beforeEnter})),oa(f)),m=function(t,n){if(o.pending!==e)return s(Xr(i,e));try{t(e,i,(function(t){!1===t?(o.ensureURL(!0),s(function(e,t){return Zr(e,t,Kr.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+t.fullPath+'" via a navigation guard.')}(i,e))):ta(t)?(o.ensureURL(!0),s(t)):"string"==typeof t||"object"==typeof t&&("string"==typeof t.path||"string"==typeof t.name)?(s(Jr(i,e)),"object"==typeof t&&t.replace?o.replace(t):o.push(t)):n(t)}))}catch(e){s(e)}};Yr(p,m,(function(){Yr(function(e){return ca(e,"beforeRouteEnter",(function(e,t,n,o){return function(e,t,n){return function(o,i,r){return e(o,i,(function(e){"function"==typeof e&&(t.enteredCbs[n]||(t.enteredCbs[n]=[]),t.enteredCbs[n].push(e)),r(e)}))}}(e,n,o)}))}(f).concat(o.router.resolveHooks),m,(function(){if(o.pending!==e)return s(Xr(i,e));o.pending=null,t(e),o.router.app&&o.router.app.$nextTick((function(){Ji(e)}))}))}))},la.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},la.prototype.setupListeners=function(){},la.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=Bi,this.pending=null};var ua=function(e){function t(t,n){e.call(this,t,n),this._startLocation=ha(this.base)}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var t=this.router,n=t.options.scrollBehavior,o=Br&&n;o&&this.listeners.push(Dr());var i=function(){var n=e.current,i=ha(e.base);e.current===Bi&&i===e._startLocation||e.transitionTo(i,(function(e){o&&Nr(t,e,n,!0)}))};window.addEventListener("popstate",i),this.listeners.push((function(){window.removeEventListener("popstate",i)}))}},t.prototype.go=function(e){window.history.go(e)},t.prototype.push=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){Vr(tr(o.base+e.fullPath)),Nr(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){Qr(tr(o.base+e.fullPath)),Nr(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.ensureURL=function(e){if(ha(this.base)!==this.current.fullPath){var t=tr(this.base+this.current.fullPath);e?Vr(t):Qr(t)}},t.prototype.getCurrentLocation=function(){return ha(this.base)},t}(la);function ha(e){var t=window.location.pathname;return e&&0===t.toLowerCase().indexOf(e.toLowerCase())&&(t=t.slice(e.length)),(t||"/")+window.location.search+window.location.hash}var fa=function(e){function t(t,n,o){e.call(this,t,n),o&&function(e){var t=ha(e);if(!/^\/#/.test(t))return window.location.replace(tr(e+"/#"+t)),!0}(this.base)||pa()}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var t=this.router.options.scrollBehavior,n=Br&&t;n&&this.listeners.push(Dr());var o=function(){var t=e.current;pa()&&e.transitionTo(ma(),(function(o){n&&Nr(e.router,o,t,!0),Br||ya(o.fullPath)}))},i=Br?"popstate":"hashchange";window.addEventListener(i,o),this.listeners.push((function(){window.removeEventListener(i,o)}))}},t.prototype.push=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){ga(e.fullPath),Nr(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this,i=this.current;this.transitionTo(e,(function(e){ya(e.fullPath),Nr(o.router,e,i,!1),t&&t(e)}),n)},t.prototype.go=function(e){window.history.go(e)},t.prototype.ensureURL=function(e){var t=this.current.fullPath;ma()!==t&&(e?ga(t):ya(t))},t.prototype.getCurrentLocation=function(){return ma()},t}(la);function pa(){var e=ma();return"/"===e.charAt(0)||(ya("/"+e),!1)}function ma(){var e=window.location.href,t=e.indexOf("#");return t<0?"":e=e.slice(t+1)}function wa(e){var t=window.location.href,n=t.indexOf("#");return(n>=0?t.slice(0,n):t)+"#"+e}function ga(e){Br?Vr(wa(e)):window.location.hash=e}function ya(e){Br?Qr(wa(e)):window.location.replace(wa(e))}var va=function(e){function t(t,n){e.call(this,t,n),this.stack=[],this.index=-1}return e&&(t.__proto__=e),t.prototype=Object.create(e&&e.prototype),t.prototype.constructor=t,t.prototype.push=function(e,t,n){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index+1).concat(e),o.index++,t&&t(e)}),n)},t.prototype.replace=function(e,t,n){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index).concat(e),t&&t(e)}),n)},t.prototype.go=function(e){var t=this,n=this.index+e;if(!(n<0||n>=this.stack.length)){var o=this.stack[n];this.confirmTransition(o,(function(){var e=t.current;t.index=n,t.updateRoute(o),t.router.afterHooks.forEach((function(t){t&&t(o,e)}))}),(function(e){na(e,Kr.duplicated)&&(t.index=n)}))}},t.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},t.prototype.ensureURL=function(){},t}(la),ba=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=_r(e.routes||[],this);var t=e.mode||"hash";switch(this.fallback="history"===t&&!Br&&!1!==e.fallback,this.fallback&&(t="hash"),Tr||(t="abstract"),this.mode=t,t){case"history":this.history=new ua(this,e.base);break;case"hash":this.history=new fa(this,e.base,this.fallback);break;case"abstract":this.history=new va(this,e.base);break;default:0}},ka={currentRoute:{configurable:!0}};function xa(e,t){return e.push(t),function(){var n=e.indexOf(t);n>-1&&e.splice(n,1)}}ba.prototype.match=function(e,t,n){return this.matcher.match(e,t,n)},ka.currentRoute.get=function(){return this.history&&this.history.current},ba.prototype.init=function(e){var t=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var n=t.apps.indexOf(e);n>-1&&t.apps.splice(n,1),t.app===e&&(t.app=t.apps[0]||null),t.app||t.history.teardown()})),!this.app){this.app=e;var n=this.history;if(n instanceof ua||n instanceof fa){var o=function(e){n.setupListeners(),function(e){var o=n.current,i=t.options.scrollBehavior;Br&&i&&"fullPath"in e&&Nr(t,e,o,!1)}(e)};n.transitionTo(n.getCurrentLocation(),o,o)}n.listen((function(e){t.apps.forEach((function(t){t._route=e}))}))}},ba.prototype.beforeEach=function(e){return xa(this.beforeHooks,e)},ba.prototype.beforeResolve=function(e){return xa(this.resolveHooks,e)},ba.prototype.afterEach=function(e){return xa(this.afterHooks,e)},ba.prototype.onReady=function(e,t){this.history.onReady(e,t)},ba.prototype.onError=function(e){this.history.onError(e)},ba.prototype.push=function(e,t,n){var o=this;if(!t&&!n&&"undefined"!=typeof Promise)return new Promise((function(t,n){o.history.push(e,t,n)}));this.history.push(e,t,n)},ba.prototype.replace=function(e,t,n){var o=this;if(!t&&!n&&"undefined"!=typeof Promise)return new Promise((function(t,n){o.history.replace(e,t,n)}));this.history.replace(e,t,n)},ba.prototype.go=function(e){this.history.go(e)},ba.prototype.back=function(){this.go(-1)},ba.prototype.forward=function(){this.go(1)},ba.prototype.getMatchedComponents=function(e){var t=e?e.matched?e:this.resolve(e).route:this.currentRoute;return t?[].concat.apply([],t.matched.map((function(e){return Object.keys(e.components).map((function(t){return e.components[t]}))}))):[]},ba.prototype.resolve=function(e,t,n){var o=br(e,t=t||this.history.current,n,this),i=this.match(o,t),r=i.redirectedFrom||i.fullPath;return{location:o,route:i,href:function(e,t,n){var o="hash"===n?"#"+t:t;return e?tr(e+"/"+o):o}(this.history.base,r,this.mode),normalizedTo:o,resolved:i}},ba.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==Bi&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(ba.prototype,ka),ba.install=function e(t){if(!e.installed||kr!==t){e.installed=!0,kr=t;var n=function(e){return void 0!==e},o=function(e,t){var o=e.$options._parentVnode;n(o)&&n(o=o.data)&&n(o=o.registerRouteInstance)&&o(e,t)};t.mixin({beforeCreate:function(){n(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),t.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,o(this,this)},destroyed:function(){o(this)}}),Object.defineProperty(t.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(t.prototype,"$route",{get:function(){return this._routerRoot._route}}),t.component("RouterView",Xi),t.component("RouterLink",Sr);var i=t.config.optionMergeStrategies;i.beforeRouteEnter=i.beforeRouteLeave=i.beforeRouteUpdate=i.created}},ba.version="3.4.8",ba.isNavigationFailure=na,ba.NavigationFailureType=Kr,Tr&&window.Vue&&window.Vue.use(ba);var Sa=ba;n(51),n(211),n(213),n(148),n(149),n(73),n(215),n(52);function Ca(e){e.locales&&Object.keys(e.locales).forEach((function(t){e.locales[t].path=t})),Object.freeze(e)}n(193),n(152),n(23),n(195),n(46),n(42),n(63),n(92);function Ta(e){return(Ta="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}var Ia=n(44),Aa={NotFound:function(){return n.e(7).then(n.bind(null,388))},Layout:function(){return Promise.all([n.e(0),n.e(2)]).then(n.bind(null,387))}},_a={"v-1c019fb2":function(){return n.e(9).then(n.bind(null,393))},"v-674c2878":function(){return n.e(10).then(n.bind(null,394))},"v-34b90f42":function(){return n.e(11).then(n.bind(null,395))},"v-6190b03c":function(){return n.e(12).then(n.bind(null,396))},"v-0403e040":function(){return n.e(13).then(n.bind(null,397))},"v-00cc835b":function(){return n.e(14).then(n.bind(null,398))},"v-da0a32bc":function(){return n.e(15).then(n.bind(null,399))},"v-1de42eb0":function(){return n.e(16).then(n.bind(null,400))},"v-8cf1c7ac":function(){return n.e(17).then(n.bind(null,401))},"v-7215aabc":function(){return n.e(18).then(n.bind(null,402))},"v-1b6ed5fc":function(){return n.e(19).then(n.bind(null,403))},"v-2044e7d6":function(){return n.e(20).then(n.bind(null,404))},"v-7557d6c2":function(){return n.e(21).then(n.bind(null,405))},"v-7bd56aa2":function(){return n.e(22).then(n.bind(null,406))},"v-194a2c22":function(){return n.e(23).then(n.bind(null,407))},"v-142b1af4":function(){return n.e(24).then(n.bind(null,408))},"v-a9abb788":function(){return n.e(25).then(n.bind(null,409))},"v-a0c27e3c":function(){return n.e(26).then(n.bind(null,410))},"v-a16b91ca":function(){return n.e(27).then(n.bind(null,411))},"v-13e86a42":function(){return n.e(28).then(n.bind(null,412))},"v-006b104c":function(){return n.e(29).then(n.bind(null,413))},"v-4b893278":function(){return n.e(30).then(n.bind(null,414))},"v-7d1ab062":function(){return n.e(31).then(n.bind(null,415))},"v-20231a48":function(){return n.e(32).then(n.bind(null,416))},"v-2cd6f7bc":function(){return n.e(33).then(n.bind(null,417))},"v-f2d516bc":function(){return n.e(34).then(n.bind(null,418))},"v-071004ea":function(){return n.e(35).then(n.bind(null,419))},"v-0f936964":function(){return n.e(36).then(n.bind(null,420))},"v-496bb1df":function(){return n.e(37).then(n.bind(null,421))},"v-211cf634":function(){return n.e(38).then(n.bind(null,422))},"v-3d11f802":function(){return n.e(39).then(n.bind(null,423))},"v-264670c2":function(){return n.e(40).then(n.bind(null,424))},"v-11997e3c":function(){return n.e(41).then(n.bind(null,425))},"v-5a20c23c":function(){return n.e(42).then(n.bind(null,426))},"v-056557ea":function(){return n.e(43).then(n.bind(null,427))},"v-ee26987c":function(){return n.e(44).then(n.bind(null,428))},"v-5951033c":function(){return n.e(45).then(n.bind(null,429))},"v-8f63b5a0":function(){return n.e(46).then(n.bind(null,430))},"v-3e031ed8":function(){return n.e(47).then(n.bind(null,431))},"v-8e66acc0":function(){return n.e(48).then(n.bind(null,432))},"v-596ec15e":function(){return n.e(49).then(n.bind(null,433))},"v-5fdea0a2":function(){return n.e(50).then(n.bind(null,434))},"v-e894b2bc":function(){return n.e(51).then(n.bind(null,435))},"v-0463910e":function(){return n.e(52).then(n.bind(null,436))},"v-5ee76854":function(){return n.e(53).then(n.bind(null,437))},"v-45b94840":function(){return n.e(54).then(n.bind(null,438))},"v-389be5ec":function(){return n.e(55).then(n.bind(null,439))},"v-651b4e0a":function(){return n.e(56).then(n.bind(null,440))},"v-e9a63714":function(){return n.e(57).then(n.bind(null,441))},"v-d91dcabc":function(){return n.e(58).then(n.bind(null,442))},"v-241aa182":function(){return n.e(59).then(n.bind(null,443))},"v-7109c462":function(){return n.e(60).then(n.bind(null,444))},"v-30ee6212":function(){return n.e(61).then(n.bind(null,445))},"v-63bf2e6c":function(){return n.e(62).then(n.bind(null,446))},"v-103bbcbc":function(){return n.e(63).then(n.bind(null,447))},"v-65ea467c":function(){return n.e(64).then(n.bind(null,448))},"v-3c7b0dd4":function(){return n.e(66).then(n.bind(null,449))},"v-b60e670c":function(){return n.e(65).then(n.bind(null,450))},"v-a558de54":function(){return n.e(67).then(n.bind(null,451))},"v-5b7bae8a":function(){return n.e(68).then(n.bind(null,452))},"v-722c1fc2":function(){return n.e(69).then(n.bind(null,453))},"v-957246a8":function(){return n.e(70).then(n.bind(null,454))},"v-389752bc":function(){return n.e(71).then(n.bind(null,455))},"v-0c11d262":function(){return n.e(72).then(n.bind(null,456))},"v-a139e6dc":function(){return n.e(73).then(n.bind(null,457))},"v-0f2e8980":function(){return n.e(74).then(n.bind(null,458))},"v-2d3e7cdb":function(){return n.e(75).then(n.bind(null,459))},"v-2dfd6d7b":function(){return n.e(76).then(n.bind(null,460))},"v-94771a14":function(){return n.e(77).then(n.bind(null,461))},"v-3cd8d962":function(){return n.e(78).then(n.bind(null,462))},"v-6d823dbc":function(){return n.e(79).then(n.bind(null,463))},"v-20490294":function(){return n.e(80).then(n.bind(null,464))},"v-681aeaca":function(){return n.e(81).then(n.bind(null,465))},"v-4997885e":function(){return n.e(82).then(n.bind(null,466))},"v-f7dd2f4a":function(){return n.e(83).then(n.bind(null,467))},"v-0615a98a":function(){return n.e(84).then(n.bind(null,468))}};function Ea(e){var t=Object.create(null);return function(n){return t[n]||(t[n]=e(n))}}var ja=/-(\w)/g,Oa=Ea((function(e){return e.replace(ja,(function(e,t){return t?t.toUpperCase():""}))})),Wa=/\B([A-Z])/g,Pa=Ea((function(e){return e.replace(Wa,"-$1").toLowerCase()})),qa=Ea((function(e){return e.charAt(0).toUpperCase()+e.slice(1)}));function Ra(e,t){if(t)return e(t)?e(t):t.includes("-")?e(qa(Oa(t))):e(qa(t))||e(Pa(t))}var Da=Object.assign({},Aa,_a),Na=function(e){return Da[e]},za=function(e){return _a[e]},Fa=function(e){return Aa[e]},La=function(e){return Pi.component(e)};function Ma(e){return Ra(za,e)}function Ga(e){return Ra(Fa,e)}function Ha(e){return Ra(Na,e)}function $a(e){return Ra(La,e)}function Ua(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return Promise.all(t.filter((function(e){return e})).map(function(){var e=Object(o.a)(regeneratorRuntime.mark((function e(t){var n;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:if($a(t)||!Ha(t)){e.next=5;break}return e.next=3,Ha(t)();case 3:n=e.sent,Pi.component(t,n.default);case 5:case"end":return e.stop()}}),e)})));return function(t){return e.apply(this,arguments)}}()))}function Ba(e,t){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=t)}n(53),n(74);var Va=n(103),Qa=n(185),Ya=n.n(Qa),Ka={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(e){return"meta"===Object(Va.a)(e,1)[0]})).map((function(e){var t=Object(Va.a)(e,2);t[0];return t[1]})),this.$ssrContext){var e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(t=e)?t.map((function(e){var t="<meta";return Object.keys(e).forEach((function(n){t+=" ".concat(n,'="').concat(e[n],'"')})),t+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=Xa(this.$canonicalUrl)}var t},mounted:function(){this.currentMetaTags=Object(Ia.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var e=this.getMergedMetaTags();this.currentMetaTags=Za(e,this.currentMetaTags)},getMergedMetaTags:function(){var e=this.$page.frontmatter.meta||[];return Ya()([{name:"description",content:this.$description}],e,this.siteMeta,es)},updateCanonicalLink:function(){Ja(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",Xa(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){Za(null,this.currentMetaTags),Ja()}};function Ja(){var e=document.querySelector("link[rel='canonical']");e&&e.remove()}function Xa(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return e?'<link href="'.concat(e,'" rel="canonical" />'):""}function Za(e,t){if(t&&Object(Ia.a)(t).filter((function(e){return e.parentNode===document.head})).forEach((function(e){return document.head.removeChild(e)})),e)return e.map((function(e){var t=document.createElement("meta");return Object.keys(e).forEach((function(n){t.setAttribute(n,e[n])})),document.head.appendChild(t),t}))}function es(e){for(var t=0,n=["name","property","itemprop"];t<n.length;t++){var o=n[t];if(e.hasOwnProperty(o))return e[o]+o}return JSON.stringify(e)}n(186);var ts=n(70),ns=n.n(ts),os={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:ns()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var e=this,t=[].slice.call(document.querySelectorAll(".sidebar-link")),n=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(e){return t.some((function(t){return t.hash===e.hash}))})),o=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),i=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),r=window.innerHeight+o,a=0;a<n.length;a++){var s=n[a],l=n[a+1],c=0===a&&0===o||o>=s.parentElement.offsetTop+10&&(!l||o<l.parentElement.offsetTop-10),d=decodeURIComponent(this.$route.hash);if(c&&d!==decodeURIComponent(s.hash)){var u=s;if(r===i)for(var h=a+1;h<n.length;h++)if(d===decodeURIComponent(n[h].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(u.hash),(function(){e.$nextTick((function(){e.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},is=(n(55),n(71)),rs=n.n(is),as={mounted:function(){var e=this;rs.a.configure({showSpinner:!1}),this.$router.beforeEach((function(e,t,n){e.path===t.path||Pi.component(e.name)||rs.a.start(),n()})),this.$router.afterEach((function(){rs.a.done(),e.isSidebarOpen=!1}))}},ss=(n(176),n(124)),ls=(n(198),{props:{parent:Object,code:String,options:{align:String,color:String,backgroundTransition:Boolean,backgroundColor:String,successText:String,staticIcon:Boolean}},data:function(){return{success:!1,originalBackground:null,originalTransition:null}},computed:{alignStyle:function(){var e={};return e[this.options.align]="7.5px",e},iconClass:function(){return this.options.staticIcon?"":"hover"}},mounted:function(){this.originalTransition=this.parent.style.transition,this.originalBackground=this.parent.style.background},beforeDestroy:function(){this.parent.style.transition=this.originalTransition,this.parent.style.background=this.originalBackground},methods:{hexToRgb:function(e){var t=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(e);return t?{r:parseInt(t[1],16),g:parseInt(t[2],16),b:parseInt(t[3],16)}:null},copyToClipboard:function(e){var t=this;if(navigator.clipboard)navigator.clipboard.writeText(this.code).then((function(){t.setSuccessTransitions()}),(function(){}));else{var n=document.createElement("textarea");document.body.appendChild(n),n.value=this.code,n.select(),document.execCommand("Copy"),n.remove(),this.setSuccessTransitions()}},setSuccessTransitions:function(){var e=this;if(clearTimeout(this.successTimeout),this.options.backgroundTransition){this.parent.style.transition="background 350ms";var t=this.hexToRgb(this.options.backgroundColor);this.parent.style.background="rgba(".concat(t.r,", ").concat(t.g,", ").concat(t.b,", 0.1)")}this.success=!0,this.successTimeout=setTimeout((function(){e.options.backgroundTransition&&(e.parent.style.background=e.originalBackground,e.parent.style.transition=e.originalTransition),e.success=!1}),500)}}}),cs=(n(314),n(8)),ds=Object(cs.a)(ls,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("div",{staticClass:"code-copy"},[n("svg",{class:e.iconClass,style:e.alignStyle,attrs:{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 24 24"},on:{click:e.copyToClipboard}},[n("path",{attrs:{fill:"none",d:"M0 0h24v24H0z"}}),e._v(" "),n("path",{attrs:{fill:e.options.color,d:"M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm-1 4l6 6v10c0 1.1-.9 2-2 2H7.99C6.89 23 6 22.1 6 21l.01-14c0-1.1.89-2 1.99-2h7zm-1 7h5.5L14 6.5V12z"}})]),e._v(" "),n("span",{class:e.success?"success":"",style:e.alignStyle},[e._v("\n        "+e._s(e.options.successText)+"\n    ")])])}),[],!1,null,"49140617",null).exports,us=(n(315),[Ka,os,as,{updated:function(){this.update()},methods:{update:function(){setTimeout((function(){document.querySelectorAll('div[class*="language-"] pre').forEach((function(e){if(!e.classList.contains("code-copy-added")){var t=new(Pi.extend(ds));t.options=Object(ss.a)({},{align:"bottom",color:"#27b1ff",backgroundTransition:!0,backgroundColor:"#0075b8",successText:"Copied!",staticIcon:!1}),t.code=e.innerText,t.parent=e,t.$mount(),e.classList.add("code-copy-added"),e.appendChild(t.$el)}}))}),100)}}}]),hs={name:"GlobalLayout",computed:{layout:function(){var e=this.getLayout();return Ba("layout",e),Pi.component(e)}},methods:{getLayout:function(){if(this.$page.path){var e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},fs=Object(cs.a)(hs,(function(){var e=this.$createElement;return(this._self._c||e)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,t,n){var o;switch(t){case"components":e[t]||(e[t]={}),Object.assign(e[t],n);break;case"mixins":e[t]||(e[t]=[]),(o=e[t]).push.apply(o,Object(Ia.a)(n));break;default:throw new Error("Unknown option name.")}}(fs,"mixins",us);var ps=[{name:"v-1c019fb2",path:"/GLOSSARY.html",component:fs,beforeEnter:function(e,t,n){Ua("default","v-1c019fb2").then(n)}},{name:"v-674c2878",path:"/docs/get-started/installation/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-674c2878").then(n)}},{path:"/docs/get-started/installation/index.html",redirect:"/docs/get-started/installation/"},{path:"/docs/01-get-started/01-server-installation.html",redirect:"/docs/get-started/installation/"},{name:"v-34b90f42",path:"/docs/get-started/java-hello-world/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-34b90f42").then(n)}},{path:"/docs/get-started/java-hello-world/index.html",redirect:"/docs/get-started/java-hello-world/"},{path:"/docs/01-get-started/02-java-hello-world.html",redirect:"/docs/get-started/java-hello-world/"},{name:"v-6190b03c",path:"/docs/get-started/golang-hello-world/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-6190b03c").then(n)}},{path:"/docs/get-started/golang-hello-world/index.html",redirect:"/docs/get-started/golang-hello-world/"},{path:"/docs/01-get-started/03-golang-hello-world.html",redirect:"/docs/get-started/golang-hello-world/"},{name:"v-0403e040",path:"/docs/get-started/video-tutorials/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-0403e040").then(n)}},{path:"/docs/get-started/video-tutorials/index.html",redirect:"/docs/get-started/video-tutorials/"},{path:"/docs/01-get-started/04-video-tutorials.html",redirect:"/docs/get-started/video-tutorials/"},{name:"v-00cc835b",path:"/docs/get-started/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-00cc835b").then(n)}},{path:"/docs/get-started/index.html",redirect:"/docs/get-started/"},{path:"/docs/01-get-started/",redirect:"/docs/get-started/"},{name:"v-da0a32bc",path:"/docs/use-cases/periodic-execution/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-da0a32bc").then(n)}},{path:"/docs/use-cases/periodic-execution/index.html",redirect:"/docs/use-cases/periodic-execution/"},{path:"/docs/02-use-cases/01-periodic-execution.html",redirect:"/docs/use-cases/periodic-execution/"},{name:"v-1de42eb0",path:"/docs/use-cases/orchestration/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-1de42eb0").then(n)}},{path:"/docs/use-cases/orchestration/index.html",redirect:"/docs/use-cases/orchestration/"},{path:"/docs/02-use-cases/02-orchestration.html",redirect:"/docs/use-cases/orchestration/"},{name:"v-8cf1c7ac",path:"/docs/use-cases/polling/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-8cf1c7ac").then(n)}},{path:"/docs/use-cases/polling/index.html",redirect:"/docs/use-cases/polling/"},{path:"/docs/02-use-cases/03-polling.html",redirect:"/docs/use-cases/polling/"},{name:"v-7215aabc",path:"/docs/use-cases/event-driven/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-7215aabc").then(n)}},{path:"/docs/use-cases/event-driven/index.html",redirect:"/docs/use-cases/event-driven/"},{path:"/docs/02-use-cases/04-event-driven.html",redirect:"/docs/use-cases/event-driven/"},{name:"v-1b6ed5fc",path:"/docs/use-cases/partitioned-scan/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-1b6ed5fc").then(n)}},{path:"/docs/use-cases/partitioned-scan/index.html",redirect:"/docs/use-cases/partitioned-scan/"},{path:"/docs/02-use-cases/05-partitioned-scan.html",redirect:"/docs/use-cases/partitioned-scan/"},{name:"v-2044e7d6",path:"/docs/use-cases/batch-job/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-2044e7d6").then(n)}},{path:"/docs/use-cases/batch-job/index.html",redirect:"/docs/use-cases/batch-job/"},{path:"/docs/02-use-cases/06-batch-job.html",redirect:"/docs/use-cases/batch-job/"},{name:"v-7557d6c2",path:"/docs/use-cases/provisioning/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-7557d6c2").then(n)}},{path:"/docs/use-cases/provisioning/index.html",redirect:"/docs/use-cases/provisioning/"},{path:"/docs/02-use-cases/07-provisioning.html",redirect:"/docs/use-cases/provisioning/"},{name:"v-7bd56aa2",path:"/docs/use-cases/deployment/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-7bd56aa2").then(n)}},{path:"/docs/use-cases/deployment/index.html",redirect:"/docs/use-cases/deployment/"},{path:"/docs/02-use-cases/08-deployment.html",redirect:"/docs/use-cases/deployment/"},{name:"v-194a2c22",path:"/docs/use-cases/operational-management/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-194a2c22").then(n)}},{path:"/docs/use-cases/operational-management/index.html",redirect:"/docs/use-cases/operational-management/"},{path:"/docs/02-use-cases/09-operational-management.html",redirect:"/docs/use-cases/operational-management/"},{name:"v-142b1af4",path:"/docs/use-cases/interactive/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-142b1af4").then(n)}},{path:"/docs/use-cases/interactive/index.html",redirect:"/docs/use-cases/interactive/"},{path:"/docs/02-use-cases/10-interactive.html",redirect:"/docs/use-cases/interactive/"},{name:"v-a9abb788",path:"/docs/use-cases/dsl/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-a9abb788").then(n)}},{path:"/docs/use-cases/dsl/index.html",redirect:"/docs/use-cases/dsl/"},{path:"/docs/02-use-cases/11-dsl.html",redirect:"/docs/use-cases/dsl/"},{name:"v-a0c27e3c",path:"/docs/use-cases/big-ml/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-a0c27e3c").then(n)}},{path:"/docs/use-cases/big-ml/index.html",redirect:"/docs/use-cases/big-ml/"},{path:"/docs/02-use-cases/12-big-ml.html",redirect:"/docs/use-cases/big-ml/"},{name:"v-a16b91ca",path:"/docs/use-cases/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-a16b91ca").then(n)}},{path:"/docs/use-cases/index.html",redirect:"/docs/use-cases/"},{path:"/docs/02-use-cases/",redirect:"/docs/use-cases/"},{name:"v-13e86a42",path:"/docs/concepts/workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-13e86a42").then(n)}},{path:"/docs/concepts/workflows/index.html",redirect:"/docs/concepts/workflows/"},{path:"/docs/03-concepts/01-workflows.html",redirect:"/docs/concepts/workflows/"},{name:"v-006b104c",path:"/docs/concepts/activities/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-006b104c").then(n)}},{path:"/docs/concepts/activities/index.html",redirect:"/docs/concepts/activities/"},{path:"/docs/03-concepts/02-activities.html",redirect:"/docs/concepts/activities/"},{name:"v-4b893278",path:"/docs/concepts/events/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-4b893278").then(n)}},{path:"/docs/concepts/events/index.html",redirect:"/docs/concepts/events/"},{path:"/docs/03-concepts/03-events.html",redirect:"/docs/concepts/events/"},{name:"v-7d1ab062",path:"/docs/concepts/queries/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-7d1ab062").then(n)}},{path:"/docs/concepts/queries/index.html",redirect:"/docs/concepts/queries/"},{path:"/docs/03-concepts/04-queries.html",redirect:"/docs/concepts/queries/"},{name:"v-20231a48",path:"/docs/concepts/topology/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-20231a48").then(n)}},{path:"/docs/concepts/topology/index.html",redirect:"/docs/concepts/topology/"},{path:"/docs/03-concepts/05-topology.html",redirect:"/docs/concepts/topology/"},{name:"v-2cd6f7bc",path:"/docs/concepts/task-lists/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-2cd6f7bc").then(n)}},{path:"/docs/concepts/task-lists/index.html",redirect:"/docs/concepts/task-lists/"},{path:"/docs/03-concepts/06-task-lists.html",redirect:"/docs/concepts/task-lists/"},{name:"v-f2d516bc",path:"/docs/concepts/archival/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-f2d516bc").then(n)}},{path:"/docs/concepts/archival/index.html",redirect:"/docs/concepts/archival/"},{path:"/docs/03-concepts/07-archival.html",redirect:"/docs/concepts/archival/"},{name:"v-071004ea",path:"/docs/concepts/cross-dc-replication/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-071004ea").then(n)}},{path:"/docs/concepts/cross-dc-replication/index.html",redirect:"/docs/concepts/cross-dc-replication/"},{path:"/docs/03-concepts/08-cross-dc-replication.html",redirect:"/docs/concepts/cross-dc-replication/"},{name:"v-0f936964",path:"/docs/concepts/search-workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-0f936964").then(n)}},{path:"/docs/concepts/search-workflows/index.html",redirect:"/docs/concepts/search-workflows/"},{path:"/docs/03-concepts/09-search-workflows.html",redirect:"/docs/concepts/search-workflows/"},{name:"v-496bb1df",path:"/docs/concepts/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-496bb1df").then(n)}},{path:"/docs/concepts/index.html",redirect:"/docs/concepts/"},{path:"/docs/03-concepts/",redirect:"/docs/concepts/"},{name:"v-211cf634",path:"/docs/java-client/client-overview/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-211cf634").then(n)}},{path:"/docs/java-client/client-overview/index.html",redirect:"/docs/java-client/client-overview/"},{path:"/docs/04-java-client/01-client-overview.html",redirect:"/docs/java-client/client-overview/"},{name:"v-3d11f802",path:"/docs/java-client/workflow-interface/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-3d11f802").then(n)}},{path:"/docs/java-client/workflow-interface/index.html",redirect:"/docs/java-client/workflow-interface/"},{path:"/docs/04-java-client/02-workflow-interface.html",redirect:"/docs/java-client/workflow-interface/"},{name:"v-264670c2",path:"/docs/java-client/implementing-workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-264670c2").then(n)}},{path:"/docs/java-client/implementing-workflows/index.html",redirect:"/docs/java-client/implementing-workflows/"},{path:"/docs/04-java-client/03-implementing-workflows.html",redirect:"/docs/java-client/implementing-workflows/"},{name:"v-11997e3c",path:"/docs/java-client/starting-workflow-executions/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-11997e3c").then(n)}},{path:"/docs/java-client/starting-workflow-executions/index.html",redirect:"/docs/java-client/starting-workflow-executions/"},{path:"/docs/04-java-client/04-starting-workflow-executions.html",redirect:"/docs/java-client/starting-workflow-executions/"},{name:"v-5a20c23c",path:"/docs/java-client/activity-interface/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-5a20c23c").then(n)}},{path:"/docs/java-client/activity-interface/index.html",redirect:"/docs/java-client/activity-interface/"},{path:"/docs/04-java-client/05-activity-interface.html",redirect:"/docs/java-client/activity-interface/"},{name:"v-056557ea",path:"/docs/java-client/implementing-activities/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-056557ea").then(n)}},{path:"/docs/java-client/implementing-activities/index.html",redirect:"/docs/java-client/implementing-activities/"},{path:"/docs/04-java-client/06-implementing-activities.html",redirect:"/docs/java-client/implementing-activities/"},{name:"v-ee26987c",path:"/docs/java-client/versioning/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-ee26987c").then(n)}},{path:"/docs/java-client/versioning/index.html",redirect:"/docs/java-client/versioning/"},{path:"/docs/04-java-client/07-versioning.html",redirect:"/docs/java-client/versioning/"},{name:"v-5951033c",path:"/docs/java-client/distributed-cron/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-5951033c").then(n)}},{path:"/docs/java-client/distributed-cron/index.html",redirect:"/docs/java-client/distributed-cron/"},{path:"/docs/04-java-client/08-distributed-cron.html",redirect:"/docs/java-client/distributed-cron/"},{name:"v-8f63b5a0",path:"/docs/java-client/workers/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-8f63b5a0").then(n)}},{path:"/docs/java-client/workers/index.html",redirect:"/docs/java-client/workers/"},{path:"/docs/04-java-client/09-workers.html",redirect:"/docs/java-client/workers/"},{name:"v-3e031ed8",path:"/docs/java-client/signals/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-3e031ed8").then(n)}},{path:"/docs/java-client/signals/index.html",redirect:"/docs/java-client/signals/"},{path:"/docs/04-java-client/10-signals.html",redirect:"/docs/java-client/signals/"},{name:"v-8e66acc0",path:"/docs/java-client/queries/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-8e66acc0").then(n)}},{path:"/docs/java-client/queries/index.html",redirect:"/docs/java-client/queries/"},{path:"/docs/04-java-client/11-queries.html",redirect:"/docs/java-client/queries/"},{name:"v-596ec15e",path:"/docs/java-client/retries/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-596ec15e").then(n)}},{path:"/docs/java-client/retries/index.html",redirect:"/docs/java-client/retries/"},{path:"/docs/04-java-client/12-retries.html",redirect:"/docs/java-client/retries/"},{name:"v-5fdea0a2",path:"/docs/java-client/child-workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-5fdea0a2").then(n)}},{path:"/docs/java-client/child-workflows/index.html",redirect:"/docs/java-client/child-workflows/"},{path:"/docs/04-java-client/13-child-workflows.html",redirect:"/docs/java-client/child-workflows/"},{name:"v-e894b2bc",path:"/docs/java-client/exception-handling/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-e894b2bc").then(n)}},{path:"/docs/java-client/exception-handling/index.html",redirect:"/docs/java-client/exception-handling/"},{path:"/docs/04-java-client/14-exception-handling.html",redirect:"/docs/java-client/exception-handling/"},{name:"v-0463910e",path:"/docs/java-client/continue-as-new/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-0463910e").then(n)}},{path:"/docs/java-client/continue-as-new/index.html",redirect:"/docs/java-client/continue-as-new/"},{path:"/docs/04-java-client/15-continue-as-new.html",redirect:"/docs/java-client/continue-as-new/"},{name:"v-5ee76854",path:"/docs/java-client/side-effect/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-5ee76854").then(n)}},{path:"/docs/java-client/side-effect/index.html",redirect:"/docs/java-client/side-effect/"},{path:"/docs/04-java-client/16-side-effect.html",redirect:"/docs/java-client/side-effect/"},{name:"v-45b94840",path:"/docs/java-client/testing/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-45b94840").then(n)}},{path:"/docs/java-client/testing/index.html",redirect:"/docs/java-client/testing/"},{path:"/docs/04-java-client/17-testing.html",redirect:"/docs/java-client/testing/"},{name:"v-389be5ec",path:"/docs/java-client/workflow-replay-shadowing/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-389be5ec").then(n)}},{path:"/docs/java-client/workflow-replay-shadowing/index.html",redirect:"/docs/java-client/workflow-replay-shadowing/"},{path:"/docs/04-java-client/18-workflow-replay-shadowing.html",redirect:"/docs/java-client/workflow-replay-shadowing/"},{name:"v-651b4e0a",path:"/docs/java-client/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-651b4e0a").then(n)}},{path:"/docs/java-client/index.html",redirect:"/docs/java-client/"},{path:"/docs/04-java-client/",redirect:"/docs/java-client/"},{name:"v-e9a63714",path:"/docs/go-client/workers/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-e9a63714").then(n)}},{path:"/docs/go-client/workers/index.html",redirect:"/docs/go-client/workers/"},{path:"/docs/05-go-client/01-workers.html",redirect:"/docs/go-client/workers/"},{name:"v-d91dcabc",path:"/docs/go-client/create-workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-d91dcabc").then(n)}},{path:"/docs/go-client/create-workflows/index.html",redirect:"/docs/go-client/create-workflows/"},{path:"/docs/05-go-client/02-create-workflows.html",redirect:"/docs/go-client/create-workflows/"},{name:"v-241aa182",path:"/docs/go-client/activities/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-241aa182").then(n)}},{path:"/docs/go-client/activities/index.html",redirect:"/docs/go-client/activities/"},{path:"/docs/05-go-client/03-activities.html",redirect:"/docs/go-client/activities/"},{name:"v-7109c462",path:"/docs/go-client/execute-activity/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-7109c462").then(n)}},{path:"/docs/go-client/execute-activity/index.html",redirect:"/docs/go-client/execute-activity/"},{path:"/docs/05-go-client/04-execute-activity.html",redirect:"/docs/go-client/execute-activity/"},{name:"v-30ee6212",path:"/docs/go-client/child-workflows/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-30ee6212").then(n)}},{path:"/docs/go-client/child-workflows/index.html",redirect:"/docs/go-client/child-workflows/"},{path:"/docs/05-go-client/05-child-workflows.html",redirect:"/docs/go-client/child-workflows/"},{name:"v-63bf2e6c",path:"/docs/go-client/retries/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-63bf2e6c").then(n)}},{path:"/docs/go-client/retries/index.html",redirect:"/docs/go-client/retries/"},{path:"/docs/05-go-client/06-retries.html",redirect:"/docs/go-client/retries/"},{name:"v-103bbcbc",path:"/docs/go-client/error-handling/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-103bbcbc").then(n)}},{path:"/docs/go-client/error-handling/index.html",redirect:"/docs/go-client/error-handling/"},{path:"/docs/05-go-client/07-error-handling.html",redirect:"/docs/go-client/error-handling/"},{name:"v-65ea467c",path:"/docs/go-client/signals/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-65ea467c").then(n)}},{path:"/docs/go-client/signals/index.html",redirect:"/docs/go-client/signals/"},{path:"/docs/05-go-client/08-signals.html",redirect:"/docs/go-client/signals/"},{name:"v-3c7b0dd4",path:"/docs/go-client/side-effect/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-3c7b0dd4").then(n)}},{path:"/docs/go-client/side-effect/index.html",redirect:"/docs/go-client/side-effect/"},{path:"/docs/05-go-client/10-side-effect.html",redirect:"/docs/go-client/side-effect/"},{name:"v-b60e670c",path:"/docs/go-client/continue-as-new/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-b60e670c").then(n)}},{path:"/docs/go-client/continue-as-new/index.html",redirect:"/docs/go-client/continue-as-new/"},{path:"/docs/05-go-client/09-continue-as-new.html",redirect:"/docs/go-client/continue-as-new/"},{name:"v-a558de54",path:"/docs/go-client/queries/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-a558de54").then(n)}},{path:"/docs/go-client/queries/index.html",redirect:"/docs/go-client/queries/"},{path:"/docs/05-go-client/11-queries.html",redirect:"/docs/go-client/queries/"},{name:"v-5b7bae8a",path:"/docs/go-client/activity-async-completion/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-5b7bae8a").then(n)}},{path:"/docs/go-client/activity-async-completion/index.html",redirect:"/docs/go-client/activity-async-completion/"},{path:"/docs/05-go-client/12-activity-async-completion.html",redirect:"/docs/go-client/activity-async-completion/"},{name:"v-722c1fc2",path:"/docs/go-client/workflow-testing/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-722c1fc2").then(n)}},{path:"/docs/go-client/workflow-testing/index.html",redirect:"/docs/go-client/workflow-testing/"},{path:"/docs/05-go-client/13-workflow-testing.html",redirect:"/docs/go-client/workflow-testing/"},{name:"v-957246a8",path:"/docs/go-client/workflow-versioning/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-957246a8").then(n)}},{path:"/docs/go-client/workflow-versioning/index.html",redirect:"/docs/go-client/workflow-versioning/"},{path:"/docs/05-go-client/14-workflow-versioning.html",redirect:"/docs/go-client/workflow-versioning/"},{name:"v-389752bc",path:"/docs/go-client/sessions/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-389752bc").then(n)}},{path:"/docs/go-client/sessions/index.html",redirect:"/docs/go-client/sessions/"},{path:"/docs/05-go-client/15-sessions.html",redirect:"/docs/go-client/sessions/"},{name:"v-0c11d262",path:"/docs/go-client/distributed-cron/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-0c11d262").then(n)}},{path:"/docs/go-client/distributed-cron/index.html",redirect:"/docs/go-client/distributed-cron/"},{path:"/docs/05-go-client/16-distributed-cron.html",redirect:"/docs/go-client/distributed-cron/"},{name:"v-a139e6dc",path:"/docs/go-client/tracing/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-a139e6dc").then(n)}},{path:"/docs/go-client/tracing/index.html",redirect:"/docs/go-client/tracing/"},{path:"/docs/05-go-client/17-tracing.html",redirect:"/docs/go-client/tracing/"},{name:"v-0f2e8980",path:"/docs/go-client/workflow-replay-shadowing/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-0f2e8980").then(n)}},{path:"/docs/go-client/workflow-replay-shadowing/index.html",redirect:"/docs/go-client/workflow-replay-shadowing/"},{path:"/docs/05-go-client/18-workflow-replay-shadowing.html",redirect:"/docs/go-client/workflow-replay-shadowing/"},{name:"v-2d3e7cdb",path:"/docs/go-client/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-2d3e7cdb").then(n)}},{path:"/docs/go-client/index.html",redirect:"/docs/go-client/"},{path:"/docs/05-go-client/",redirect:"/docs/go-client/"},{name:"v-2dfd6d7b",path:"/docs/cli/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-2dfd6d7b").then(n)}},{path:"/docs/cli/index.html",redirect:"/docs/cli/"},{path:"/docs/06-cli/",redirect:"/docs/cli/"},{name:"v-94771a14",path:"/docs/operation-guide/setup/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-94771a14").then(n)}},{path:"/docs/operation-guide/setup/index.html",redirect:"/docs/operation-guide/setup/"},{path:"/docs/07-operation-guide/01-setup.html",redirect:"/docs/operation-guide/setup/"},{name:"v-3cd8d962",path:"/docs/operation-guide/maintain/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-3cd8d962").then(n)}},{path:"/docs/operation-guide/maintain/index.html",redirect:"/docs/operation-guide/maintain/"},{path:"/docs/07-operation-guide/02-maintain.html",redirect:"/docs/operation-guide/maintain/"},{name:"v-6d823dbc",path:"/docs/operation-guide/monitor/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-6d823dbc").then(n)}},{path:"/docs/operation-guide/monitor/index.html",redirect:"/docs/operation-guide/monitor/"},{path:"/docs/07-operation-guide/03-monitoring.html",redirect:"/docs/operation-guide/monitor/"},{name:"v-20490294",path:"/docs/operation-guide/troubleshooting/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-20490294").then(n)}},{path:"/docs/operation-guide/troubleshooting/index.html",redirect:"/docs/operation-guide/troubleshooting/"},{path:"/docs/07-operation-guide/04-troubleshooting.html",redirect:"/docs/operation-guide/troubleshooting/"},{name:"v-681aeaca",path:"/docs/operation-guide/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-681aeaca").then(n)}},{path:"/docs/operation-guide/index.html",redirect:"/docs/operation-guide/"},{path:"/docs/07-operation-guide/",redirect:"/docs/operation-guide/"},{name:"v-4997885e",path:"/docs/about/license/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-4997885e").then(n)}},{path:"/docs/about/license/index.html",redirect:"/docs/about/license/"},{path:"/docs/08-about/01-license.html",redirect:"/docs/about/license/"},{name:"v-f7dd2f4a",path:"/docs/about/",component:fs,beforeEnter:function(e,t,n){Ua("default","v-f7dd2f4a").then(n)}},{path:"/docs/about/index.html",redirect:"/docs/about/"},{path:"/docs/08-about/",redirect:"/docs/about/"},{name:"v-0615a98a",path:"/",component:fs,beforeEnter:function(e,t,n){Ua("Layout","v-0615a98a").then(n)}},{path:"/index.html",redirect:"/"},{path:"*",component:fs}],ms={title:"Cadence",description:"",base:"/",headTags:[["link",{rel:"icon",href:"/img/favicon.ico"}]],pages:[{title:"Glossary",frontmatter:{layout:"default",title:"Glossary",terms:{activity:"A business-level function that implements your application logic such as calling a service or transcoding a media file. An activity usually implements a single well-defined action; it can be short or long running. An activity can be implemented as a synchronous method or fully asynchronously involving multiple processes. An activity can be retried indefinitely according to the provided exponential retry policy. If for any reason an activity is not completed within the specified timeout, an error is reported to the workflow and the workflow decides how to handle it. There is no limit on potential activity duration.","activity task":"A task that contains an activity invocation information that is delivered to an activity worker through and an  activity task list. An activity worker upon receiving activity task executes a correponding activity","activity task list":"Task list that is used to deliver activity task to activity worker","activity worker":"An object that is executed in the client application and receives activity task from an  activity task list it is subscribed to. Once task is received it invokes a correspondent activity.",archival:"Archival is a feature that automatically moves event history from persistence to a blobstore after the workflow retention period. The purpose of archival is to be able to keep histories as long as needed while not overwhelming the persistence store. There are two reasons you may want to keep the histories after the retention period has passed: 1. Compliance: For legal reasons, histories may need to be stored for a long period of time. 2. Debugging: Old histories can still be accessed for debugging.",CLI:"Cadence command-line interface.","client stub":"A client-side proxy used to make remote invocations to an entity that it represents. For example, to start a workflow, a stub object that represents this workflow is created through a special API. Then this stub is used to start, query, or signal the corresponding workflow.\nThe Go client doesn't use this.",decision:"Any action taken by the workflow durable function is called a decision. For example: scheduling an activity, canceling a child workflow, or starting a timer. A decision task contains an optional list of decisions. Every decision is recorded in the event history as an event. See also [1] for more explanation","decision task":"Every time a new external event that might affect a workflow state is recorded, a decision task that contains it is added to a decision task list and then picked up by a workflow worker. After the new event is handled, the decision task is completed with a list of decision. Note that handling of a decision task is usually very fast and is not related to duration of operations that the workflow invokes. See also [1] for more explanation","decision task list":"Task list that is used to deliver decision task to workflow worker. From user's point of view, it can be viewed as a worker pool. It defines a pool of worker executing workflow or activity tasks.",domain:"Cadence is backed by a multitenant service. The unit of isolation is called a domain. Each domain acts as a namespace for task list names as well as workflow IDs. For example, when a workflow is started, it is started in a specific domain. Cadence guarantees a unique workflow ID within a domain, and supports running workflow executions to use the same workflow ID if they are in different domains. Various configuration options like retention period or archival destination are configured per domain as well through a special CRUD API or through the Cadence CLI. In the multi-cluster deployment, domain is a unit of fail-over. Each domain can only be active on a single Cadence cluster at a time. However, different domains can be active in different clusters and can fail-over independently.",event:"An indivisible operation performed by your application. For example, activity_task_started, task_failed, or timer_canceled. Events are recorded in the event history.","event history":"An append log of events for your application. History is durably persisted by the Cadence service, enabling seamless recovery of your application state from crashes or failures. It also serves as an audit log for debugging.","local activity":"A local activity is an activity that is invoked directly in the same process by a workflow code. It consumes much less resources than a normal activity, but imposes a lot of limitations like low duration and lack of rate limiting.",query:"A synchronous (from the caller's point of view) operation that is used to report a workflow state. Note that a query is inherently read only and cannot affect a workflow state.","run ID":"A UUID that a Cadence service assigns to each workflow run. If allowed by a configured policy, you might be able to re-execute a workflow, after it has closed or failed, with the same workflow id. Each such re-execution is called a run. run id is used to uniquely identify a run even if it shares a workflow id with others.",signal:"An external asynchronous request to a workflow. It can be used to deliver notifications or updates to a running workflow at any point in its existence.",task:"The context needed to execute a specific activity or workflow state transition. There are two types of tasks: an activity task and a decision task (aka workflow task). Note that a single activity execution corresponds to a single activity task, while a workflow execution employs multiple decision tasks.","task list":"Common name for activity task list and decision task list","task token":"A unique correlation ID for a Cadence activity. Activity completion calls take either task token or DomainName, WorkflowID, ActivityID arguments.",worker:"Also known as a worker service. A service that hosts the workflow and activity implementations. The worker polls the Cadence service for tasks, performs those tasks, and communicates task execution results back to the Cadence service. Worker services are developed, deployed, and operated by Cadence customers.",workflow:"A fault-oblivious stateful function that orchestrates activities. A workflow has full control over which activities are executed, and in which order. A workflow must not affect the external world directly, only through activities. What makes workflow code a workflow is that its state is preserved by Cadence. Therefore any failure of a worker process that hosts the workflow code does not affect the workflow execution. The workflow continues as if these failures did not happen. At the same time, activities can fail any moment for any reason. Because workflow code is fully fault-oblivious, it is guaranteed to get notifications about activity failures or timeouts and act accordingly. There is no limit on potential workflow duration.","workflow execution":"An instance of a workflow. The instance can be in the process of executing or it could have already completed execution.","workflow ID":"A unique identifier for a workflow execution. Cadence guarantees the uniqueness of an ID within a domain. An attempt to start a workflow with a duplicate ID results in an already started error.","workflow task":"Synonym of the decision task.","workflow worker":"An object that is executed in the client application and receives decision task from an  decision task list it is subscribed to. Once task is received it is handled by a correponding workflow."},readingShow:"top"},regularPath:"/GLOSSARY.html",relativePath:"GLOSSARY.md",key:"v-1c019fb2",path:"/GLOSSARY.html",headersStr:null,content:"# Glossary\n1 What exactly is a Cadence decision task?",normalizedContent:"# glossary\n1 what exactly is a cadence decision task?",charsets:{}},{title:"Server Installation",frontmatter:{layout:"default",title:"Server Installation",permalink:"/docs/get-started/installation",readingShow:"top"},regularPath:"/docs/01-get-started/01-server-installation.html",relativePath:"docs/01-get-started/01-server-installation.md",key:"v-674c2878",path:"/docs/get-started/installation/",headers:[{level:2,title:"Install docker",slug:"install-docker",normalizedTitle:"install docker",charIndex:36},{level:2,title:"Run Cadence Server Using Docker Compose",slug:"run-cadence-server-using-docker-compose",normalizedTitle:"run cadence server using docker compose",charIndex:155},{level:2,title:"Register a Domain Using the CLI",slug:"register-a-domain-using-the-cli",normalizedTitle:"register a domain using the cli",charIndex:2112},{level:2,title:"Troubleshooting",slug:"troubleshooting",normalizedTitle:"troubleshooting",charIndex:3203},{level:2,title:"What's Next",slug:"what-s-next",normalizedTitle:"what's next",charIndex:3865}],headersStr:"Install docker Run Cadence Server Using Docker Compose Register a Domain Using the CLI Troubleshooting What's Next",content:'# Install Cadence Service Locally\n# Install docker\nFollow the Docker installation instructions found here: https://docs.docker.com/engine/installation/\n\n# Run Cadence Server Using Docker Compose\nDownload the Cadence docker-compose file:\n\n> curl -O https://raw.githubusercontent.com/uber/cadence/master/docker/docker-compose.yml && curl -O https://raw.githubusercontent.com/uber/cadence/master/docker/prometheus_config.yml\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100   675  100   675    0     0    958      0 --:--:-- --:--:-- --:--:--   958\n> ls\ndocker-compose.yml\n\n\nStart Cadence Service:\n\n> docker-compose up\nCreating network "quick_start_default" with the default driver\nPulling cadence (ubercadence/server:0.5.8)...\n0.5.8: Pulling from ubercadence/server\ndb0035920883: Pull complete\n...\n...\n27ec3755f89c: Pull complete\n0a5d2a29a5e5: Pull complete\nCreating quick_start_statsd_1    ... done\nCreating quick_start_cassandra_1 ... done\nCreating quick_start_cadence_1   ... done\nCreating quick_start_cadence-web_1 ... done\nAttaching to quick_start_cassandra_1, quick_start_statsd_1, quick_start_cadence_1, quick_start_cadence-web_1\nstatsd_1       | *** Running /etc/my_init.d/00_regen_ssh_host_keys.sh...\nstatsd_1       | *** Running /etc/my_init.d/01_conf_init.sh...\ncadence_1      | + CADENCE_HOME=/cadence\ncadence_1      | + DB=cassandra\n...\n...\n...\ncadence_1      | {"level":"info","ts":"2019-06-06T15:26:38.199Z","msg":"Get dynamic config","name":"matching.longPollExpirationInterval","value":"1m0s","default-value":"1m0s","logging-call-at":"config.go:57"}\ncadence_1      | {"level":"info","ts":"2019-06-06T15:26:38.199Z","msg":"Get dynamic config","name":"matching.updateAckInterval","value":"1m0s","default-value":"1m0s","logging-call-at":"config.go:57"}\n...\n...\ncadence_1      | {"level":"info","ts":"2019-06-06T15:27:24.905Z","msg":"Get dynamic config","name":"history.timerProcessorCompleteTimerFailureRetryCount","value":"10","default-value":"10","logging-call-at":"config.go:57"}\n\n\n# Register a Domain Using the CLI\nFrom a different console window:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain domain register -rd 1\nUnable to find image \'ubercadence/cli:master\' locally\nmaster: Pulling from ubercadence/cli\n22dc81ace0ea: Pull complete\n...\n...\nda2cfe74be81: Pull complete\n5320bde81c0c: Pull complete\nDigest: sha256:f5e5e708347909c8d3f74c47878b201d91606994394e94eaede9a80e3b9f077b\nStatus: Downloaded newer image for ubercadence/cli:master\nDomain test-domain successfully registered.\n>\n\n\nCheck that the domain is indeed registered:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain domain describe\nName: test-domain\nDescription:\nOwnerEmail:\nDomainData: map[]\nStatus: REGISTERED\nRetentionInDays: 1\nEmitMetrics: false\nActiveClusterName: active\nClusters: active\nArchivalStatus: DISABLED\nBad binaries to reset:\n+-----------------+----------+------------+--------+\n| BINARY CHECKSUM | OPERATOR | START TIME | REASON |\n+-----------------+----------+------------+--------+\n+-----------------+----------+------------+--------+\n>\n\n\n# Troubleshooting\nThere can be various reasons that docker-compose up cannot succeed:\n\n * In case of the image being too old, update the docker image by docker pull ubercadence/server:master-auto-setup and retry\n * In case of the local docker env is messed up: docker system prune --all and retry (see details about it )\n * See logs of different container: * If Cassandra is not able to get up: docker logs -f docker_cassandra_1\n    * If Cadence is not able to get up: docker logs -f docker_cadence_1\n    * If Cadence Web is not able to get up: docker logs -f docker_cadence-web_1\n   \n   \n\nIf the above is still not working, open an issue in Server(main) repo.\n\n# What\'s Next\nGo to Java HelloWorld or Golang HelloWorld.',normalizedContent:'# install cadence service locally\n# install docker\nfollow the docker installation instructions found here: https://docs.docker.com/engine/installation/\n\n# run cadence server using docker compose\ndownload the cadence docker-compose file:\n\n> curl -o https://raw.githubusercontent.com/uber/cadence/master/docker/docker-compose.yml && curl -o https://raw.githubusercontent.com/uber/cadence/master/docker/prometheus_config.yml\n  % total    % received % xferd  average speed   time    time     time  current\n                                 dload  upload   total   spent    left  speed\n100   675  100   675    0     0    958      0 --:--:-- --:--:-- --:--:--   958\n> ls\ndocker-compose.yml\n\n\nstart cadence service:\n\n> docker-compose up\ncreating network "quick_start_default" with the default driver\npulling cadence (ubercadence/server:0.5.8)...\n0.5.8: pulling from ubercadence/server\ndb0035920883: pull complete\n...\n...\n27ec3755f89c: pull complete\n0a5d2a29a5e5: pull complete\ncreating quick_start_statsd_1    ... done\ncreating quick_start_cassandra_1 ... done\ncreating quick_start_cadence_1   ... done\ncreating quick_start_cadence-web_1 ... done\nattaching to quick_start_cassandra_1, quick_start_statsd_1, quick_start_cadence_1, quick_start_cadence-web_1\nstatsd_1       | *** running /etc/my_init.d/00_regen_ssh_host_keys.sh...\nstatsd_1       | *** running /etc/my_init.d/01_conf_init.sh...\ncadence_1      | + cadence_home=/cadence\ncadence_1      | + db=cassandra\n...\n...\n...\ncadence_1      | {"level":"info","ts":"2019-06-06t15:26:38.199z","msg":"get dynamic config","name":"matching.longpollexpirationinterval","value":"1m0s","default-value":"1m0s","logging-call-at":"config.go:57"}\ncadence_1      | {"level":"info","ts":"2019-06-06t15:26:38.199z","msg":"get dynamic config","name":"matching.updateackinterval","value":"1m0s","default-value":"1m0s","logging-call-at":"config.go:57"}\n...\n...\ncadence_1      | {"level":"info","ts":"2019-06-06t15:27:24.905z","msg":"get dynamic config","name":"history.timerprocessorcompletetimerfailureretrycount","value":"10","default-value":"10","logging-call-at":"config.go:57"}\n\n\n# register a domain using the cli\nfrom a different console window:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain domain register -rd 1\nunable to find image \'ubercadence/cli:master\' locally\nmaster: pulling from ubercadence/cli\n22dc81ace0ea: pull complete\n...\n...\nda2cfe74be81: pull complete\n5320bde81c0c: pull complete\ndigest: sha256:f5e5e708347909c8d3f74c47878b201d91606994394e94eaede9a80e3b9f077b\nstatus: downloaded newer image for ubercadence/cli:master\ndomain test-domain successfully registered.\n>\n\n\ncheck that the domain is indeed registered:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain domain describe\nname: test-domain\ndescription:\nowneremail:\ndomaindata: map[]\nstatus: registered\nretentionindays: 1\nemitmetrics: false\nactiveclustername: active\nclusters: active\narchivalstatus: disabled\nbad binaries to reset:\n+-----------------+----------+------------+--------+\n| binary checksum | operator | start time | reason |\n+-----------------+----------+------------+--------+\n+-----------------+----------+------------+--------+\n>\n\n\n# troubleshooting\nthere can be various reasons that docker-compose up cannot succeed:\n\n * in case of the image being too old, update the docker image by docker pull ubercadence/server:master-auto-setup and retry\n * in case of the local docker env is messed up: docker system prune --all and retry (see details about it )\n * see logs of different container: * if cassandra is not able to get up: docker logs -f docker_cassandra_1\n    * if cadence is not able to get up: docker logs -f docker_cadence_1\n    * if cadence web is not able to get up: docker logs -f docker_cadence-web_1\n   \n   \n\nif the above is still not working, open an issue in server(main) repo.\n\n# what\'s next\ngo to java helloworld or golang helloworld.',charsets:{cjk:!0}},{title:"Java hello world",frontmatter:{layout:"default",title:"Java hello world",permalink:"/docs/get-started/java-hello-world",readingShow:"top"},regularPath:"/docs/01-get-started/02-java-hello-world.html",relativePath:"docs/01-get-started/02-java-hello-world.md",key:"v-34b90f42",path:"/docs/get-started/java-hello-world/",headers:[{level:2,title:"Include Cadence Java Client Dependency",slug:"include-cadence-java-client-dependency",normalizedTitle:"include cadence java client dependency",charIndex:194},{level:2,title:"Implement Hello World Workflow",slug:"implement-hello-world-workflow",normalizedTitle:"implement hello world workflow",charIndex:1820},{level:2,title:"Execute Hello World Workflow using the CLI",slug:"execute-hello-world-workflow-using-the-cli",normalizedTitle:"execute hello world workflow using the cli",charIndex:3536},{level:2,title:"List Workflows and Workflow History",slug:"list-workflows-and-workflow-history",normalizedTitle:"list workflows and workflow history",charIndex:7610},{level:2,title:"What is Next",slug:"what-is-next",normalizedTitle:"what is next",charIndex:10098}],headersStr:"Include Cadence Java Client Dependency Implement Hello World Workflow Execute Hello World Workflow using the CLI List Workflows and Workflow History What is Next",content:'# Java Hello World\nThis section only describe how to write and run a HelloWorld with Java. Go to Cadence-Java-Samples for more examples, and Java-Client and java-docs for more documentation.\n\n# Include Cadence Java Client Dependency\nGo to the Maven Repository Uber Cadence Java Client Pageand find the latest version of the library. Include it as a dependency into your Java project. For example if you are using Gradle the dependency looks like:\n\ncompile group: \'com.uber.cadence\', name: \'cadence-client\', version: \'<latest_version>\'\n\n\nAlso add the following dependencies that cadence-client relies on:\n\ncompile group: \'commons-configuration\', name: \'commons-configuration\', version: \'1.9\'\ncompile group: \'ch.qos.logback\', name: \'logback-classic\', version: \'1.2.3\'\n\n\nMake sure that the following code compiles:\n\nimport com.uber.cadence.workflow.Workflow;\nimport com.uber.cadence.workflow.WorkflowMethod;\nimport org.slf4j.Logger;\n\npublic class GettingStarted {\n\n    private static Logger logger = Workflow.getLogger(GettingStarted.class);\n\n    interface HelloWorld {\n        @WorkflowMethod\n        void sayHello(String name);\n    }\n\n}\n\n\nIf you are having problems setting up the build files use theCadence Java Samples GitHub repository as a reference.\n\nAlso add the following logback config file somewhere in your classpath:\n\n<configuration>\n    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">\n        \x3c!-- encoders are assigned the type\n             ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --\x3e\n        <encoder>\n            <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>\n        </encoder>\n    </appender>\n    <logger name="io.netty" level="INFO"/>\n    <root level="INFO">\n        <appender-ref ref="STDOUT" />\n    </root>\n</configuration>\n\n\n# Implement Hello World Workflow\nLet\'s add HelloWorldImpl with the sayHello method that just logs the "Hello ..." and returns.\n\nimport com.uber.cadence.worker.Worker;\nimport com.uber.cadence.workflow.Workflow;\nimport com.uber.cadence.workflow.WorkflowMethod;\nimport org.slf4j.Logger;\n\npublic class GettingStarted {\n\n    private static Logger logger = Workflow.getLogger(GettingStarted.class);\n\n    public interface HelloWorld {\n        @WorkflowMethod\n        void sayHello(String name);\n    }\n\n    public static class HelloWorldImpl implements HelloWorld {\n\n        @Override\n        public void sayHello(String name) {\n            logger.info("Hello " + name + "!");\n        }\n    }\n}\n\n\nTo link the implementation to the Cadence framework, it should be registered with a that connects to a Cadence Service. By default the connects to the locally running Cadence service.\n\npublic static void main(String[] args) {\n  WorkflowClient workflowClient =\n      WorkflowClient.newInstance(\n          new WorkflowServiceTChannel(ClientOptions.defaultInstance()),\n          WorkflowClientOptions.newBuilder().setDomain(DOMAIN).build());\n  // Get worker to poll the task list.\n  WorkerFactory factory = WorkerFactory.newInstance(workflowClient);\n  Worker worker = factory.newWorker(TASK_LIST);\n  worker.registerWorkflowImplementationTypes(HelloWorldImpl.class);\n  factory.start();\n}\n\n\nThe code is slightly different if you are using client version prior to 3.0.0:\n\npublic static void main(String[] args) {\n    Worker.Factory factory = new Worker.Factory("test-domain");\n    Worker worker = factory.newWorker("HelloWorldTaskList");\n    worker.registerWorkflowImplementationTypes(HelloWorldImpl.class);\n    factory.start();\n}\n\n\n# Execute Hello World Workflow using the CLI\nNow run the program. Following is an example log:\n\n13:35:02.575 [main] INFO  c.u.c.s.WorkflowServiceTChannel - Initialized TChannel for service cadence-frontend, LibraryVersion: 2.2.0, FeatureVersion: 1.0.0\n13:35:02.671 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'Workflow Poller taskList="HelloWorldTaskList", domain="test-domain", type="workflow"\'}, identity=45937@maxim-C02XD0AAJGH6}\n13:35:02.673 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n\n\nNo Hello printed. This is expected because a is just a code host. The has to be started to execute. Let\'s use Cadence to start the workflow:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start --tasklist HelloWorldTaskList --workflow_type HelloWorld::sayHello --execution_timeout 3600 --input \\"World\\"\nStarted Workflow Id: bcacfabd-9f9a-46ac-9b25-83bcea5d7fd7, run Id: e7c40431-8e23-485b-9649-e8f161219efe\n\n\nThe output of the program should change to:\n\n13:35:02.575 [main] INFO  c.u.c.s.WorkflowServiceTChannel - Initialized TChannel for service cadence-frontend, LibraryVersion: 2.2.0, FeatureVersion: 1.0.0\n13:35:02.671 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'Workflow Poller taskList="HelloWorldTaskList", domain="test-domain", type="workflow"\'}, identity=45937@maxim-C02XD0AAJGH6}\n13:35:02.673 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n13:40:28.308 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - Hello World!\n\n\nLet\'s start another \n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start --tasklist HelloWorldTaskList --workflow_type HelloWorld::sayHello --execution_timeout 3600 --input \\"Cadence\\"\nStarted Workflow Id: d2083532-9c68-49ab-90e1-d960175377a7, run Id: 331bfa04-834b-45a7-861e-bcb9f6ddae3e\n\n\nAnd the output changed to:\n\n13:35:02.575 [main] INFO  c.u.c.s.WorkflowServiceTChannel - Initialized TChannel for service cadence-frontend, LibraryVersion: 2.2.0, FeatureVersion: 1.0.0\n13:35:02.671 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'Workflow Poller taskList="HelloWorldTaskList", domain="test-domain", type="workflow"\'}, identity=45937@maxim-C02XD0AAJGH6}\n13:35:02.673 [main] INFO  c.u.cadence.internal.worker.Poller - start(): Poller{options=PollerOptions{maximumPollRateIntervalMilliseconds=1000, maximumPollRatePerSecond=0.0, pollBackoffCoefficient=2.0, pollBackoffInitialInterval=PT0.2S, pollBackoffMaximumInterval=PT20S, pollThreadCount=1, pollThreadNamePrefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n13:40:28.308 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - Hello World!\n13:42:34.994 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - Hello Cadence!\n\n\n# List Workflows and Workflow History\nLet\'s list our in the \n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow list\n             WORKFLOW TYPE            |             WORKFLOW ID              |                RUN ID                | START TIME | EXECUTION TIME | END TIME\n  HelloWorld::sayHello                | d2083532-9c68-49ab-90e1-d960175377a7 | 331bfa04-834b-45a7-861e-bcb9f6ddae3e | 20:42:34   | 20:42:34       | 20:42:35\n  HelloWorld::sayHello                | bcacfabd-9f9a-46ac-9b25-83bcea5d7fd7 | e7c40431-8e23-485b-9649-e8f161219efe | 20:40:28   | 20:40:28       | 20:40:29\n\n\nNow let\'s look at the history:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow showid 1965109f-607f-4b14-a5f2-24399a7b8fa7\n  1  WorkflowExecutionStarted    {WorkflowType:{Name:HelloWorld::sayHello},\n                                  TaskList:{Name:HelloWorldTaskList},\n                                  Input:["World"],\n                                  ExecutionStartToCloseTimeoutSeconds:3600,\n                                  TaskStartToCloseTimeoutSeconds:10,\n                                  ContinuedFailureDetails:[],\n                                  LastCompletionResult:[],\n                                  Identity:cadence-cli@linuxkit-025000000001,\n                                  Attempt:0,\n                                  FirstDecisionTaskBackoffSeconds:0}\n  2  DecisionTaskScheduled       {TaskList:{Name:HelloWorldTaskList},\n                                  StartToCloseTimeoutSeconds:10,\n                                  Attempt:0}\n  3  DecisionTaskStarted         {ScheduledEventId:2,\n                                  Identity:45937@maxim-C02XD0AAJGH6,\n                                  RequestId:481a14e5-67a4-436e-9a23-7f7fb7f87ef3}\n  4  DecisionTaskCompleted       {ExecutionContext:[],\n                                  ScheduledEventId:2,\n                                  StartedEventId:3,\n                                  Identity:45937@maxim-C02XD0AAJGH6}\n  5  WorkflowExecutionCompleted  {Result:[],\n                                  DecisionTaskCompletedEventId:4}\n\n\nEven for such a trivial , the history gives a lot of useful information. For complex this is a really useful tool for production and development troubleshooting. History can be automatically archived to a long-term blob store (for example Amazon S3) upon completion for compliance, analytical, and troubleshooting purposes.\n\n# What is Next\nNow you have completed the tutorials. You can continue to explore the key concepts in Cadence, and also how to use them with Java Client',normalizedContent:'# java hello world\nthis section only describe how to write and run a helloworld with java. go to cadence-java-samples for more examples, and java-client and java-docs for more documentation.\n\n# include cadence java client dependency\ngo to the maven repository uber cadence java client pageand find the latest version of the library. include it as a dependency into your java project. for example if you are using gradle the dependency looks like:\n\ncompile group: \'com.uber.cadence\', name: \'cadence-client\', version: \'<latest_version>\'\n\n\nalso add the following dependencies that cadence-client relies on:\n\ncompile group: \'commons-configuration\', name: \'commons-configuration\', version: \'1.9\'\ncompile group: \'ch.qos.logback\', name: \'logback-classic\', version: \'1.2.3\'\n\n\nmake sure that the following code compiles:\n\nimport com.uber.cadence.workflow.workflow;\nimport com.uber.cadence.workflow.workflowmethod;\nimport org.slf4j.logger;\n\npublic class gettingstarted {\n\n    private static logger logger = workflow.getlogger(gettingstarted.class);\n\n    interface helloworld {\n        @workflowmethod\n        void sayhello(string name);\n    }\n\n}\n\n\nif you are having problems setting up the build files use thecadence java samples github repository as a reference.\n\nalso add the following logback config file somewhere in your classpath:\n\n<configuration>\n    <appender name="stdout" class="ch.qos.logback.core.consoleappender">\n        \x3c!-- encoders are assigned the type\n             ch.qos.logback.classic.encoder.patternlayoutencoder by default --\x3e\n        <encoder>\n            <pattern>%d{hh:mm:ss.sss} [%thread] %-5level %logger{36} - %msg%n</pattern>\n        </encoder>\n    </appender>\n    <logger name="io.netty" level="info"/>\n    <root level="info">\n        <appender-ref ref="stdout" />\n    </root>\n</configuration>\n\n\n# implement hello world workflow\nlet\'s add helloworldimpl with the sayhello method that just logs the "hello ..." and returns.\n\nimport com.uber.cadence.worker.worker;\nimport com.uber.cadence.workflow.workflow;\nimport com.uber.cadence.workflow.workflowmethod;\nimport org.slf4j.logger;\n\npublic class gettingstarted {\n\n    private static logger logger = workflow.getlogger(gettingstarted.class);\n\n    public interface helloworld {\n        @workflowmethod\n        void sayhello(string name);\n    }\n\n    public static class helloworldimpl implements helloworld {\n\n        @override\n        public void sayhello(string name) {\n            logger.info("hello " + name + "!");\n        }\n    }\n}\n\n\nto link the implementation to the cadence framework, it should be registered with a that connects to a cadence service. by default the connects to the locally running cadence service.\n\npublic static void main(string[] args) {\n  workflowclient workflowclient =\n      workflowclient.newinstance(\n          new workflowservicetchannel(clientoptions.defaultinstance()),\n          workflowclientoptions.newbuilder().setdomain(domain).build());\n  // get worker to poll the task list.\n  workerfactory factory = workerfactory.newinstance(workflowclient);\n  worker worker = factory.newworker(task_list);\n  worker.registerworkflowimplementationtypes(helloworldimpl.class);\n  factory.start();\n}\n\n\nthe code is slightly different if you are using client version prior to 3.0.0:\n\npublic static void main(string[] args) {\n    worker.factory factory = new worker.factory("test-domain");\n    worker worker = factory.newworker("helloworldtasklist");\n    worker.registerworkflowimplementationtypes(helloworldimpl.class);\n    factory.start();\n}\n\n\n# execute hello world workflow using the cli\nnow run the program. following is an example log:\n\n13:35:02.575 [main] info  c.u.c.s.workflowservicetchannel - initialized tchannel for service cadence-frontend, libraryversion: 2.2.0, featureversion: 1.0.0\n13:35:02.671 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'workflow poller tasklist="helloworldtasklist", domain="test-domain", type="workflow"\'}, identity=45937@maxim-c02xd0aajgh6}\n13:35:02.673 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n\n\nno hello printed. this is expected because a is just a code host. the has to be started to execute. let\'s use cadence to start the workflow:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start --tasklist helloworldtasklist --workflow_type helloworld::sayhello --execution_timeout 3600 --input \\"world\\"\nstarted workflow id: bcacfabd-9f9a-46ac-9b25-83bcea5d7fd7, run id: e7c40431-8e23-485b-9649-e8f161219efe\n\n\nthe output of the program should change to:\n\n13:35:02.575 [main] info  c.u.c.s.workflowservicetchannel - initialized tchannel for service cadence-frontend, libraryversion: 2.2.0, featureversion: 1.0.0\n13:35:02.671 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'workflow poller tasklist="helloworldtasklist", domain="test-domain", type="workflow"\'}, identity=45937@maxim-c02xd0aajgh6}\n13:35:02.673 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n13:40:28.308 [workflow-root] info  c.u.c.samples.hello.gettingstarted - hello world!\n\n\nlet\'s start another \n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start --tasklist helloworldtasklist --workflow_type helloworld::sayhello --execution_timeout 3600 --input \\"cadence\\"\nstarted workflow id: d2083532-9c68-49ab-90e1-d960175377a7, run id: 331bfa04-834b-45a7-861e-bcb9f6ddae3e\n\n\nand the output changed to:\n\n13:35:02.575 [main] info  c.u.c.s.workflowservicetchannel - initialized tchannel for service cadence-frontend, libraryversion: 2.2.0, featureversion: 1.0.0\n13:35:02.671 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'workflow poller tasklist="helloworldtasklist", domain="test-domain", type="workflow"\'}, identity=45937@maxim-c02xd0aajgh6}\n13:35:02.673 [main] info  c.u.cadence.internal.worker.poller - start(): poller{options=polleroptions{maximumpollrateintervalmilliseconds=1000, maximumpollratepersecond=0.0, pollbackoffcoefficient=2.0, pollbackoffinitialinterval=pt0.2s, pollbackoffmaximuminterval=pt20s, pollthreadcount=1, pollthreadnameprefix=\'null\'}, identity=81b8d0ac-ff89-47e8-b842-3dd26337feea}\n13:40:28.308 [workflow-root] info  c.u.c.samples.hello.gettingstarted - hello world!\n13:42:34.994 [workflow-root] info  c.u.c.samples.hello.gettingstarted - hello cadence!\n\n\n# list workflows and workflow history\nlet\'s list our in the \n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow list\n             workflow type            |             workflow id              |                run id                | start time | execution time | end time\n  helloworld::sayhello                | d2083532-9c68-49ab-90e1-d960175377a7 | 331bfa04-834b-45a7-861e-bcb9f6ddae3e | 20:42:34   | 20:42:34       | 20:42:35\n  helloworld::sayhello                | bcacfabd-9f9a-46ac-9b25-83bcea5d7fd7 | e7c40431-8e23-485b-9649-e8f161219efe | 20:40:28   | 20:40:28       | 20:40:29\n\n\nnow let\'s look at the history:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow showid 1965109f-607f-4b14-a5f2-24399a7b8fa7\n  1  workflowexecutionstarted    {workflowtype:{name:helloworld::sayhello},\n                                  tasklist:{name:helloworldtasklist},\n                                  input:["world"],\n                                  executionstarttoclosetimeoutseconds:3600,\n                                  taskstarttoclosetimeoutseconds:10,\n                                  continuedfailuredetails:[],\n                                  lastcompletionresult:[],\n                                  identity:cadence-cli@linuxkit-025000000001,\n                                  attempt:0,\n                                  firstdecisiontaskbackoffseconds:0}\n  2  decisiontaskscheduled       {tasklist:{name:helloworldtasklist},\n                                  starttoclosetimeoutseconds:10,\n                                  attempt:0}\n  3  decisiontaskstarted         {scheduledeventid:2,\n                                  identity:45937@maxim-c02xd0aajgh6,\n                                  requestid:481a14e5-67a4-436e-9a23-7f7fb7f87ef3}\n  4  decisiontaskcompleted       {executioncontext:[],\n                                  scheduledeventid:2,\n                                  startedeventid:3,\n                                  identity:45937@maxim-c02xd0aajgh6}\n  5  workflowexecutioncompleted  {result:[],\n                                  decisiontaskcompletedeventid:4}\n\n\neven for such a trivial , the history gives a lot of useful information. for complex this is a really useful tool for production and development troubleshooting. history can be automatically archived to a long-term blob store (for example amazon s3) upon completion for compliance, analytical, and troubleshooting purposes.\n\n# what is next\nnow you have completed the tutorials. you can continue to explore the key concepts in cadence, and also how to use them with java client',charsets:{cjk:!0}},{title:"Golang hello world",frontmatter:{layout:"default",title:"Golang hello world",permalink:"/docs/get-started/golang-hello-world",readingShow:"top"},regularPath:"/docs/01-get-started/03-golang-hello-world.html",relativePath:"docs/01-get-started/03-golang-hello-world.md",key:"v-6190b03c",path:"/docs/get-started/golang-hello-world/",headers:[{level:2,title:"Include Golang cadence-client dependency",slug:"include-golang-cadence-client-dependency",normalizedTitle:"include golang cadence-client dependency",charIndex:194},{level:2,title:"Implement HelloWorld Workflow",slug:"implement-helloworld-workflow",normalizedTitle:"implement helloworld workflow",charIndex:334},{level:2,title:"Implement Worker",slug:"implement-worker",normalizedTitle:"implement worker",charIndex:1519},{level:2,title:"Start the Worker and Workflow",slug:"start-the-worker-and-workflow",normalizedTitle:"start the worker and workflow",charIndex:1611},{level:2,title:"What is Next",slug:"what-is-next",normalizedTitle:"what is next",charIndex:1821}],headersStr:"Include Golang cadence-client dependency Implement HelloWorld Workflow Implement Worker Start the Worker and Workflow What is Next",content:'# Golang Hello World\nThis section only describe how to write and run a HelloWorld with Golang. Go to Cadence-Samples for more examples, and Cadence-Client and go-docs for more documentation.\n\n# Include Golang cadence-client dependency\nAdd cadence-client library to your project by running the command:\n\ngo get go.uber.org/cadence\n\n\n# Implement HelloWorld Workflow\npackage main\n\nimport (\n\t"context"\n\t"time"\n\n\t"go.uber.org/cadence/activity"\n\t"go.uber.org/cadence/workflow"\n\t"go.uber.org/zap"\n)\n\n/**\n * This is the hello world workflow sample.\n */\n\n// ApplicationName is the task list for this sample\nconst ApplicationName = "helloWorldGroup"\n\n// helloWorkflow workflow decider\nfunc helloWorldWorkflow(ctx workflow.Context, name string) error {\n\tao := workflow.ActivityOptions{\n\t\tScheduleToStartTimeout: time.Minute,\n\t\tStartToCloseTimeout:    time.Minute,\n\t\tHeartbeatTimeout:       time.Second * 20,\n\t}\n\tctx = workflow.WithActivityOptions(ctx, ao)\n\n\tlogger := workflow.GetLogger(ctx)\n\tlogger.Info("helloworld workflow started")\n\tvar helloworldResult string\n\terr := workflow.ExecuteActivity(ctx, helloWorldActivity, name).Get(ctx, &helloworldResult)\n\tif err != nil {\n\t\tlogger.Error("Activity failed.", zap.Error(err))\n\t\treturn err\n\t}\n\n\tlogger.Info("Workflow completed.", zap.String("Result", helloworldResult))\n\n\treturn nil\n}\n\nfunc helloWorldActivity(ctx context.Context, name string) (string, error) {\n\tlogger := activity.GetLogger(ctx)\n\tlogger.Info("helloworld activity started")\n\treturn "Hello " + name + "!", nil\n}\n\n\n# Implement Worker\nFollow the worker documentation or the samples to implement the worker.\n\n# Start the Worker and Workflow\nBuild the worker program with your project. You can follow the way in the sample.\n\nThen start the worker:\n\n./bin/helloworld -m worker\n\n\nAnd start a workflow:\n\n./bin/helloworld\n\n\n# What is Next\nNow you have completed the tutorials. You can continue to explore the key concepts in Cadence, and also how to use them with Go Client',normalizedContent:'# golang hello world\nthis section only describe how to write and run a helloworld with golang. go to cadence-samples for more examples, and cadence-client and go-docs for more documentation.\n\n# include golang cadence-client dependency\nadd cadence-client library to your project by running the command:\n\ngo get go.uber.org/cadence\n\n\n# implement helloworld workflow\npackage main\n\nimport (\n\t"context"\n\t"time"\n\n\t"go.uber.org/cadence/activity"\n\t"go.uber.org/cadence/workflow"\n\t"go.uber.org/zap"\n)\n\n/**\n * this is the hello world workflow sample.\n */\n\n// applicationname is the task list for this sample\nconst applicationname = "helloworldgroup"\n\n// helloworkflow workflow decider\nfunc helloworldworkflow(ctx workflow.context, name string) error {\n\tao := workflow.activityoptions{\n\t\tscheduletostarttimeout: time.minute,\n\t\tstarttoclosetimeout:    time.minute,\n\t\theartbeattimeout:       time.second * 20,\n\t}\n\tctx = workflow.withactivityoptions(ctx, ao)\n\n\tlogger := workflow.getlogger(ctx)\n\tlogger.info("helloworld workflow started")\n\tvar helloworldresult string\n\terr := workflow.executeactivity(ctx, helloworldactivity, name).get(ctx, &helloworldresult)\n\tif err != nil {\n\t\tlogger.error("activity failed.", zap.error(err))\n\t\treturn err\n\t}\n\n\tlogger.info("workflow completed.", zap.string("result", helloworldresult))\n\n\treturn nil\n}\n\nfunc helloworldactivity(ctx context.context, name string) (string, error) {\n\tlogger := activity.getlogger(ctx)\n\tlogger.info("helloworld activity started")\n\treturn "hello " + name + "!", nil\n}\n\n\n# implement worker\nfollow the worker documentation or the samples to implement the worker.\n\n# start the worker and workflow\nbuild the worker program with your project. you can follow the way in the sample.\n\nthen start the worker:\n\n./bin/helloworld -m worker\n\n\nand start a workflow:\n\n./bin/helloworld\n\n\n# what is next\nnow you have completed the tutorials. you can continue to explore the key concepts in cadence, and also how to use them with go client',charsets:{}},{title:"Video Tutorials",frontmatter:{layout:"default",title:"Video Tutorials",permalink:"/docs/get-started/video-tutorials",readingShow:"top"},regularPath:"/docs/01-get-started/04-video-tutorials.html",relativePath:"docs/01-get-started/04-video-tutorials.md",key:"v-0403e040",path:"/docs/get-started/video-tutorials/",headers:[{level:2,title:"HelloWorld",slug:"helloworld",normalizedTitle:"helloworld",charIndex:86}],headersStr:"HelloWorld",content:"# Overview\nAn Introduction to the Cadence programming model and value proposition.\n\n# HelloWorld\nA step-by-step video tutorial about how to install and run HellowWorld(Java).",normalizedContent:"# overview\nan introduction to the cadence programming model and value proposition.\n\n# helloworld\na step-by-step video tutorial about how to install and run hellowworld(java).",charsets:{}},{title:"Overview",frontmatter:{layout:"default",title:"Overview",description:"A large number of use cases span beyond a single request-reply, require tracking of a complex state, respond to asynchronous events, and communicate to external unreliable dependencies.",permalink:"/docs/get-started/",readingShow:"top"},regularPath:"/docs/01-get-started/",relativePath:"docs/01-get-started/index.md",key:"v-00cc835b",path:"/docs/get-started/",headers:[{level:2,title:"What's Next",slug:"what-s-next",normalizedTitle:"what's next",charIndex:1985}],headersStr:"What's Next",content:"# Overview\nA large number of use cases span beyond a single request-reply, require tracking of a complex state, respond to asynchronous , and communicate to external unreliable dependencies. The usual approach to building such applications is a hodgepodge of stateless services, databases, cron jobs, and queuing systems. This negatively impacts the developer productivity as most of the code is dedicated to plumbing, obscuring the actual business logic behind a myriad of low-level details. Such systems frequently have availability problems as it is hard to keep all the components healthy.\n\nThe Cadence solution is a fault-oblivious stateful programming model that obscures most of the complexities of building scalable distributed applications. In essence, Cadence provides a durable virtual memory that is not linked to a specific process, and preserves the full application state, including function stacks, with local variables across all sorts of host and software failures. This allows you to write code using the full power of a programming language while Cadence takes care of durability, availability, and scalability of the application.\n\nCadence consists of a programming framework (or client library) and a managed service (or backend). The framework enables developers to author and coordinate in familiar languages (Go and Javaare supported officially, and Python andRuby by the community.\n\nThe backend service is stateless and relies on a persistent store. Currently, Cassandra and MySQL/Postgres storages are supported. An adapter to any other database that provides multi-row single shard transactions can be added. There are different service deployment models. At Uber, our team operates multitenant clusters that are shared by hundreds of applications. See service topology to understand the overall architecture. The GitHub repo for the Cadence server is uber/cadence. The docker image for the Cadence server is available on Docker Hub atubercadence/server.\n\n# What's Next\nLet's try with some sample workflows. To start with, go to server installation to install cadence locally, and run a HelloWorld sample with Java or Golang.\n\nWhen you have any trouble with the instructions, you can watch the video tutorials, and reach out to us on Slack Channel, or raise any question on StackOverflow or open an Github issue.",normalizedContent:"# overview\na large number of use cases span beyond a single request-reply, require tracking of a complex state, respond to asynchronous , and communicate to external unreliable dependencies. the usual approach to building such applications is a hodgepodge of stateless services, databases, cron jobs, and queuing systems. this negatively impacts the developer productivity as most of the code is dedicated to plumbing, obscuring the actual business logic behind a myriad of low-level details. such systems frequently have availability problems as it is hard to keep all the components healthy.\n\nthe cadence solution is a fault-oblivious stateful programming model that obscures most of the complexities of building scalable distributed applications. in essence, cadence provides a durable virtual memory that is not linked to a specific process, and preserves the full application state, including function stacks, with local variables across all sorts of host and software failures. this allows you to write code using the full power of a programming language while cadence takes care of durability, availability, and scalability of the application.\n\ncadence consists of a programming framework (or client library) and a managed service (or backend). the framework enables developers to author and coordinate in familiar languages (go and javaare supported officially, and python andruby by the community.\n\nthe backend service is stateless and relies on a persistent store. currently, cassandra and mysql/postgres storages are supported. an adapter to any other database that provides multi-row single shard transactions can be added. there are different service deployment models. at uber, our team operates multitenant clusters that are shared by hundreds of applications. see service topology to understand the overall architecture. the github repo for the cadence server is uber/cadence. the docker image for the cadence server is available on docker hub atubercadence/server.\n\n# what's next\nlet's try with some sample workflows. to start with, go to server installation to install cadence locally, and run a helloworld sample with java or golang.\n\nwhen you have any trouble with the instructions, you can watch the video tutorials, and reach out to us on slack channel, or raise any question on stackoverflow or open an github issue.",charsets:{}},{title:"Periodic execution",frontmatter:{layout:"default",title:"Periodic execution",permalink:"/docs/use-cases/periodic-execution",readingShow:"top"},regularPath:"/docs/02-use-cases/01-periodic-execution.html",relativePath:"docs/02-use-cases/01-periodic-execution.md",key:"v-da0a32bc",path:"/docs/use-cases/periodic-execution/",headersStr:null,content:"# Periodic execution (aka Distributed Cron)\nPeriodic execution, frequently referred to as distributed cron, is when you execute business logic periodically. The advantage of Cadence for these scenarios is that it guarantees execution, sophisticated error handling, retry policies, and visibility into execution history.\n\nAnother important dimension is scale. Some use cases require periodic execution for a large number of entities. At Uber, there are applications that create periodic per customer. Imagine 100+ million parallel cron jobs that don't require a separate batch processing framework.\n\nPeriodic execution is often part of other use cases. For example, once a month report generation is a periodic service orchestration. Or an event-driven that accumulates loyalty points for a customer and applies those points once a month.\n\nThere are many real-world examples of Cadence periodic executions. Such as the following:\n\n * An Uber backend service that recalculates various statistics for each hex in each city once a minute.\n * Monthly Uber for Business report generation.",normalizedContent:"# periodic execution (aka distributed cron)\nperiodic execution, frequently referred to as distributed cron, is when you execute business logic periodically. the advantage of cadence for these scenarios is that it guarantees execution, sophisticated error handling, retry policies, and visibility into execution history.\n\nanother important dimension is scale. some use cases require periodic execution for a large number of entities. at uber, there are applications that create periodic per customer. imagine 100+ million parallel cron jobs that don't require a separate batch processing framework.\n\nperiodic execution is often part of other use cases. for example, once a month report generation is a periodic service orchestration. or an event-driven that accumulates loyalty points for a customer and applies those points once a month.\n\nthere are many real-world examples of cadence periodic executions. such as the following:\n\n * an uber backend service that recalculates various statistics for each hex in each city once a minute.\n * monthly uber for business report generation.",charsets:{}},{title:"Orchestration",frontmatter:{layout:"default",title:"Orchestration",permalink:"/docs/use-cases/orchestration",readingShow:"top"},regularPath:"/docs/02-use-cases/02-orchestration.html",relativePath:"docs/02-use-cases/02-orchestration.md",key:"v-1de42eb0",path:"/docs/use-cases/orchestration/",headersStr:null,content:"# Microservice Orchestration and Saga\nIt is common that some business processes are implemented as multiple microservice calls. And the implementation must guarantee that all of the calls must eventually succeed even with the occurrence of prolonged downstream service failures. In some cases, instead of trying to complete the process by retrying for a long time, compensation rollback logic should be executed.Saga Pattern is one way to standardize on compensation APIs.\n\nCadence is a perfect fit for such scenarios. It guarantees that code eventually completes, has built-in support for unlimited exponential retries and simplifies coding of the compensation logic. It also gives full visibility into the state of each , in contrast to an orchestration based on queues where getting a current status of each individual request is practically impossible.\n\nFollowing are some real-world examples of Cadence-based service orchestration scenarios:\n\n * Using Cadence workflows to spin up Kubernetes by Banzai Cloud\n * Improving the User Experience with Uber’s Customer Obsession Ticket Routing Workflow and Orchestration Engine",normalizedContent:"# microservice orchestration and saga\nit is common that some business processes are implemented as multiple microservice calls. and the implementation must guarantee that all of the calls must eventually succeed even with the occurrence of prolonged downstream service failures. in some cases, instead of trying to complete the process by retrying for a long time, compensation rollback logic should be executed.saga pattern is one way to standardize on compensation apis.\n\ncadence is a perfect fit for such scenarios. it guarantees that code eventually completes, has built-in support for unlimited exponential retries and simplifies coding of the compensation logic. it also gives full visibility into the state of each , in contrast to an orchestration based on queues where getting a current status of each individual request is practically impossible.\n\nfollowing are some real-world examples of cadence-based service orchestration scenarios:\n\n * using cadence workflows to spin up kubernetes by banzai cloud\n * improving the user experience with uber’s customer obsession ticket routing workflow and orchestration engine",charsets:{}},{title:"Polling",frontmatter:{layout:"default",title:"Polling",permalink:"/docs/use-cases/polling",readingShow:"top"},regularPath:"/docs/02-use-cases/03-polling.html",relativePath:"docs/02-use-cases/03-polling.md",key:"v-8cf1c7ac",path:"/docs/use-cases/polling/",headersStr:null,content:"# Polling\nPolling is executing a periodic action checking for a state change. Examples are pinging a host, calling a REST API, or listing an Amazon S3 bucket for newly uploaded files.\n\nCadence support for long running and unlimited retries makes it a good fit.\n\nSome real-world use cases:\n\n * Network, host and service monitoring\n * Processing files uploaded to FTP or S3\n * Polling an external API for a specific resource to become available",normalizedContent:"# polling\npolling is executing a periodic action checking for a state change. examples are pinging a host, calling a rest api, or listing an amazon s3 bucket for newly uploaded files.\n\ncadence support for long running and unlimited retries makes it a good fit.\n\nsome real-world use cases:\n\n * network, host and service monitoring\n * processing files uploaded to ftp or s3\n * polling an external api for a specific resource to become available",charsets:{}},{title:"Event driven application",frontmatter:{layout:"default",title:"Event driven application",permalink:"/docs/use-cases/event-driven",readingShow:"top"},regularPath:"/docs/02-use-cases/04-event-driven.html",relativePath:"docs/02-use-cases/04-event-driven.md",key:"v-7215aabc",path:"/docs/use-cases/event-driven/",headersStr:null,content:"# Event driven application\nMany applications listen to multiple sources, update the state of correspondent business entities, and have to execute actions if some state is reached. Cadence is a good fit for many of these. It has direct support for asynchronous (aka ), has a simple programming model that obscures a lot of complexity around state persistence, and ensures external action execution through built-in retries.\n\nReal-world examples:\n\n * Fraud detection where reacts to generated by consumer behavior\n * Customer loyalty program where the accumulates reward points and applies them when requested",normalizedContent:"# event driven application\nmany applications listen to multiple sources, update the state of correspondent business entities, and have to execute actions if some state is reached. cadence is a good fit for many of these. it has direct support for asynchronous (aka ), has a simple programming model that obscures a lot of complexity around state persistence, and ensures external action execution through built-in retries.\n\nreal-world examples:\n\n * fraud detection where reacts to generated by consumer behavior\n * customer loyalty program where the accumulates reward points and applies them when requested",charsets:{}},{title:"Storage scan",frontmatter:{layout:"default",title:"Storage scan",permalink:"/docs/use-cases/partitioned-scan",readingShow:"top"},regularPath:"/docs/02-use-cases/05-partitioned-scan.html",relativePath:"docs/02-use-cases/05-partitioned-scan.md",key:"v-1b6ed5fc",path:"/docs/use-cases/partitioned-scan/",headersStr:null,content:"# Storage scan\nIt is common to have large data sets partitioned across a large number of hosts or databases, or having billions of files in an Amazon S3 bucket. Cadence is an ideal solution for implementing the full scan of such data in a scalable and resilient way. The standard pattern is to run an (or multiple parallel for partitioned data sets) that performs the scan and heartbeats its progress back to Cadence. In the case of a host failure, the is retried on a different host and continues execution from the last reported progress.\n\nA real-world example:\n\n * Cadence internal system that performs periodic scan of all records",normalizedContent:"# storage scan\nit is common to have large data sets partitioned across a large number of hosts or databases, or having billions of files in an amazon s3 bucket. cadence is an ideal solution for implementing the full scan of such data in a scalable and resilient way. the standard pattern is to run an (or multiple parallel for partitioned data sets) that performs the scan and heartbeats its progress back to cadence. in the case of a host failure, the is retried on a different host and continues execution from the last reported progress.\n\na real-world example:\n\n * cadence internal system that performs periodic scan of all records",charsets:{}},{title:"Batch job",frontmatter:{layout:"default",title:"Batch job",permalink:"/docs/use-cases/batch-job",readingShow:"top"},regularPath:"/docs/02-use-cases/06-batch-job.html",relativePath:"docs/02-use-cases/06-batch-job.md",key:"v-2044e7d6",path:"/docs/use-cases/batch-job/",headersStr:null,content:"# Batch job\nA lot of batch jobs are not pure data manipulation programs. For those, the existing big data frameworks are the best fit. But if processing a record requires external API calls that might fail and potentially take a long time, Cadence might be preferable.\n\nOne of our internal Uber customer uses Cadence for end of month statement generation. Each statement requires calls to multiple microservices and some statements can be really large. Cadence was chosen because it provides hard guarantees around durability of the financial data and seamlessly deals with long running operations, retries, and intermittent failures.",normalizedContent:"# batch job\na lot of batch jobs are not pure data manipulation programs. for those, the existing big data frameworks are the best fit. but if processing a record requires external api calls that might fail and potentially take a long time, cadence might be preferable.\n\none of our internal uber customer uses cadence for end of month statement generation. each statement requires calls to multiple microservices and some statements can be really large. cadence was chosen because it provides hard guarantees around durability of the financial data and seamlessly deals with long running operations, retries, and intermittent failures.",charsets:{}},{title:"Infrastructure provisioning",frontmatter:{layout:"default",title:"Infrastructure provisioning",permalink:"/docs/use-cases/provisioning",readingShow:"top"},regularPath:"/docs/02-use-cases/07-provisioning.html",relativePath:"docs/02-use-cases/07-provisioning.md",key:"v-7557d6c2",path:"/docs/use-cases/provisioning/",headersStr:null,content:"# Infrastructure provisioning\nProvisioning a new datacenter or a pool of machines in a public cloud is a potentially long running operation with a lot of possibilities for intermittent failures. The scale is also a concern when tens or even hundreds of thousands of resources should be provisioned and configured. One useful feature for provisioning scenarios is Cadence support for routing execution to a specific process or host.\n\nA lot of operations require some sort of locking to ensure that no more than one mutation is executed on a resource at a time. Cadence provides strong guarantees of uniqueness by business ID. This can be used to implement such locking behavior in a fault tolerant and scalable manner.\n\nSome real-world use cases:\n\n * Using Cadence workflows to spin up Kubernetes, by Banzai Cloud\n * Using Cadence to orchestrate cluster life cycle in HashiCorp Consul, by HashiCorp",normalizedContent:"# infrastructure provisioning\nprovisioning a new datacenter or a pool of machines in a public cloud is a potentially long running operation with a lot of possibilities for intermittent failures. the scale is also a concern when tens or even hundreds of thousands of resources should be provisioned and configured. one useful feature for provisioning scenarios is cadence support for routing execution to a specific process or host.\n\na lot of operations require some sort of locking to ensure that no more than one mutation is executed on a resource at a time. cadence provides strong guarantees of uniqueness by business id. this can be used to implement such locking behavior in a fault tolerant and scalable manner.\n\nsome real-world use cases:\n\n * using cadence workflows to spin up kubernetes, by banzai cloud\n * using cadence to orchestrate cluster life cycle in hashicorp consul, by hashicorp",charsets:{}},{title:"Deployment",frontmatter:{layout:"default",title:"Deployment",permalink:"/docs/use-cases/deployment",readingShow:"top"},regularPath:"/docs/02-use-cases/08-deployment.html",relativePath:"docs/02-use-cases/08-deployment.md",key:"v-7bd56aa2",path:"/docs/use-cases/deployment/",headersStr:null,content:"# CI/CD and Deployment\nImplementing CI/CD pipelines and deployment of applications to containers or virtual or physical machines is a non-trivial process. Its business logic has to deal with complex requirements around rolling upgrades, canary deployments, and rollbacks. Cadence is a perfect platform for building a deployment solution because it provides all the necessary guarantees and abstractions allowing developers to focus on the business logic.\n\nExample production systems:\n\n * Uber internal deployment infrastructure\n * Update push to IoT devices",normalizedContent:"# ci/cd and deployment\nimplementing ci/cd pipelines and deployment of applications to containers or virtual or physical machines is a non-trivial process. its business logic has to deal with complex requirements around rolling upgrades, canary deployments, and rollbacks. cadence is a perfect platform for building a deployment solution because it provides all the necessary guarantees and abstractions allowing developers to focus on the business logic.\n\nexample production systems:\n\n * uber internal deployment infrastructure\n * update push to iot devices",charsets:{}},{title:"Operational management",frontmatter:{layout:"default",title:"Operational management",permalink:"/docs/use-cases/operational-management",readingShow:"top"},regularPath:"/docs/02-use-cases/09-operational-management.html",relativePath:"docs/02-use-cases/09-operational-management.md",key:"v-194a2c22",path:"/docs/use-cases/operational-management/",headersStr:null,content:"# Operational management\nImagine that you have to create a self operating database similar to Amazon RDS. Cadence is used in multiple projects that automate managing and automatic recovery of various products like MySQL, Elasticsearch and Apache Cassandra.\n\nSuch systems are usually a mixture of different use cases. They need to monitor the status of resources using polling. They have to execute orchestration API calls to administrative interfaces of a database. They have to provision new hardware or Docker instances if necessary. They need to push configuration updates and perform other actions like backups periodically.",normalizedContent:"# operational management\nimagine that you have to create a self operating database similar to amazon rds. cadence is used in multiple projects that automate managing and automatic recovery of various products like mysql, elasticsearch and apache cassandra.\n\nsuch systems are usually a mixture of different use cases. they need to monitor the status of resources using polling. they have to execute orchestration api calls to administrative interfaces of a database. they have to provision new hardware or docker instances if necessary. they need to push configuration updates and perform other actions like backups periodically.",charsets:{}},{title:"Interactive application",frontmatter:{layout:"default",title:"Interactive application",permalink:"/docs/use-cases/interactive",readingShow:"top"},regularPath:"/docs/02-use-cases/10-interactive.html",relativePath:"docs/02-use-cases/10-interactive.md",key:"v-142b1af4",path:"/docs/use-cases/interactive/",headersStr:null,content:"# Interactive application\nCadence is performant and scalable enough to support interactive applications. It can be used to track UI session state and at the same time execute background operations. For example, while placing an order a customer might need to go through several screens while a background evaluates the customer for fraudulent .",normalizedContent:"# interactive application\ncadence is performant and scalable enough to support interactive applications. it can be used to track ui session state and at the same time execute background operations. for example, while placing an order a customer might need to go through several screens while a background evaluates the customer for fraudulent .",charsets:{}},{title:"DSL workflows",frontmatter:{layout:"default",title:"DSL workflows",permalink:"/docs/use-cases/dsl",readingShow:"top"},regularPath:"/docs/02-use-cases/11-dsl.html",relativePath:"docs/02-use-cases/11-dsl.md",key:"v-a9abb788",path:"/docs/use-cases/dsl/",headersStr:null,content:'# DSL workflows\nCadence supports implementing business logic directly in programming languages like Java and Go. But there are cases when using a domain-specific language is more appropriate. Or there might be a legacy system that uses some form of DSL for process definition but it is not operationally stable and scalable. This also applies to more recent systems like Apache Airflow, various BPMN engines and AWS Step Functions.\n\nAn application that interprets the DSL definition can be written using the Cadence SDK. It automatically becomes highly fault tolerant, scalable, and durable when running on Cadence. Cadence has been used to deprecate several Uber internal DSL engines. The customers continue to use existing process definitions, but Cadence is used as an execution engine.\n\nThere are multiple benefits of unifying all company engines on top of Cadence. The most obvious one is that it is more efficient to support a single product instead of many. It is also difficult to beat the scalability and stability of Cadence which each of the integrations it comes with. Additionally, the ability to share across "engines" might be a huge benefit in some cases.',normalizedContent:'# dsl workflows\ncadence supports implementing business logic directly in programming languages like java and go. but there are cases when using a domain-specific language is more appropriate. or there might be a legacy system that uses some form of dsl for process definition but it is not operationally stable and scalable. this also applies to more recent systems like apache airflow, various bpmn engines and aws step functions.\n\nan application that interprets the dsl definition can be written using the cadence sdk. it automatically becomes highly fault tolerant, scalable, and durable when running on cadence. cadence has been used to deprecate several uber internal dsl engines. the customers continue to use existing process definitions, but cadence is used as an execution engine.\n\nthere are multiple benefits of unifying all company engines on top of cadence. the most obvious one is that it is more efficient to support a single product instead of many. it is also difficult to beat the scalability and stability of cadence which each of the integrations it comes with. additionally, the ability to share across "engines" might be a huge benefit in some cases.',charsets:{}},{title:"Big data and ML",frontmatter:{layout:"default",title:"Big data and ML",permalink:"/docs/use-cases/big-ml",readingShow:"top"},regularPath:"/docs/02-use-cases/12-big-ml.html",relativePath:"docs/02-use-cases/12-big-ml.md",key:"v-a0c27e3c",path:"/docs/use-cases/big-ml/",headersStr:null,content:"# Big data and ML\nA lot of companies build custom ETL and ML training and deployment solutions. Cadence is a good fit for a control plane for such applications.\n\nOne important feature of Cadence is its ability to route execution to a specific process or host. It is useful to control how ML models and other large files are allocated to hosts. For example, if an ML model is partitioned by city, the requests should be routed to hosts that contain the corresponding city model.",normalizedContent:"# big data and ml\na lot of companies build custom etl and ml training and deployment solutions. cadence is a good fit for a control plane for such applications.\n\none important feature of cadence is its ability to route execution to a specific process or host. it is useful to control how ml models and other large files are allocated to hosts. for example, if an ml model is partitioned by city, the requests should be routed to hosts that contain the corresponding city model.",charsets:{}},{title:"Introduction",frontmatter:{layout:"default",title:"Introduction",permalink:"/docs/use-cases/",readingShow:"top"},regularPath:"/docs/02-use-cases/",relativePath:"docs/02-use-cases/index.md",key:"v-a16b91ca",path:"/docs/use-cases/",headersStr:null,content:'# Use cases\nAs Cadence developers, we face a difficult non-technical problem: How to position and describe the Cadence platform.\n\nWe call it workflow. But when most people hear the word "workflow" they think about low-code and UIs. While these might be useful for non technical users, they frequently bring more pain than value to software engineers. Most UIs and low-code DSLs are awesome for "hello world" demo applications, but any diagram with 100+ elements or a few thousand lines of JSON DSL is completely impractical. So positioning Cadence as a is not ideal as it turns away developers that would enjoy its code-only approach.\n\nWe call it orchestrator. But this term is pretty narrow and turns away customers that want to implement business process automation solutions.\n\nWe call it durable function platform. It is technically a correct term. But most developers outside of the Microsoft ecosystem have never heard of Durable Functions.\n\nWe believe that problem in naming comes from the fact that Cadence is indeed a new way to write distributed applications. It is generic enough that it can be applied to practically any use case that goes beyond a single request reply. It can be used to build applications that are in traditional areas of or orchestration platforms. But it is also huge developer productivity boost for multiple use cases that traditionally rely on databases and/or queues.\n\nThis section represents a far from complete list of use cases where Cadence is a good fit. All of them have been used by real production services inside and outside of Uber.\n\nDon\'t think of this list as exhaustive. It is common to employ multiple use types in a single application. For example, an operational management use case might need periodic execution, service orchestration, polling, driven, as well as interactive parts.',normalizedContent:'# use cases\nas cadence developers, we face a difficult non-technical problem: how to position and describe the cadence platform.\n\nwe call it workflow. but when most people hear the word "workflow" they think about low-code and uis. while these might be useful for non technical users, they frequently bring more pain than value to software engineers. most uis and low-code dsls are awesome for "hello world" demo applications, but any diagram with 100+ elements or a few thousand lines of json dsl is completely impractical. so positioning cadence as a is not ideal as it turns away developers that would enjoy its code-only approach.\n\nwe call it orchestrator. but this term is pretty narrow and turns away customers that want to implement business process automation solutions.\n\nwe call it durable function platform. it is technically a correct term. but most developers outside of the microsoft ecosystem have never heard of durable functions.\n\nwe believe that problem in naming comes from the fact that cadence is indeed a new way to write distributed applications. it is generic enough that it can be applied to practically any use case that goes beyond a single request reply. it can be used to build applications that are in traditional areas of or orchestration platforms. but it is also huge developer productivity boost for multiple use cases that traditionally rely on databases and/or queues.\n\nthis section represents a far from complete list of use cases where cadence is a good fit. all of them have been used by real production services inside and outside of uber.\n\ndon\'t think of this list as exhaustive. it is common to employ multiple use types in a single application. for example, an operational management use case might need periodic execution, service orchestration, polling, driven, as well as interactive parts.',charsets:{}},{title:"Workflows",frontmatter:{layout:"default",title:"Workflows",permalink:"/docs/concepts/workflows",readingShow:"top"},regularPath:"/docs/03-concepts/01-workflows.html",relativePath:"docs/03-concepts/01-workflows.md",key:"v-13e86a42",path:"/docs/concepts/workflows/",headers:[{level:2,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:43},{level:2,title:"Example",slug:"example",normalizedTitle:"example",charIndex:343},{level:2,title:"State Recovery and Determinism",slug:"state-recovery-and-determinism",normalizedTitle:"state recovery and determinism",charIndex:5381},{level:2,title:"ID Uniqueness",slug:"id-uniqueness",normalizedTitle:"id uniqueness",charIndex:6114},{level:2,title:"Child Workflow",slug:"child-workflow",normalizedTitle:"child workflow",charIndex:7238},{level:2,title:"Workflow Retries",slug:"workflow-retries",normalizedTitle:"workflow retries",charIndex:8809},{level:2,title:"How does workflow run",slug:"how-does-workflow-run",normalizedTitle:"how does workflow run",charIndex:10352}],headersStr:"Overview Example State Recovery and Determinism ID Uniqueness Child Workflow Workflow Retries How does workflow run",content:"# Fault-oblivious stateful workflow code\n# Overview\nCadence core abstraction is a fault-oblivious stateful . The state of the code, including local variables and threads it creates, is immune to process and Cadence service failures. This is a very powerful concept as it encapsulates state, processing threads, durable timers and handlers.\n\n# Example\nLet's look at a use case. A customer signs up for an application with a trial period. After the period, if the customer has not cancelled, he should be charged once a month for the renewal. The customer has to be notified by email about the charges and should be able to cancel the subscription at any time.\n\nThe business logic of this use case is not very complicated and can be expressed in a few dozen lines of code. But any practical implementation has to ensure that the business process is fault tolerant and scalable. There are various ways to approach the design of such a system.\n\nOne approach is to center it around a database. An application process would periodically scan database tables for customers in specific states, execute necessary actions, and update the state to reflect that. While feasible, this approach has various drawbacks. The most obvious is that the state machine of the customer state quickly becomes extremely complicated. For example, charging a credit card or sending emails can fail due to a downstream system unavailability. The failed calls might need to be retried for a long time, ideally using an exponential retry policy. These calls should be throttled to not overload external systems. There should be support for poison pills to avoid blocking the whole process if a single customer record cannot be processed for whatever reason. The database-based approach also usually has performance problems. Databases are not efficient for scenarios that require constant polling for records in a specific state.\n\nAnother commonly employed approach is to use a timer service and queues. Any update is pushed to a queue and then a that consumes from it updates a database and possibly pushes more messages in downstream queues. For operations that require scheduling, an external timer service can be used. This approach usually scales much better because a database is not constantly polled for changes. But it makes the programming model more complex and error prone as usually there is no transactional update between a queuing system and a database.\n\nWith Cadence, the entire logic can be encapsulated in a simple durable function that directly implements the business logic. Because the function is stateful, the implementer doesn't need to employ any additional systems to ensure durability and fault tolerance.\n\nHere is an example that implements the subscription management use case. It is in Java, but Go is also supported. The Python and .NET libraries are under active development.\n\npublic interface SubscriptionWorkflow {\n    @WorkflowMethod\n    void execute(String customerId);\n}\n\npublic class SubscriptionWorkflowImpl implements SubscriptionWorkflow {\n\n    private final SubscriptionActivities activities =\n        Workflow.newActivityStub(SubscriptionActivities.class);\n\n    @Override\n    public void execute(String customerId) {\n        activities.sendWelcomeEmail(customerId);\n        try {\n            boolean trialPeriod = true;\n            while (true) {\n                Workflow.sleep(Duration.ofDays(30));\n                activities.chargeMonthlyFee(customerId);\n                if (trialPeriod) {\n                    activities.sendEndOfTrialEmail(customerId);\n                    trialPeriod = false;\n                } else {\n                    activities.sendMonthlyChargeEmail(customerId);\n                }\n            }\n        } catch (CancellationException e) {\n            activities.processSubscriptionCancellation(customerId);\n            activities.sendSorryToSeeYouGoEmail(customerId);\n        }\n    }\n}\n\n\nAgain, note that this code directly implements the business logic. If any of the invoked operations (aka ) takes a long time, the code is not going to change. It is okay to block on chargeMonthlyFee for a day if the downstream processing service is down that long. The same way that blocking sleep for 30 days is a normal operation inside the code.\n\nCadence has practically no scalability limits on the number of open instances. So even if your site has hundreds of millions of consumers, the above code is not going to change.\n\nThe commonly asked question by developers that learn Cadence is \"How do I handle process failure/restart in my \"? The answer is that you do not. The code is completely oblivious to any failures and downtime of or even the Cadence service itself. As soon as they are recovered and the needs to handle some , like timer or an completion, the current state of the is fully restored and the execution is continued. The only reason for a failure is the business code throwing an exception, not underlying infrastructure outages.\n\nAnother commonly asked question is whether a can handle more instances than its cache size or number of threads it can support. The answer is that a , when in a blocked state, can be safely removed from a . Later it can be resurrected on a different or the same when the need (in the form of an external ) arises. So a single can handle millions of open , assuming it can handle the update rate.\n\n# State Recovery and Determinism\nThe state recovery utilizes sourcing which puts a few restrictions on how the code is written. The main restriction is that the code must be deterministic which means that it must produce exactly the same result if executed multiple times. This rules out any external API calls from the code as external calls can fail intermittently or change its output any time. That is why all communication with the external world should happen through . For the same reason, code must use Cadence APIs to get current time, sleep, and create new threads.\n\nTo understand the Cadence execution model as well as the recovery mechanism, watch the following webcast. The animation covering recovery starts at 15:50.\n\n# ID Uniqueness\n is assigned by a client when starting a . It is usually a business level ID like customer ID or order ID.\n\nCadence guarantees that there could be only one (across all types) with a given ID open per at any time. An attempt to start a with the same ID is going to fail with WorkflowExecutionAlreadyStarted error.\n\nAn attempt to start a if there is a completed with the same ID depends on a WorkflowIdReusePolicy option:\n\n * AllowDuplicateFailedOnly means that it is allowed to start a only if a previously executed with the same ID failed.\n * AllowDuplicate means that it is allowed to start independently of the previous completion status.\n * RejectDuplicate means that it is not allowed to start a using the same at all.\n * TerminateIfRunning means terminating the current running workflow if one exists, and start a new one.\n\nThe default is AllowDuplicateFailedOnly.\n\nTo distinguish multiple runs of a with the same , Cadence identifies a with two IDs: Workflow ID and Run ID. Run ID is a service-assigned UUID. To be precise, any is uniquely identified by a triple: Domain Name, Workflow ID and Run ID.\n\n# Child Workflow\nA can execute other as child :workflow:workflows:. A child completion or failure is reported to its parent.\n\nSome reasons to use child are:\n\n * A child can be hosted by a separate set of which don't contain the parent code. So it would act as a separate service that can be invoked from multiple other .\n * A single has a limited size. For example, it cannot execute 100k . Child can be used to partition the problem into smaller chunks. One parent with 1000 children each executing 1000 is 1 million executed .\n * A child can be used to manage some resource using its ID to guarantee uniqueness. For example, a that manages host upgrades can have a child per host (host name being a ) and use them to ensure that all operations on the host are serialized.\n * A child can be used to execute some periodic logic without blowing up the parent history size. When a parent starts a child, it executes periodic logic calling that continues as many times as needed, then completes. From the parent point if view, it is just a single child invocation.\n\nThe main limitation of a child versus collocating all the application logic in a single is lack of the shared state. Parent and child can communicate only through asynchronous . But if there is a tight coupling between them, it might be simpler to use a single and just rely on a shared object state.\n\nWe recommended starting from a single implementation if your problem has bounded size in terms of number of executed and processed . It is more straightforward than multiple asynchronously communicating .\n\n# Workflow Retries\n code is unaffected by infrastructure level downtime and failures. But it still can fail due to business logic level failures. For example, an can fail due to exceeding the retry interval and the error is not handled by application code, or the code having a bug.\n\nSome require a guarantee that they keep running even in presence of such failures. To support such use cases, an optional exponential retry policy can be specified when starting a . When it is specified, a failure restarts a from the beginning after the calculated retry interval. Following are the retry policy parameters:\n\n * InitialInterval is a delay before the first retry.\n * BackoffCoefficient. Retry policies are exponential. The coefficient specifies how fast the retry interval is growing. The coefficient of 1 means that the retry interval is always equal to the InitialInterval.\n * MaximumInterval specifies the maximum interval between retries. Useful for coefficients of more than 1.\n * MaximumAttempts specifies how many times to attempt to execute a in the presence of failures. If this limit is exceeded, the fails without retry. Not required if ExpirationInterval is specified.\n * ExpirationInterval specifies for how long to attempt executing a in the presence of failures. If this interval is exceeded, the fails without retry. Not required if MaximumAttempts is specified.\n * NonRetryableErrorReasons allows to specify errors that shouldn't be retried. For example, retrying invalid arguments error doesn't make sense in some scenarios.\n\n# How does workflow run\nYou may wonder how it works. Behind the scene, workflow decision is driving the whole workflow running. It's the internal entities for client and server to run your workflows. If interesting to you, read this stack Overflow QA.",normalizedContent:"# fault-oblivious stateful workflow code\n# overview\ncadence core abstraction is a fault-oblivious stateful . the state of the code, including local variables and threads it creates, is immune to process and cadence service failures. this is a very powerful concept as it encapsulates state, processing threads, durable timers and handlers.\n\n# example\nlet's look at a use case. a customer signs up for an application with a trial period. after the period, if the customer has not cancelled, he should be charged once a month for the renewal. the customer has to be notified by email about the charges and should be able to cancel the subscription at any time.\n\nthe business logic of this use case is not very complicated and can be expressed in a few dozen lines of code. but any practical implementation has to ensure that the business process is fault tolerant and scalable. there are various ways to approach the design of such a system.\n\none approach is to center it around a database. an application process would periodically scan database tables for customers in specific states, execute necessary actions, and update the state to reflect that. while feasible, this approach has various drawbacks. the most obvious is that the state machine of the customer state quickly becomes extremely complicated. for example, charging a credit card or sending emails can fail due to a downstream system unavailability. the failed calls might need to be retried for a long time, ideally using an exponential retry policy. these calls should be throttled to not overload external systems. there should be support for poison pills to avoid blocking the whole process if a single customer record cannot be processed for whatever reason. the database-based approach also usually has performance problems. databases are not efficient for scenarios that require constant polling for records in a specific state.\n\nanother commonly employed approach is to use a timer service and queues. any update is pushed to a queue and then a that consumes from it updates a database and possibly pushes more messages in downstream queues. for operations that require scheduling, an external timer service can be used. this approach usually scales much better because a database is not constantly polled for changes. but it makes the programming model more complex and error prone as usually there is no transactional update between a queuing system and a database.\n\nwith cadence, the entire logic can be encapsulated in a simple durable function that directly implements the business logic. because the function is stateful, the implementer doesn't need to employ any additional systems to ensure durability and fault tolerance.\n\nhere is an example that implements the subscription management use case. it is in java, but go is also supported. the python and .net libraries are under active development.\n\npublic interface subscriptionworkflow {\n    @workflowmethod\n    void execute(string customerid);\n}\n\npublic class subscriptionworkflowimpl implements subscriptionworkflow {\n\n    private final subscriptionactivities activities =\n        workflow.newactivitystub(subscriptionactivities.class);\n\n    @override\n    public void execute(string customerid) {\n        activities.sendwelcomeemail(customerid);\n        try {\n            boolean trialperiod = true;\n            while (true) {\n                workflow.sleep(duration.ofdays(30));\n                activities.chargemonthlyfee(customerid);\n                if (trialperiod) {\n                    activities.sendendoftrialemail(customerid);\n                    trialperiod = false;\n                } else {\n                    activities.sendmonthlychargeemail(customerid);\n                }\n            }\n        } catch (cancellationexception e) {\n            activities.processsubscriptioncancellation(customerid);\n            activities.sendsorrytoseeyougoemail(customerid);\n        }\n    }\n}\n\n\nagain, note that this code directly implements the business logic. if any of the invoked operations (aka ) takes a long time, the code is not going to change. it is okay to block on chargemonthlyfee for a day if the downstream processing service is down that long. the same way that blocking sleep for 30 days is a normal operation inside the code.\n\ncadence has practically no scalability limits on the number of open instances. so even if your site has hundreds of millions of consumers, the above code is not going to change.\n\nthe commonly asked question by developers that learn cadence is \"how do i handle process failure/restart in my \"? the answer is that you do not. the code is completely oblivious to any failures and downtime of or even the cadence service itself. as soon as they are recovered and the needs to handle some , like timer or an completion, the current state of the is fully restored and the execution is continued. the only reason for a failure is the business code throwing an exception, not underlying infrastructure outages.\n\nanother commonly asked question is whether a can handle more instances than its cache size or number of threads it can support. the answer is that a , when in a blocked state, can be safely removed from a . later it can be resurrected on a different or the same when the need (in the form of an external ) arises. so a single can handle millions of open , assuming it can handle the update rate.\n\n# state recovery and determinism\nthe state recovery utilizes sourcing which puts a few restrictions on how the code is written. the main restriction is that the code must be deterministic which means that it must produce exactly the same result if executed multiple times. this rules out any external api calls from the code as external calls can fail intermittently or change its output any time. that is why all communication with the external world should happen through . for the same reason, code must use cadence apis to get current time, sleep, and create new threads.\n\nto understand the cadence execution model as well as the recovery mechanism, watch the following webcast. the animation covering recovery starts at 15:50.\n\n# id uniqueness\n is assigned by a client when starting a . it is usually a business level id like customer id or order id.\n\ncadence guarantees that there could be only one (across all types) with a given id open per at any time. an attempt to start a with the same id is going to fail with workflowexecutionalreadystarted error.\n\nan attempt to start a if there is a completed with the same id depends on a workflowidreusepolicy option:\n\n * allowduplicatefailedonly means that it is allowed to start a only if a previously executed with the same id failed.\n * allowduplicate means that it is allowed to start independently of the previous completion status.\n * rejectduplicate means that it is not allowed to start a using the same at all.\n * terminateifrunning means terminating the current running workflow if one exists, and start a new one.\n\nthe default is allowduplicatefailedonly.\n\nto distinguish multiple runs of a with the same , cadence identifies a with two ids: workflow id and run id. run id is a service-assigned uuid. to be precise, any is uniquely identified by a triple: domain name, workflow id and run id.\n\n# child workflow\na can execute other as child :workflow:workflows:. a child completion or failure is reported to its parent.\n\nsome reasons to use child are:\n\n * a child can be hosted by a separate set of which don't contain the parent code. so it would act as a separate service that can be invoked from multiple other .\n * a single has a limited size. for example, it cannot execute 100k . child can be used to partition the problem into smaller chunks. one parent with 1000 children each executing 1000 is 1 million executed .\n * a child can be used to manage some resource using its id to guarantee uniqueness. for example, a that manages host upgrades can have a child per host (host name being a ) and use them to ensure that all operations on the host are serialized.\n * a child can be used to execute some periodic logic without blowing up the parent history size. when a parent starts a child, it executes periodic logic calling that continues as many times as needed, then completes. from the parent point if view, it is just a single child invocation.\n\nthe main limitation of a child versus collocating all the application logic in a single is lack of the shared state. parent and child can communicate only through asynchronous . but if there is a tight coupling between them, it might be simpler to use a single and just rely on a shared object state.\n\nwe recommended starting from a single implementation if your problem has bounded size in terms of number of executed and processed . it is more straightforward than multiple asynchronously communicating .\n\n# workflow retries\n code is unaffected by infrastructure level downtime and failures. but it still can fail due to business logic level failures. for example, an can fail due to exceeding the retry interval and the error is not handled by application code, or the code having a bug.\n\nsome require a guarantee that they keep running even in presence of such failures. to support such use cases, an optional exponential retry policy can be specified when starting a . when it is specified, a failure restarts a from the beginning after the calculated retry interval. following are the retry policy parameters:\n\n * initialinterval is a delay before the first retry.\n * backoffcoefficient. retry policies are exponential. the coefficient specifies how fast the retry interval is growing. the coefficient of 1 means that the retry interval is always equal to the initialinterval.\n * maximuminterval specifies the maximum interval between retries. useful for coefficients of more than 1.\n * maximumattempts specifies how many times to attempt to execute a in the presence of failures. if this limit is exceeded, the fails without retry. not required if expirationinterval is specified.\n * expirationinterval specifies for how long to attempt executing a in the presence of failures. if this interval is exceeded, the fails without retry. not required if maximumattempts is specified.\n * nonretryableerrorreasons allows to specify errors that shouldn't be retried. for example, retrying invalid arguments error doesn't make sense in some scenarios.\n\n# how does workflow run\nyou may wonder how it works. behind the scene, workflow decision is driving the whole workflow running. it's the internal entities for client and server to run your workflows. if interesting to you, read this stack overflow qa.",charsets:{}},{title:"Activities",frontmatter:{layout:"default",title:"Activities",permalink:"/docs/concepts/activities",readingShow:"top"},regularPath:"/docs/03-concepts/02-activities.html",relativePath:"docs/03-concepts/02-activities.md",key:"v-006b104c",path:"/docs/concepts/activities/",headers:[{level:2,title:"Timeouts",slug:"timeouts",normalizedTitle:"timeouts",charIndex:853},{level:2,title:"Retries",slug:"retries",normalizedTitle:"retries",charIndex:1832},{level:2,title:"Long Running Activities",slug:"long-running-activities",normalizedTitle:"long running activities",charIndex:1599},{level:2,title:"Cancellation",slug:"cancellation",normalizedTitle:"cancellation",charIndex:4819},{level:2,title:"Activity Task Routing through Task Lists",slug:"activity-task-routing-through-task-lists",normalizedTitle:"activity task routing through task lists",charIndex:5426},{level:2,title:"Asynchronous Activity Completion",slug:"asynchronous-activity-completion",normalizedTitle:"asynchronous activity completion",charIndex:7231},{level:2,title:"Local Activities",slug:"local-activities",normalizedTitle:"local activities",charIndex:7849}],headersStr:"Timeouts Retries Long Running Activities Cancellation Activity Task Routing through Task Lists Asynchronous Activity Completion Local Activities",content:"# Activities\nFault-oblivious stateful code is the core abstraction of Cadence. But, due to deterministic execution requirements, they are not allowed to call any external API directly. Instead they orchestrate execution of . In its simplest form, a Cadence is a function or an object method in one of the supported languages. Cadence does not recover state in case of failures. Therefore an function is allowed to contain any code without restrictions.\n\n are invoked asynchronously through . A is essentially a queue used to store an until it is picked up by an available . The processes an by invoking its implementation function. When the function returns, the reports the result back to the Cadence service which in turn notifies the about completion. It is possible to implement an fully asynchronously by completing it from a different process.\n\n# Timeouts\nCadence does not impose any system limit on duration. It is up to the application to choose the timeouts for its execution. These are the configurable timeouts:\n\n * ScheduleToStart is the maximum time from a requesting execution to a starting its execution. The usual reason for this timeout to fire is all being down or not being able to keep up with the request rate. We recommend setting this timeout to the maximum time a is willing to wait for an execution in the presence of all possible outages.\n * StartToClose is the maximum time an can execute after it was picked by a .\n * ScheduleToClose is the maximum time from the requesting an execution to its completion.\n * Heartbeat is the maximum time between heartbeat requests. See Long Running Activities.\n\nEither ScheduleToClose or both ScheduleToStart and StartToClose timeouts are required.\n\nTimeouts are the key to manage activities. For more tips of how to set proper timeout, read this stack Overflow QA.\n\n# Retries\nAs Cadence doesn't recover an 's state and they can communicate to any external system, failures are expected. Therefore, Cadence supports automatic retries. Any when invoked can have an associated retry policy. Here are the retry policy parameters:\n\n * InitialInterval is a delay before the first retry.\n * BackoffCoefficient. Retry policies are exponential. The coefficient specifies how fast the retry interval is growing. The coefficient of 1 means that the retry interval is always equal to the InitialInterval.\n * MaximumInterval specifies the maximum interval between retries. Useful for coefficients more than 1.\n * MaximumAttempts specifies how many times to attempt to execute an in the presence of failures. If this limit is exceeded, the error is returned back to the that invoked the . Not required if ExpirationInterval is specified.\n * ExpirationInterval specifies for how long to attempt executing an in the presence of failures. If this interval is exceeded, the error is returned back to the that invoked the . Not required if MaximumAttempts is specified.\n * NonRetryableErrorReasons allows you to specify errors that shouldn't be retried. For example retrying invalid arguments error doesn't make sense in some scenarios.\n\nThere are scenarios when not a single but rather the whole part of a should be retried on failure. For example, a media encoding that downloads a file to a host, processes it, and then uploads the result back to storage. In this , if the host that hosts the dies, all three should be retried on a different host. Such retries should be handled by the code as they are very use case specific.\n\n# Long Running Activities\nFor long running , we recommended that you specify a relatively short heartbeat timeout and constantly heartbeat. This way failures for even very long running can be handled in a timely manner. An that specifies the heartbeat timeout is expected to call the heartbeat method periodically from its implementation.\n\nA heartbeat request can include application specific payload. This is useful to save execution progress. If an times out due to a missed heartbeat, the next attempt to execute it can access that progress and continue its execution from that point.\n\nLong running can be used as a special case of leader election. Cadence timeouts use second resolution. So it is not a solution for realtime applications. But if it is okay to react to the process failure within a few seconds, then a Cadence heartbeat is a good fit.\n\nOne common use case for such leader election is monitoring. An executes an internal loop that periodically polls some API and checks for some condition. It also heartbeats on every iteration. If the condition is satisfied, the completes which lets its to handle it. If the dies, the times out after the heartbeat interval is exceeded and is retried on a different . The same pattern works for polling for new files in Amazon S3 buckets or responses in REST or other synchronous APIs.\n\n# Cancellation\nA can request an cancellation. Currently the only way for an to learn that it was cancelled is through heart beating. The heartbeat request fails with a special error indicating that the was cancelled. Then it is up to the implementation to perform all the necessary cleanup and report that it is done with it. It is up to the implementation to decide if it wants to wait for the cancellation confirmation or just proceed without waiting.\n\nAnother common case for heartbeat failure is that the that invoked it is in a completed state. In this case an is expected to perform cleanup as well.\n\n# Activity Task Routing through Task Lists\n are dispatched to through . are queues that listen on. are highly dynamic and lightweight. They don't need to be explicitly registered. And it is okay to have one per process. It is normal to have more than one type to be invoked through a single . And it is normal in some cases (like host routing) to invoke the same type on multiple .\n\nHere are some use cases for employing multiple in a single workflow:\n\n * Flow control. A that consumes from a asks for an only when it has available capacity. So are never overloaded by request spikes. If executions are requested faster than can process them, they are backlogged in the .\n * Throttling. Each can specify the maximum rate it is allowed to processes on a . It does not exceed this limit even if it has spare capacity. There is also support for global rate limiting. This limit works across all for the given . It is frequently used to limit load on a downstream service that an calls into.\n * Deploying a set of independently. Think about a service that hosts and can be deployed independently from other and . To send to this service, a separate is needed.\n *  with different capabilities. For example, on GPU boxes vs non GPU boxes. Having two separate in this case allows to pick which one to send an execution request to.\n * Routing to a specific host. For example, in the media encoding case the transform and upload have to run on the same host as the download one.\n * Routing to a specific process. For example, some load large data sets and caches it in the process. The that rely on this data set should be routed to the same process.\n * Multiple priorities. One per priority and having a pool per priority.\n * Versioning. A new backwards incompatible implementation of an might use a different .\n\n# Asynchronous Activity Completion\nBy default an is a function or a method depending on a client side library language. As soon as the function returns, an completes. But in some cases an implementation is asynchronous. For example it is forwarded to an external system through a message queue. And the reply comes through a different queue.\n\nTo support such use cases, Cadence allows implementations that do not complete upon function completions. A separate API should be used in this case to complete the . This API can be called from any process, even in a different programming language, that the original used.\n\n# Local Activities\nSome of the are very short lived and do not need the queing semantic, flow control, rate limiting and routing capabilities. For these Cadence supports so called feature. are executed in the same process as the that invoked them. Consider using for functions that are:\n\n * no longer than a few seconds\n * do not require global rate limiting\n * do not require routing to specific or pools of \n * can be implemented in the same binary as the that invokes them\n\nThe main benefit of is that they are much more efficient in utilizing Cadence service resources and have much lower latency overhead comparing to the usual invocation.",normalizedContent:"# activities\nfault-oblivious stateful code is the core abstraction of cadence. but, due to deterministic execution requirements, they are not allowed to call any external api directly. instead they orchestrate execution of . in its simplest form, a cadence is a function or an object method in one of the supported languages. cadence does not recover state in case of failures. therefore an function is allowed to contain any code without restrictions.\n\n are invoked asynchronously through . a is essentially a queue used to store an until it is picked up by an available . the processes an by invoking its implementation function. when the function returns, the reports the result back to the cadence service which in turn notifies the about completion. it is possible to implement an fully asynchronously by completing it from a different process.\n\n# timeouts\ncadence does not impose any system limit on duration. it is up to the application to choose the timeouts for its execution. these are the configurable timeouts:\n\n * scheduletostart is the maximum time from a requesting execution to a starting its execution. the usual reason for this timeout to fire is all being down or not being able to keep up with the request rate. we recommend setting this timeout to the maximum time a is willing to wait for an execution in the presence of all possible outages.\n * starttoclose is the maximum time an can execute after it was picked by a .\n * scheduletoclose is the maximum time from the requesting an execution to its completion.\n * heartbeat is the maximum time between heartbeat requests. see long running activities.\n\neither scheduletoclose or both scheduletostart and starttoclose timeouts are required.\n\ntimeouts are the key to manage activities. for more tips of how to set proper timeout, read this stack overflow qa.\n\n# retries\nas cadence doesn't recover an 's state and they can communicate to any external system, failures are expected. therefore, cadence supports automatic retries. any when invoked can have an associated retry policy. here are the retry policy parameters:\n\n * initialinterval is a delay before the first retry.\n * backoffcoefficient. retry policies are exponential. the coefficient specifies how fast the retry interval is growing. the coefficient of 1 means that the retry interval is always equal to the initialinterval.\n * maximuminterval specifies the maximum interval between retries. useful for coefficients more than 1.\n * maximumattempts specifies how many times to attempt to execute an in the presence of failures. if this limit is exceeded, the error is returned back to the that invoked the . not required if expirationinterval is specified.\n * expirationinterval specifies for how long to attempt executing an in the presence of failures. if this interval is exceeded, the error is returned back to the that invoked the . not required if maximumattempts is specified.\n * nonretryableerrorreasons allows you to specify errors that shouldn't be retried. for example retrying invalid arguments error doesn't make sense in some scenarios.\n\nthere are scenarios when not a single but rather the whole part of a should be retried on failure. for example, a media encoding that downloads a file to a host, processes it, and then uploads the result back to storage. in this , if the host that hosts the dies, all three should be retried on a different host. such retries should be handled by the code as they are very use case specific.\n\n# long running activities\nfor long running , we recommended that you specify a relatively short heartbeat timeout and constantly heartbeat. this way failures for even very long running can be handled in a timely manner. an that specifies the heartbeat timeout is expected to call the heartbeat method periodically from its implementation.\n\na heartbeat request can include application specific payload. this is useful to save execution progress. if an times out due to a missed heartbeat, the next attempt to execute it can access that progress and continue its execution from that point.\n\nlong running can be used as a special case of leader election. cadence timeouts use second resolution. so it is not a solution for realtime applications. but if it is okay to react to the process failure within a few seconds, then a cadence heartbeat is a good fit.\n\none common use case for such leader election is monitoring. an executes an internal loop that periodically polls some api and checks for some condition. it also heartbeats on every iteration. if the condition is satisfied, the completes which lets its to handle it. if the dies, the times out after the heartbeat interval is exceeded and is retried on a different . the same pattern works for polling for new files in amazon s3 buckets or responses in rest or other synchronous apis.\n\n# cancellation\na can request an cancellation. currently the only way for an to learn that it was cancelled is through heart beating. the heartbeat request fails with a special error indicating that the was cancelled. then it is up to the implementation to perform all the necessary cleanup and report that it is done with it. it is up to the implementation to decide if it wants to wait for the cancellation confirmation or just proceed without waiting.\n\nanother common case for heartbeat failure is that the that invoked it is in a completed state. in this case an is expected to perform cleanup as well.\n\n# activity task routing through task lists\n are dispatched to through . are queues that listen on. are highly dynamic and lightweight. they don't need to be explicitly registered. and it is okay to have one per process. it is normal to have more than one type to be invoked through a single . and it is normal in some cases (like host routing) to invoke the same type on multiple .\n\nhere are some use cases for employing multiple in a single workflow:\n\n * flow control. a that consumes from a asks for an only when it has available capacity. so are never overloaded by request spikes. if executions are requested faster than can process them, they are backlogged in the .\n * throttling. each can specify the maximum rate it is allowed to processes on a . it does not exceed this limit even if it has spare capacity. there is also support for global rate limiting. this limit works across all for the given . it is frequently used to limit load on a downstream service that an calls into.\n * deploying a set of independently. think about a service that hosts and can be deployed independently from other and . to send to this service, a separate is needed.\n *  with different capabilities. for example, on gpu boxes vs non gpu boxes. having two separate in this case allows to pick which one to send an execution request to.\n * routing to a specific host. for example, in the media encoding case the transform and upload have to run on the same host as the download one.\n * routing to a specific process. for example, some load large data sets and caches it in the process. the that rely on this data set should be routed to the same process.\n * multiple priorities. one per priority and having a pool per priority.\n * versioning. a new backwards incompatible implementation of an might use a different .\n\n# asynchronous activity completion\nby default an is a function or a method depending on a client side library language. as soon as the function returns, an completes. but in some cases an implementation is asynchronous. for example it is forwarded to an external system through a message queue. and the reply comes through a different queue.\n\nto support such use cases, cadence allows implementations that do not complete upon function completions. a separate api should be used in this case to complete the . this api can be called from any process, even in a different programming language, that the original used.\n\n# local activities\nsome of the are very short lived and do not need the queing semantic, flow control, rate limiting and routing capabilities. for these cadence supports so called feature. are executed in the same process as the that invoked them. consider using for functions that are:\n\n * no longer than a few seconds\n * do not require global rate limiting\n * do not require routing to specific or pools of \n * can be implemented in the same binary as the that invokes them\n\nthe main benefit of is that they are much more efficient in utilizing cadence service resources and have much lower latency overhead comparing to the usual invocation.",charsets:{}},{title:"Event handling",frontmatter:{layout:"default",title:"Event handling",permalink:"/docs/concepts/events",readingShow:"top"},regularPath:"/docs/03-concepts/03-events.html",relativePath:"docs/03-concepts/03-events.md",key:"v-4b893278",path:"/docs/concepts/events/",headers:[{level:2,title:"Event Aggregation and Correlation",slug:"event-aggregation-and-correlation",normalizedTitle:"event aggregation and correlation",charIndex:246},{level:2,title:"Human Tasks",slug:"human-tasks",normalizedTitle:"human tasks",charIndex:1864},{level:2,title:"Process Execution Alteration",slug:"process-execution-alteration",normalizedTitle:"process execution alteration",charIndex:2444},{level:2,title:"Synchronization",slug:"synchronization",normalizedTitle:"synchronization",charIndex:2961}],headersStr:"Event Aggregation and Correlation Human Tasks Process Execution Alteration Synchronization",content:"# Event handling\nFault-oblivious stateful can be about an external . A is always point to point destined to a specific instance. are always processed in the order in which they are received.\n\nThere are multiple scenarios for which are useful.\n\n# Event Aggregation and Correlation\nCadence is not a replacement for generic stream processing engines like Apache Flink or Apache Spark. But in certain scenarios it is a better fit. For example, when all that should be aggregated and correlated are always applied to to some business entity with a clear ID. And then when a certain condition is met, actions should be executed.\n\nThe main limitation is that a single Cadence has a pretty limited throughput, while the number of is practically unlimited. So if you need to aggregate per customer, and your application has 100 million customers and each customer doesn't generate more than 20 per second, then Cadence would work fine. But if you want to aggregate all for US customers then the rate of these would be beyond the single capacity.\n\nFor example, an IoT device generates and a certain sequence of indicates that the device should be reprovisioned. A instance per device would be created and each instance would manage the state machine of the device and execute reprovision when necessary.\n\nAnother use case is a customer loyalty program. Every time a customer makes a purchase, an is generated into Apache Kafka for downstream systems to process. A loyalty service Kafka consumer receives the and a customer about the purchase using the Cadence signalWorkflowExecution API. The accumulates the count of the purchases. If a specified threshold is achieved, the executes an that notifies some external service that the customer has reached the next level of loyalty program. The also executes to periodically message the customer about their current status.\n\n# Human Tasks\nA lot of business processes involve human participants. The standard Cadence pattern for implementing an external interaction is to execute an that creates a human in an external system. It can be an email with a form, or a record in some external database, or a mobile app notification. When a user changes the status of the , a is sent to the corresponding . For example, when the form is submitted, or a mobile app notification is acknowledged. Some have multiple possible actions like claim, return, complete, reject. So multiple can be sent in relation to it.\n\n# Process Execution Alteration\nSome business processes should change their behavior if some external has happened. For example, while executing an order shipment , any change in item quantity could be delivered in a form of a .\n\nAnother example is a service deployment . While rolling out new software version to a Kubernetes cluster some problem was identified. A can be used to ask the to pause while the problem is investigated. Then either a continue or a rollback can be used to execute the appropriate action.\n\n# Synchronization\nCadence are strongly consistent so they can be used as a synchronization point for executing actions. For example, there is a requirement that all messages for a single user are processed sequentially but the underlying messaging infrastructure can deliver them in parallel. The Cadence solution would be to have a per user and it when an is received. Then the would buffer all in an internal data structure and then call an for every received. See the following Stack Overflow answer for an example.",normalizedContent:"# event handling\nfault-oblivious stateful can be about an external . a is always point to point destined to a specific instance. are always processed in the order in which they are received.\n\nthere are multiple scenarios for which are useful.\n\n# event aggregation and correlation\ncadence is not a replacement for generic stream processing engines like apache flink or apache spark. but in certain scenarios it is a better fit. for example, when all that should be aggregated and correlated are always applied to to some business entity with a clear id. and then when a certain condition is met, actions should be executed.\n\nthe main limitation is that a single cadence has a pretty limited throughput, while the number of is practically unlimited. so if you need to aggregate per customer, and your application has 100 million customers and each customer doesn't generate more than 20 per second, then cadence would work fine. but if you want to aggregate all for us customers then the rate of these would be beyond the single capacity.\n\nfor example, an iot device generates and a certain sequence of indicates that the device should be reprovisioned. a instance per device would be created and each instance would manage the state machine of the device and execute reprovision when necessary.\n\nanother use case is a customer loyalty program. every time a customer makes a purchase, an is generated into apache kafka for downstream systems to process. a loyalty service kafka consumer receives the and a customer about the purchase using the cadence signalworkflowexecution api. the accumulates the count of the purchases. if a specified threshold is achieved, the executes an that notifies some external service that the customer has reached the next level of loyalty program. the also executes to periodically message the customer about their current status.\n\n# human tasks\na lot of business processes involve human participants. the standard cadence pattern for implementing an external interaction is to execute an that creates a human in an external system. it can be an email with a form, or a record in some external database, or a mobile app notification. when a user changes the status of the , a is sent to the corresponding . for example, when the form is submitted, or a mobile app notification is acknowledged. some have multiple possible actions like claim, return, complete, reject. so multiple can be sent in relation to it.\n\n# process execution alteration\nsome business processes should change their behavior if some external has happened. for example, while executing an order shipment , any change in item quantity could be delivered in a form of a .\n\nanother example is a service deployment . while rolling out new software version to a kubernetes cluster some problem was identified. a can be used to ask the to pause while the problem is investigated. then either a continue or a rollback can be used to execute the appropriate action.\n\n# synchronization\ncadence are strongly consistent so they can be used as a synchronization point for executing actions. for example, there is a requirement that all messages for a single user are processed sequentially but the underlying messaging infrastructure can deliver them in parallel. the cadence solution would be to have a per user and it when an is received. then the would buffer all in an internal data structure and then call an for every received. see the following stack overflow answer for an example.",charsets:{}},{title:"Synchronous query",frontmatter:{layout:"default",title:"Synchronous query",permalink:"/docs/concepts/queries",readingShow:"top"},regularPath:"/docs/03-concepts/04-queries.html",relativePath:"docs/03-concepts/04-queries.md",key:"v-7d1ab062",path:"/docs/concepts/queries/",headers:[{level:2,title:"Stack Trace Query",slug:"stack-trace-query",normalizedTitle:"stack trace query",charIndex:1119}],headersStr:"Stack Trace Query",content:"# Synchronous query\n code is stateful with the Cadence framework preserving it over various software and hardware failures. The state is constantly mutated during . To expose this internal state to the external world Cadence provides a synchronous feature. From the implementer point of view the is exposed as a synchronous callback that is invoked by external entities. Multiple such callbacks can be provided per type exposing different information to different external systems.\n\nTo execute a an external client calls a synchronous Cadence API providing , workflowID, name and optional arguments.\n\n callbacks must be read-only not mutating the state in any way. The other limitation is that the callback cannot contain any blocking code. Both above limitations rule out ability to invoke from the handlers.\n\nCadence team is currently working on implementing update feature that would be similar to in the way it is invoked, but would support state mutation and invocations. From user's point of view, update is similar to signal + strong consistent query, but implemented in a much less expensive way in Cadence.\n\n# Stack Trace Query\nThe Cadence client libraries expose some predefined out of the box. Currently the only supported built-in is stack_trace. This returns stacks of all owned threads. This is a great way to troubleshoot any in production.",normalizedContent:"# synchronous query\n code is stateful with the cadence framework preserving it over various software and hardware failures. the state is constantly mutated during . to expose this internal state to the external world cadence provides a synchronous feature. from the implementer point of view the is exposed as a synchronous callback that is invoked by external entities. multiple such callbacks can be provided per type exposing different information to different external systems.\n\nto execute a an external client calls a synchronous cadence api providing , workflowid, name and optional arguments.\n\n callbacks must be read-only not mutating the state in any way. the other limitation is that the callback cannot contain any blocking code. both above limitations rule out ability to invoke from the handlers.\n\ncadence team is currently working on implementing update feature that would be similar to in the way it is invoked, but would support state mutation and invocations. from user's point of view, update is similar to signal + strong consistent query, but implemented in a much less expensive way in cadence.\n\n# stack trace query\nthe cadence client libraries expose some predefined out of the box. currently the only supported built-in is stack_trace. this returns stacks of all owned threads. this is a great way to troubleshoot any in production.",charsets:{}},{title:"Deployment topology",frontmatter:{layout:"default",title:"Deployment topology",permalink:"/docs/concepts/topology",readingShow:"top"},regularPath:"/docs/03-concepts/05-topology.html",relativePath:"docs/03-concepts/05-topology.md",key:"v-20231a48",path:"/docs/concepts/topology/",headers:[{level:2,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:24},{level:2,title:"Cadence Service",slug:"cadence-service",normalizedTitle:"cadence service",charIndex:459},{level:2,title:"Workflow Worker",slug:"workflow-worker",normalizedTitle:"workflow worker",charIndex:1413},{level:2,title:"Activity Worker",slug:"activity-worker",normalizedTitle:"activity worker",charIndex:2482},{level:2,title:"External Clients",slug:"external-clients",normalizedTitle:"external clients",charIndex:3174}],headersStr:"Overview Cadence Service Workflow Worker Activity Worker External Clients",content:"# Deployment topology\n# Overview\nCadence is a highly scalable fault-oblivious stateful code platform. The fault-oblivious code is a next level of abstraction over commonly used techniques to achieve fault tolerance and durability.\n\nA common Cadence-based application consists of a Cadence service, and , and external clients. Note that both types of as well as external clients are roles and can be collocated in a single application process if necessary.\n\n# Cadence Service\n\n\nAt the core of Cadence is a highly scalable multitentant service. The service exposes all its functionality through a strongly typed Thrift API.\n\nInternally it depends on a persistent store. Currently, Apache Cassandra and MySQL stores are supported out of the box. For listing using complex predicates, Elasticsearch cluster can be used.\n\nCadence service is responsible for keeping state and associated durable timers. It maintains internal queues (called ) which are used to dispatch to external .\n\nCadence service is multitentant. Therefore it is expected that multiple pools of implementing different use cases connect to the same service instance. For example, at Uber a single service is used by more than a hundred applications. At the same time some external customers deploy an instance of Cadence service per application. For local development, a local Cadence service instance configured through docker-compose is used.\n\n\n\n# Workflow Worker\nCadence reuses terminology from workflow automation . So fault-oblivious stateful code is called .\n\nThe Cadence service does not execute code directly. The code is hosted by an external (from the service point of view) process. These processes receive that contain that the is expected to handle from the Cadence service, delivers them to the code, and communicates back to the service.\n\nAs code is external to the service, it can be implemented in any language that can talk service Thrift API. Currently Java and Go clients are production ready. While Python and C# clients are under development. Let us know if you are interested in contributing a client in your preferred language.\n\nThe Cadence service API doesn't impose any specific definition language. So a specific can be implemented to execute practically any existing specification. The model the Cadence team chose to support out of the box is based on the idea of durable function. Durable functions are as close as possible to application business logic with minimal plumbing required.\n\n# Activity Worker\n fault-oblivious code is immune to infrastructure failures. But it has to communicate with the imperfect external world where failures are common. All communication to the external world is done through . are pieces of code that can perform any application-specific action like calling a service, updating a database record, or downloading a file from Amazon S3. Cadence are very feature-rich compared to queuing systems. Example features are routing to specific processes, infinite retries, heartbeats, and unlimited execution time.\n\n are hosted by processes that receive from the Cadence service, invoke correspondent implementations and report back completion statuses.\n\n# External Clients\n and host and code. But to create a instance (an execution in Cadence terminology) the StartWorkflowExecution Cadence service API call should be used. Usually, are started by outside entities like UIs, microservices or CLIs.\n\nThese entities can also:\n\n * notify about asynchronous external in the form of \n * synchronously state\n * synchronously wait for a completion\n * cancel, terminate, restart, and reset \n * search for specific using list API",normalizedContent:"# deployment topology\n# overview\ncadence is a highly scalable fault-oblivious stateful code platform. the fault-oblivious code is a next level of abstraction over commonly used techniques to achieve fault tolerance and durability.\n\na common cadence-based application consists of a cadence service, and , and external clients. note that both types of as well as external clients are roles and can be collocated in a single application process if necessary.\n\n# cadence service\n\n\nat the core of cadence is a highly scalable multitentant service. the service exposes all its functionality through a strongly typed thrift api.\n\ninternally it depends on a persistent store. currently, apache cassandra and mysql stores are supported out of the box. for listing using complex predicates, elasticsearch cluster can be used.\n\ncadence service is responsible for keeping state and associated durable timers. it maintains internal queues (called ) which are used to dispatch to external .\n\ncadence service is multitentant. therefore it is expected that multiple pools of implementing different use cases connect to the same service instance. for example, at uber a single service is used by more than a hundred applications. at the same time some external customers deploy an instance of cadence service per application. for local development, a local cadence service instance configured through docker-compose is used.\n\n\n\n# workflow worker\ncadence reuses terminology from workflow automation . so fault-oblivious stateful code is called .\n\nthe cadence service does not execute code directly. the code is hosted by an external (from the service point of view) process. these processes receive that contain that the is expected to handle from the cadence service, delivers them to the code, and communicates back to the service.\n\nas code is external to the service, it can be implemented in any language that can talk service thrift api. currently java and go clients are production ready. while python and c# clients are under development. let us know if you are interested in contributing a client in your preferred language.\n\nthe cadence service api doesn't impose any specific definition language. so a specific can be implemented to execute practically any existing specification. the model the cadence team chose to support out of the box is based on the idea of durable function. durable functions are as close as possible to application business logic with minimal plumbing required.\n\n# activity worker\n fault-oblivious code is immune to infrastructure failures. but it has to communicate with the imperfect external world where failures are common. all communication to the external world is done through . are pieces of code that can perform any application-specific action like calling a service, updating a database record, or downloading a file from amazon s3. cadence are very feature-rich compared to queuing systems. example features are routing to specific processes, infinite retries, heartbeats, and unlimited execution time.\n\n are hosted by processes that receive from the cadence service, invoke correspondent implementations and report back completion statuses.\n\n# external clients\n and host and code. but to create a instance (an execution in cadence terminology) the startworkflowexecution cadence service api call should be used. usually, are started by outside entities like uis, microservices or clis.\n\nthese entities can also:\n\n * notify about asynchronous external in the form of \n * synchronously state\n * synchronously wait for a completion\n * cancel, terminate, restart, and reset \n * search for specific using list api",charsets:{}},{title:"Task lists",frontmatter:{layout:"default",title:"Task lists",permalink:"/docs/concepts/task-lists",readingShow:"top"},regularPath:"/docs/03-concepts/06-task-lists.html",relativePath:"docs/03-concepts/06-task-lists.md",key:"v-2cd6f7bc",path:"/docs/concepts/task-lists/",headersStr:null,content:"# Task lists\nWhen a invokes an , it sends the ScheduleActivityTask to the Cadence service. As a result, the service updates the state and dispatches an to a that implements the . Instead of calling the directly, an intermediate queue is used. So the service adds an to this queue and a receives the using a long poll request. Cadence calls this queue used to dispatch an .\n\nSimilarly, when a needs to handle an external , a is created. A is used to deliver it to the (also called decider).\n\nWhile Cadence are queues, they have some differences from commonly used queuing technologies. The main one is that they do not require explicit registration and are created on demand. The number of is not limited. A common use case is to have a per process and use it to deliver to the process. Another use case is to have a per pool of .\n\nThere are multiple advantages of using a to deliver instead of invoking an through a synchronous RPC:\n\n *  doesn't need to have any open ports, which is more secure.\n *  doesn't need to advertise itself through DNS or any other network discovery mechanism.\n * When all are down, messages are persisted in a waiting for the to recover.\n * A polls for a message only when it has spare capacity, so it never gets overloaded.\n * Automatic load balancing across a large number of .\n *  support server side throttling. This allows you to limit the dispatch rate to the pool of and still supports adding a with a higher rate when spikes happen.\n *  can be used to route a request to specific pools of or even a specific process.",normalizedContent:"# task lists\nwhen a invokes an , it sends the scheduleactivitytask to the cadence service. as a result, the service updates the state and dispatches an to a that implements the . instead of calling the directly, an intermediate queue is used. so the service adds an to this queue and a receives the using a long poll request. cadence calls this queue used to dispatch an .\n\nsimilarly, when a needs to handle an external , a is created. a is used to deliver it to the (also called decider).\n\nwhile cadence are queues, they have some differences from commonly used queuing technologies. the main one is that they do not require explicit registration and are created on demand. the number of is not limited. a common use case is to have a per process and use it to deliver to the process. another use case is to have a per pool of .\n\nthere are multiple advantages of using a to deliver instead of invoking an through a synchronous rpc:\n\n *  doesn't need to have any open ports, which is more secure.\n *  doesn't need to advertise itself through dns or any other network discovery mechanism.\n * when all are down, messages are persisted in a waiting for the to recover.\n * a polls for a message only when it has spare capacity, so it never gets overloaded.\n * automatic load balancing across a large number of .\n *  support server side throttling. this allows you to limit the dispatch rate to the pool of and still supports adding a with a higher rate when spikes happen.\n *  can be used to route a request to specific pools of or even a specific process.",charsets:{}},{title:"Archival",frontmatter:{layout:"default",title:"Archival",permalink:"/docs/concepts/archival",readingShow:"top"},regularPath:"/docs/03-concepts/07-archival.html",relativePath:"docs/03-concepts/07-archival.md",key:"v-f2d516bc",path:"/docs/concepts/archival/",headers:[{level:2,title:"Concepts",slug:"concepts",normalizedTitle:"concepts",charIndex:1026},{level:2,title:"Configuring Archival",slug:"configuring-archival",normalizedTitle:"configuring archival",charIndex:1457},{level:3,title:"Cluster Archival Config",slug:"cluster-archival-config",normalizedTitle:"cluster archival config",charIndex:1543},{level:3,title:"Domain Archival Config",slug:"domain-archival-config",normalizedTitle:"domain archival config",charIndex:2172},{level:2,title:"Running Locally",slug:"running-locally",normalizedTitle:"running locally",charIndex:2611},{level:2,title:"Running in Production",slug:"running-in-production",normalizedTitle:"running in production",charIndex:3357},{level:2,title:"FAQ",slug:"faq",normalizedTitle:"faq",charIndex:3965},{level:3,title:"How does archival interact with global domains?",slug:"how-does-archival-interact-with-global-domains",normalizedTitle:"how does archival interact with global domains?",charIndex:3971},{level:3,title:"Can I specify multiple archival URIs?",slug:"can-i-specify-multiple-archival-uris",normalizedTitle:"can i specify multiple archival uris?",charIndex:4242},{level:3,title:"How does archival work with PII?",slug:"how-does-archival-work-with-pii",normalizedTitle:"how does archival work with pii?",charIndex:4426},{level:2,title:"Planned Future Work",slug:"planned-future-work",normalizedTitle:"planned future work",charIndex:4728}],headersStr:"Concepts Configuring Archival Cluster Archival Config Domain Archival Config Running Locally Running in Production FAQ How does archival interact with global domains? Can I specify multiple archival URIs? How does archival work with PII? Planned Future Work",content:'# Archival\n is a feature that automatically moves histories from persistence to another location after the retention period. The purpose of archival is to be able to keep histories as long as needed while not overwhelming the persistence store. There are two reasons you may want to keep the histories after the retention period has past:\n\n 1. Compliance: For legal reasons histories may need to be stored for a long period of time.\n 2. Debugging: Old histories can still be accessed for debugging.\n\n is still in beta and there are three limits to its feature set:\n\n 1. Only Histories: Only histories are archived, visibility records are simply deleted after the retention period.\n 2. RunID Required: In order to access an archived history, both workflowID and runID are required.\n 3. Best Effort: There are cases in which a history can be deleted from persistence without being archived first. These cases are rare but are possible with the current state of .\n\nWork is being prioritized on to eliminate these limitations.\n\n# Concepts\n * Archiver: Archiver is the component responsible for archiving and retrieving histories. Its interface is quite generic and supports different kinds of locations: local file system, S3, Kafka, etc. Check this README for how to add a new archiver implementation.\n * URI: An URI is used to specify the location. Based on the scheme part of an URI, the corresponding archiver will be selected by the system to perform .\n\n# Configuring Archival\n is controlled by both level config and cluster level config.\n\n# Cluster Archival Config\nA Cadence cluster can be in one of three states:\n\n * Disabled: No will occur and the archivers will be not initialized on startup.\n * Paused: This state is not yet implemented. Currently setting cluster to paused is the same as setting it to disabled.\n * Enabled: will occur.\n\nEnabling the cluster for simply means histories are being archived. There is another config which controls whether histories can be accessed from . Both these configs have defaults defined in static yaml, and have dynamic config overwrites. Note, however, dynamic config will take effect only when is enabled in static yaml.\n\n# Domain Archival Config\nA includes two pieces of related config:\n\n * Status: Either enabled or disabled. If a is in the disabled state no will occur for that . A can safely switch between statuses.\n * URI: The scheme and location where histories will be archived to. When a enables for the first time URI is set and can never be mutated. If URI is not specified when first enabling a for , a default URI from static config will be used.\n\n# Running Locally\nIn order to run locally do the following:\n\n 1. ./cadence-server start\n 2. ./cadence --do samples-domain domain register --gd false --history_archival_status enabled --retention 0\n 3. Run the helloworld cadence-sample by following the README\n 4. Copy the workflowID and runID of the completed from log output\n 5. ./cadence --do samples-domain wf show --wid <workflowID> --rid <runID>\n\nIn step 2, we registered a new and enabled history feature for that . Since we didn\'t provide an URI when registering the new , the default URI specified in config/development.yaml is used. The default URI is file:///tmp/cadence_archival/development, so you can find the archived history under the /tmp/cadence_archival/development directory.\n\n# Running in Production\nIn production, Google Cloud and S3 is supported for archival. Check documentation in GCloud archival component and S3 archival component.\n\nBelow is an example of S3 archival configuration:\n\narchival:\n  history:\n    status: "enabled"\n    enableRead: true\n    provider:\n      s3store:\n        region: "us-east-2"\n  visibility:\n    status: "enabled"\n    enableRead: true\n    provider:\n      s3store:\n        region: "us-east-2"\ndomainDefaults:\n  archival:\n    history:\n      status: "enabled"\n      URI: "s3://dev-cad"\n    visibility:\n      status: "enabled"\n      URI: "s3://dev-cad"\n\n\n# FAQ\n# How does archival interact with global domains?\nWhen occurs it will first run on the active side and some time later it will run on the standby side as well. Before uploading history a check is done to see if it has already been uploaded, if so it is not re-uploaded.\n\n# Can I specify multiple archival URIs?\nNo, each can only have one URI for history and one URI for visibility . Different , however, can have different URIs (with different schemes).\n\n# How does archival work with PII?\nNo cadence should ever operate on clear text PII. Cadence can be thought of as a database and just as one would not store PII in a database PII should not be stored in Cadence. This is even more important when is enabled because these histories can be kept forever.\n\n# Planned Future Work\n * Support of visibility.\n * Support accessing histories without providing runID.\n * Provide hard guarantee that no history is deleted from persistence before being archived if is enabled.\n * Implement paused state. In this state no will occur but histories also will not be deleted from persistence. Once enabled again from paused state, all skipped will occur.',normalizedContent:'# archival\n is a feature that automatically moves histories from persistence to another location after the retention period. the purpose of archival is to be able to keep histories as long as needed while not overwhelming the persistence store. there are two reasons you may want to keep the histories after the retention period has past:\n\n 1. compliance: for legal reasons histories may need to be stored for a long period of time.\n 2. debugging: old histories can still be accessed for debugging.\n\n is still in beta and there are three limits to its feature set:\n\n 1. only histories: only histories are archived, visibility records are simply deleted after the retention period.\n 2. runid required: in order to access an archived history, both workflowid and runid are required.\n 3. best effort: there are cases in which a history can be deleted from persistence without being archived first. these cases are rare but are possible with the current state of .\n\nwork is being prioritized on to eliminate these limitations.\n\n# concepts\n * archiver: archiver is the component responsible for archiving and retrieving histories. its interface is quite generic and supports different kinds of locations: local file system, s3, kafka, etc. check this readme for how to add a new archiver implementation.\n * uri: an uri is used to specify the location. based on the scheme part of an uri, the corresponding archiver will be selected by the system to perform .\n\n# configuring archival\n is controlled by both level config and cluster level config.\n\n# cluster archival config\na cadence cluster can be in one of three states:\n\n * disabled: no will occur and the archivers will be not initialized on startup.\n * paused: this state is not yet implemented. currently setting cluster to paused is the same as setting it to disabled.\n * enabled: will occur.\n\nenabling the cluster for simply means histories are being archived. there is another config which controls whether histories can be accessed from . both these configs have defaults defined in static yaml, and have dynamic config overwrites. note, however, dynamic config will take effect only when is enabled in static yaml.\n\n# domain archival config\na includes two pieces of related config:\n\n * status: either enabled or disabled. if a is in the disabled state no will occur for that . a can safely switch between statuses.\n * uri: the scheme and location where histories will be archived to. when a enables for the first time uri is set and can never be mutated. if uri is not specified when first enabling a for , a default uri from static config will be used.\n\n# running locally\nin order to run locally do the following:\n\n 1. ./cadence-server start\n 2. ./cadence --do samples-domain domain register --gd false --history_archival_status enabled --retention 0\n 3. run the helloworld cadence-sample by following the readme\n 4. copy the workflowid and runid of the completed from log output\n 5. ./cadence --do samples-domain wf show --wid <workflowid> --rid <runid>\n\nin step 2, we registered a new and enabled history feature for that . since we didn\'t provide an uri when registering the new , the default uri specified in config/development.yaml is used. the default uri is file:///tmp/cadence_archival/development, so you can find the archived history under the /tmp/cadence_archival/development directory.\n\n# running in production\nin production, google cloud and s3 is supported for archival. check documentation in gcloud archival component and s3 archival component.\n\nbelow is an example of s3 archival configuration:\n\narchival:\n  history:\n    status: "enabled"\n    enableread: true\n    provider:\n      s3store:\n        region: "us-east-2"\n  visibility:\n    status: "enabled"\n    enableread: true\n    provider:\n      s3store:\n        region: "us-east-2"\ndomaindefaults:\n  archival:\n    history:\n      status: "enabled"\n      uri: "s3://dev-cad"\n    visibility:\n      status: "enabled"\n      uri: "s3://dev-cad"\n\n\n# faq\n# how does archival interact with global domains?\nwhen occurs it will first run on the active side and some time later it will run on the standby side as well. before uploading history a check is done to see if it has already been uploaded, if so it is not re-uploaded.\n\n# can i specify multiple archival uris?\nno, each can only have one uri for history and one uri for visibility . different , however, can have different uris (with different schemes).\n\n# how does archival work with pii?\nno cadence should ever operate on clear text pii. cadence can be thought of as a database and just as one would not store pii in a database pii should not be stored in cadence. this is even more important when is enabled because these histories can be kept forever.\n\n# planned future work\n * support of visibility.\n * support accessing histories without providing runid.\n * provide hard guarantee that no history is deleted from persistence before being archived if is enabled.\n * implement paused state. in this state no will occur but histories also will not be deleted from persistence. once enabled again from paused state, all skipped will occur.',charsets:{}},{title:"Cross DC replication",frontmatter:{layout:"default",title:"Cross DC replication",permalink:"/docs/concepts/cross-dc-replication",readingShow:"top"},regularPath:"/docs/03-concepts/08-cross-dc-replication.html",relativePath:"docs/03-concepts/08-cross-dc-replication.md",key:"v-071004ea",path:"/docs/concepts/cross-dc-replication/",headers:[{level:2,title:"Global Domains Architecture",slug:"global-domains-architecture",normalizedTitle:"global domains architecture",charIndex:298},{level:2,title:"New config for Global Domains",slug:"new-config-for-global-domains",normalizedTitle:"new config for global domains",charIndex:1334},{level:3,title:"IsGlobal",slug:"isglobal",normalizedTitle:"isglobal",charIndex:1366},{level:3,title:"Clusters",slug:"clusters",normalizedTitle:"clusters",charIndex:1630},{level:3,title:"Active Cluster Name",slug:"active-cluster-name",normalizedTitle:"active cluster name",charIndex:1916},{level:3,title:"Failover Version",slug:"failover-version",normalizedTitle:"failover version",charIndex:2070},{level:2,title:"Conflict Resolution",slug:"conflict-resolution",normalizedTitle:"conflict resolution",charIndex:2366},{level:2,title:"Visibility API",slug:"visibility-api",normalizedTitle:"visibility api",charIndex:3193},{level:2,title:"CLI",slug:"cli",normalizedTitle:"cli",charIndex:3640},{level:3,title:"Query Global Domain",slug:"query-global-domain",normalizedTitle:"query global domain",charIndex:3743},{level:3,title:"Failover Global Domain",slug:"failover-global-domain",normalizedTitle:"failover global domain",charIndex:4102},{level:2,title:"Running Locally",slug:"running-locally",normalizedTitle:"running locally",charIndex:4266},{level:2,title:"Running in Production",slug:"running-in-production",normalizedTitle:"running in production",charIndex:4385},{level:2,title:"FAQ",slug:"faq",normalizedTitle:"faq",charIndex:6822},{level:3,title:"What happens to outstanding activities after failover?",slug:"what-happens-to-outstanding-activities-after-failover",normalizedTitle:"what happens to outstanding activities after failover?",charIndex:6828},{level:3,title:"What happens when a start or signal API call is made to a standby cluster?",slug:"what-happens-when-a-start-or-signal-api-call-is-made-to-a-standby-cluster",normalizedTitle:"what happens when a start or signal api call is made to a standby cluster?",charIndex:7234}],headersStr:"Global Domains Architecture New config for Global Domains IsGlobal Clusters Active Cluster Name Failover Version Conflict Resolution Visibility API CLI Query Global Domain Failover Global Domain Running Locally Running in Production FAQ What happens to outstanding activities after failover? What happens when a start or signal API call is made to a standby cluster?",content:'# Cross-DC replication\nThe Cadence Global feature provides clients with the capability to continue their from another cluster in the event of a datacenter failover. Although you can configure a Global to be replicated to any number of clusters, it is only considered active in a single cluster.\n\n# Global Domains Architecture\nCadence has introduced a new top level entity, Global , which provides support for replication of execution across clusters. Client applications need to run polling on / on all clusters. Cadence will only dispatch tasks on the current active cluster; on the standby cluster will sit idle until the Global is failed over.\n\nBecause Cadence is a service that provides highly consistent semantics, we only allow external likeStartWorkflowExecution, SignalWorkflowExecution, etc. on an active cluster. Global relies on light-weight transactions (paxos) on the local cluster (Local_Quorum) to update the state and create replication which are applied asynchronously to replicate state across clusters. If an application makes these API calls on a cluster where Global is in standby mode, Cadence will reject those calls with DomainNotActiveError, which contains the name of the current active cluster. It is the responsibility of the application to forward the external to the cluster that is currently active.\n\n# New config for Global Domains\n# IsGlobal\nThis config is used to distinguish local to the cluster from the global . It controls the creation of replication on updates allowing the state to be replicated across clusters. This is a read-only setting that can only be set when the is provisioned.\n\n# Clusters\nA list of clusters where the can fail over to, including the current active cluster. This is also a read-only setting that can only be set when the is provisioned. A re-replication feature on the roadmap will allow updating this config to add/remove clusters in the future.\n\n# Active Cluster Name\nName of the current active cluster for the Global . This config is updated each time the Global is failed over to another cluster.\n\n# Failover Version\nUnique failover version which also represents the current active cluster for Global . Cadence allows failover to be triggered from any cluster, so failover version is designed in a way to not allow conflicts if failover is mistakenly triggered simultaneously on two clusters.\n\n# Conflict Resolution\nUnlike local which provide at-most-once semantics for execution, Global can only support at-least-once semantics. Cadence XDC relies on asynchronous replication of across clusters, so in the event of a failover it is possible that gets dispatched again on the new active cluster due to a replication lag. This also means that whenever is updated after a failover by the new cluster, any previous replication for that execution cannot be applied. This results in loss of some progress made by the in the previous active cluster. During such conflict resolution, Cadence re-injects any external like to the new history before discarding replication . Even though some progress could rollback during failovers, Cadence provides the guarantee that won’t get stuck and will continue to make forward progress.\n\n# Visibility API\nAll Visibility APIs are allowed on both active and standby clusters. This enablesCadence Web to work seamlessly for Global as all visibility records for can be queried from any cluster the is replicated to. Applications making API calls directly to the Cadence Visibility API will continue to work even if a Global is in standby mode. However, they might see a lag due to replication delay when the state from a standby cluster.\n\n# CLI\nThe Cadence can also be used to the config or perform failovers. Here are some useful commands.\n\n# Query Global Domain\nThe following command can be used to describe Global metadata:\n\n$ cadence --do cadence-canary-xdc d desc\nName: cadence-canary-xdc\nDescription: cadence canary cross dc testing domain\nOwnerEmail: cadence-dev@cadenceworkflow.io\nDomainData:\nStatus: REGISTERED\nRetentionInDays: 7\nEmitMetrics: true\nActiveClusterName: dc1\nClusters: dc1, dc2\n\n\n# Failover Global Domain\nThe following command can be used to failover Global my-domain-global to the dc2 cluster:\n\n$ cadence --do my-domain-global d up --ac dc2\n\n\n# Running Locally\nThe best way is to use Cadence docker-compose:docker-compose -f docker-compose-multiclusters.yml up\n\n# Running in Production\nEnable global domain feature needs to be enabled in static config.\n\nHere we use clusterDCA and clusterDCB as an example. We pick clusterDCA as the primary(used to called "master") cluster. The only difference of being a primary cluster is that it is responsible for domain registration. Primary can be changed later but it needs to be the same across all clusters.\n\nThe ClusterMeta config of clusterDCA should be\n\nclusterMetadata:\n  enableGlobalDomain: true\n  failoverVersionIncrement: 10\n  masterClusterName: "clusterDCA"\n  currentClusterName: "clusterDCA"\n  clusterInformation:\n    clusterDCA:\n      enabled: true\n      initialFailoverVersion: 1\n      rpcName: "cadence-frontend"\n      rpcAddress: "<>:<>"\n    clusterDCB:\n      enabled: true\n      initialFailoverVersion: 0\n      rpcName: "cadence-frontend"\n      rpcAddress: "<>:<>"\n\n\nAnd ClusterMeta config of clusterDCB should be\n\nclusterMetadata:\n  enableGlobalDomain: true\n  failoverVersionIncrement: 10\n  masterClusterName: "clusterDCA"\n  currentClusterName: "clusterDCB"\n  clusterInformation:\n    clusterDCA:\n      enabled: true\n      initialFailoverVersion: 1\n      rpcName: "cadence-frontend"\n      rpcAddress: "<>:<>"\n    clusterDCB:\n      enabled: true\n      initialFailoverVersion: 0\n\n      rpcName: "cadence-frontend"\n      rpcAddress: "<>:<>"\n\n\nAfter the configuration is deployed:\n\n 1. Register a global domaincadence --do <domain_name> domain register --global_domain true --clusters clusterDCA clusterDCB --active_cluster clusterDCA\n    \n    \n 2. Run some workflow and failover domain from one to anothercadence --do <domain_name> domain update --active_cluster clusterDCB\n    \n    \n\nThen the domain should be failed over to clusterDCB. Now worklfows are read-only in clusterDCA. So your workers polling tasks from clusterDCA will become idle.\n\nNote 1: that even though clusterDCA is standy/read-only for this domain, it can be active for another domain. So being active/standy is per domain basis not per clusters. In other words, for example if you use XDC in case of DC failure of clusterDCA, you need to failover all domains from clusterDCA to clusterDCB.\n\nNote 2: even though a domain is standy/read-only in a cluster, say clusterDCA, sending write requests(startWF, signalWF, etc) could still work because there is a forwarding component in the Frontend service. It will try to re-route the requests to an active cluster for the domain.\n\n# FAQ\n# What happens to outstanding activities after failover?\nCadence does not forward completions across clusters. Any outstanding will eventually timeout based on the configuration. Your application should have retry logic in place so that the gets retried and dispatched again to a after the failover to the new DC. Handling this is pretty much the same as timeout caused by a restart even without Global .\n\n# What happens when a start or signal API call is made to a standby cluster?\nCadence will to forward the API calls to active cluster if possible(some APIs are not forwardable, for example PollForDecisionTask). General APIs like StartWorkflow/SignalWorkflow can be forwarded.',normalizedContent:'# cross-dc replication\nthe cadence global feature provides clients with the capability to continue their from another cluster in the event of a datacenter failover. although you can configure a global to be replicated to any number of clusters, it is only considered active in a single cluster.\n\n# global domains architecture\ncadence has introduced a new top level entity, global , which provides support for replication of execution across clusters. client applications need to run polling on / on all clusters. cadence will only dispatch tasks on the current active cluster; on the standby cluster will sit idle until the global is failed over.\n\nbecause cadence is a service that provides highly consistent semantics, we only allow external likestartworkflowexecution, signalworkflowexecution, etc. on an active cluster. global relies on light-weight transactions (paxos) on the local cluster (local_quorum) to update the state and create replication which are applied asynchronously to replicate state across clusters. if an application makes these api calls on a cluster where global is in standby mode, cadence will reject those calls with domainnotactiveerror, which contains the name of the current active cluster. it is the responsibility of the application to forward the external to the cluster that is currently active.\n\n# new config for global domains\n# isglobal\nthis config is used to distinguish local to the cluster from the global . it controls the creation of replication on updates allowing the state to be replicated across clusters. this is a read-only setting that can only be set when the is provisioned.\n\n# clusters\na list of clusters where the can fail over to, including the current active cluster. this is also a read-only setting that can only be set when the is provisioned. a re-replication feature on the roadmap will allow updating this config to add/remove clusters in the future.\n\n# active cluster name\nname of the current active cluster for the global . this config is updated each time the global is failed over to another cluster.\n\n# failover version\nunique failover version which also represents the current active cluster for global . cadence allows failover to be triggered from any cluster, so failover version is designed in a way to not allow conflicts if failover is mistakenly triggered simultaneously on two clusters.\n\n# conflict resolution\nunlike local which provide at-most-once semantics for execution, global can only support at-least-once semantics. cadence xdc relies on asynchronous replication of across clusters, so in the event of a failover it is possible that gets dispatched again on the new active cluster due to a replication lag. this also means that whenever is updated after a failover by the new cluster, any previous replication for that execution cannot be applied. this results in loss of some progress made by the in the previous active cluster. during such conflict resolution, cadence re-injects any external like to the new history before discarding replication . even though some progress could rollback during failovers, cadence provides the guarantee that won’t get stuck and will continue to make forward progress.\n\n# visibility api\nall visibility apis are allowed on both active and standby clusters. this enablescadence web to work seamlessly for global as all visibility records for can be queried from any cluster the is replicated to. applications making api calls directly to the cadence visibility api will continue to work even if a global is in standby mode. however, they might see a lag due to replication delay when the state from a standby cluster.\n\n# cli\nthe cadence can also be used to the config or perform failovers. here are some useful commands.\n\n# query global domain\nthe following command can be used to describe global metadata:\n\n$ cadence --do cadence-canary-xdc d desc\nname: cadence-canary-xdc\ndescription: cadence canary cross dc testing domain\nowneremail: cadence-dev@cadenceworkflow.io\ndomaindata:\nstatus: registered\nretentionindays: 7\nemitmetrics: true\nactiveclustername: dc1\nclusters: dc1, dc2\n\n\n# failover global domain\nthe following command can be used to failover global my-domain-global to the dc2 cluster:\n\n$ cadence --do my-domain-global d up --ac dc2\n\n\n# running locally\nthe best way is to use cadence docker-compose:docker-compose -f docker-compose-multiclusters.yml up\n\n# running in production\nenable global domain feature needs to be enabled in static config.\n\nhere we use clusterdca and clusterdcb as an example. we pick clusterdca as the primary(used to called "master") cluster. the only difference of being a primary cluster is that it is responsible for domain registration. primary can be changed later but it needs to be the same across all clusters.\n\nthe clustermeta config of clusterdca should be\n\nclustermetadata:\n  enableglobaldomain: true\n  failoverversionincrement: 10\n  masterclustername: "clusterdca"\n  currentclustername: "clusterdca"\n  clusterinformation:\n    clusterdca:\n      enabled: true\n      initialfailoverversion: 1\n      rpcname: "cadence-frontend"\n      rpcaddress: "<>:<>"\n    clusterdcb:\n      enabled: true\n      initialfailoverversion: 0\n      rpcname: "cadence-frontend"\n      rpcaddress: "<>:<>"\n\n\nand clustermeta config of clusterdcb should be\n\nclustermetadata:\n  enableglobaldomain: true\n  failoverversionincrement: 10\n  masterclustername: "clusterdca"\n  currentclustername: "clusterdcb"\n  clusterinformation:\n    clusterdca:\n      enabled: true\n      initialfailoverversion: 1\n      rpcname: "cadence-frontend"\n      rpcaddress: "<>:<>"\n    clusterdcb:\n      enabled: true\n      initialfailoverversion: 0\n\n      rpcname: "cadence-frontend"\n      rpcaddress: "<>:<>"\n\n\nafter the configuration is deployed:\n\n 1. register a global domaincadence --do <domain_name> domain register --global_domain true --clusters clusterdca clusterdcb --active_cluster clusterdca\n    \n    \n 2. run some workflow and failover domain from one to anothercadence --do <domain_name> domain update --active_cluster clusterdcb\n    \n    \n\nthen the domain should be failed over to clusterdcb. now worklfows are read-only in clusterdca. so your workers polling tasks from clusterdca will become idle.\n\nnote 1: that even though clusterdca is standy/read-only for this domain, it can be active for another domain. so being active/standy is per domain basis not per clusters. in other words, for example if you use xdc in case of dc failure of clusterdca, you need to failover all domains from clusterdca to clusterdcb.\n\nnote 2: even though a domain is standy/read-only in a cluster, say clusterdca, sending write requests(startwf, signalwf, etc) could still work because there is a forwarding component in the frontend service. it will try to re-route the requests to an active cluster for the domain.\n\n# faq\n# what happens to outstanding activities after failover?\ncadence does not forward completions across clusters. any outstanding will eventually timeout based on the configuration. your application should have retry logic in place so that the gets retried and dispatched again to a after the failover to the new dc. handling this is pretty much the same as timeout caused by a restart even without global .\n\n# what happens when a start or signal api call is made to a standby cluster?\ncadence will to forward the api calls to active cluster if possible(some apis are not forwardable, for example pollfordecisiontask). general apis like startworkflow/signalworkflow can be forwarded.',charsets:{}},{title:"Search workflows(Advanced visibility)",frontmatter:{layout:"default",title:"Search workflows(Advanced visibility)",permalink:"/docs/concepts/search-workflows",readingShow:"top"},regularPath:"/docs/03-concepts/09-search-workflows.html",relativePath:"docs/03-concepts/09-search-workflows.md",key:"v-0f936964",path:"/docs/concepts/search-workflows/",headers:[{level:2,title:"Introduction",slug:"introduction",normalizedTitle:"introduction",charIndex:45},{level:2,title:"Memo vs Search Attributes",slug:"memo-vs-search-attributes",normalizedTitle:"memo vs search attributes",charIndex:820},{level:2,title:"Search Attributes (Go Client Usage)",slug:"search-attributes-go-client-usage",normalizedTitle:"search attributes (go client usage)",charIndex:2496},{level:3,title:"Allow Listing Search Attributes",slug:"allow-listing-search-attributes",normalizedTitle:"allow listing search attributes",charIndex:2848},{level:3,title:"Value Types",slug:"value-types",normalizedTitle:"value types",charIndex:4916},{level:3,title:"Limit",slug:"limit",normalizedTitle:"limit",charIndex:5125},{level:3,title:"Upsert Search Attributes in Workflow",slug:"upsert-search-attributes-in-workflow",normalizedTitle:"upsert search attributes in workflow",charIndex:5458},{level:3,title:"ContinueAsNew and Cron",slug:"continueasnew-and-cron",normalizedTitle:"continueasnew and cron",charIndex:6757},{level:2,title:"Query Capabilities",slug:"query-capabilities",normalizedTitle:"query capabilities",charIndex:6907},{level:3,title:"Supported Operators",slug:"supported-operators",normalizedTitle:"supported operators",charIndex:7086},{level:3,title:"Default Attributes",slug:"default-attributes",normalizedTitle:"default attributes",charIndex:7184},{level:3,title:"General Notes About Queries",slug:"general-notes-about-queries",normalizedTitle:"general notes about queries",charIndex:9099},{level:2,title:"Tools Support",slug:"tools-support",normalizedTitle:"tools support",charIndex:9621},{level:3,title:"CLI",slug:"cli",normalizedTitle:"cli",charIndex:467},{level:3,title:"Web UI Support",slug:"web-ui-support",normalizedTitle:"web ui support",charIndex:11018},{level:2,title:"Running Locally",slug:"running-locally",normalizedTitle:"running locally",charIndex:11180},{level:2,title:"Running in Production",slug:"running-in-production",normalizedTitle:"running in production",charIndex:11983}],headersStr:"Introduction Memo vs Search Attributes Search Attributes (Go Client Usage) Allow Listing Search Attributes Value Types Limit Upsert Search Attributes in Workflow ContinueAsNew and Cron Query Capabilities Supported Operators Default Attributes General Notes About Queries Tools Support CLI Web UI Support Running Locally Running in Production",content:'# Searching Workflows(Advanced visibility)\n# Introduction\nCadence supports creating with customized key-value pairs, updating the information within the code, and then listing/searching with a SQL-like . For example, you can create with keys city and age, then search all with city = seattle and age > 22.\n\nAlso note that normal properties like start time and type can be queried as well. For example, the following could be specified when listing workflows from the CLI or using the list APIs (Go, Java):\n\nWorkflowType = "main.Workflow" and CloseStatus != 0 and (StartTime > "2019-06-07T16:46:34-08:00" or CloseTime > "2019-06-07T16:46:34-08:00" order by StartTime desc)\n\n\nIn other places, this is also called as advanced visibility. While basic visibility is referred to basic listing without being able to search.\n\n# Memo vs Search Attributes\nCadence offers two methods for creating with key-value pairs: memo and search attributes. Memo can only be provided on start. Also, memo data are not indexed, and are therefore not searchable. Memo data are visible when listing using the list APIs. Search attributes data are indexed so you can search by on these attributes. However, search attributes require the use of Elasticsearch.\n\nMemo and search attributes are available in the Go client in StartWorkflowOptions.\n\ntype StartWorkflowOptions struct {\n    // ...\n\n    // Memo - Optional non-indexed info that will be shown in list workflow.\n    Memo map[string]interface{}\n\n    // SearchAttributes - Optional indexed info that can be used in query of List/Scan/Count workflow APIs (only\n    // supported when Cadence server is using Elasticsearch). The key and value type must be registered on Cadence server side.\n    // Use GetSearchAttributes API to get valid key and corresponding value type.\n    SearchAttributes map[string]interface{}\n}\n\n\nIn the Java client, the WorkflowOptions.Builder has similar methods for memo and search attributes.\n\nSome important distinctions between memo and search attributes:\n\n * Memo can support all data types because it is not indexed. Search attributes only support basic data types (including String, Int, Float, Bool, Datetime) because it is indexed by Elasticsearch.\n * Memo does not restrict on key names. Search attributes require that keys are allowlisted before using them because Elasticsearch has a limit on indexed keys.\n * Memo doesn\'t require Cadence clusters to depend on Elasticsearch while search attributes only works with Elasticsearch.\n\n# Search Attributes (Go Client Usage)\nWhen using the Cadence Go client, provide key-value pairs as SearchAttributes in StartWorkflowOptions.\n\nSearchAttributes is map[string]interface{} where the keys need to be allowlisted so that Cadence knows the attribute key name and value type. The value provided in the map must be the same type as registered.\n\n# Allow Listing Search Attributes\nStart by the list of search attributes using the \n\n$ cadence --domain samples-domain cl get-search-attr\n+---------------------+------------+\n|         KEY         | VALUE TYPE |\n+---------------------+------------+\n| CloseStatus         | INT        |\n| CloseTime           | INT        |\n| CustomBoolField     | DOUBLE     |\n| CustomDatetimeField | DATETIME   |\n| CustomDomain        | KEYWORD    |\n| CustomDoubleField   | BOOL       |\n| CustomIntField      | INT        |\n| CustomKeywordField  | KEYWORD    |\n| CustomStringField   | STRING     |\n| DomainID            | KEYWORD    |\n| ExecutionTime       | INT        |\n| HistoryLength       | INT        |\n| RunID               | KEYWORD    |\n| StartTime           | INT        |\n| WorkflowID          | KEYWORD    |\n| WorkflowType        | KEYWORD    |\n+---------------------+------------+\n\n\nUse the admin to add a new search attribute:\n\ncadence --domain samples-domain adm cl asa --search_attr_key NewKey --search_attr_type 1\n\n\nThe numbers for the attribute types map as follows:\n\n * 0 = String\n * 1 = Keyword\n * 2 = Int\n * 3 = Double\n * 4 = Bool\n * 5 = DateTime\n\n# Keyword vs String\nNote that Keyword and String are concepts taken from Elasticsearch. Each word in a String is considered a searchable keyword. For a UUID, that can be problematic as Elasticsearch will index each portion of the UUID separately. To have the whole string considered as a searchable keyword, use the Keyword type.\n\nFor example, key RunID with value "2dd29ab7-2dd8-4668-83e0-89cae261cfb1"\n\n * as a Keyword will only be matched by RunID = "2dd29ab7-2dd8-4668-83e0-89cae261cfb1" (or in the future with regular expressions)\n * as a String will be matched by RunID = "2dd8", which may cause unwanted matches\n\nNote: String type can not be used in Order By .\n\nThere are some pre-allowlisted search attributes that are handy for testing:\n\n * CustomKeywordField\n * CustomIntField\n * CustomDoubleField\n * CustomBoolField\n * CustomDatetimeField\n * CustomStringField\n\nTheir types are indicated in their names.\n\n# Value Types\nHere are the Search Attribute value types and their correspondent Golang types:\n\n * Keyword = string\n * Int = int64\n * Double = float64\n * Bool = bool\n * Datetime = time.Time\n * String = string\n\n# Limit\nWe recommend limiting the number of Elasticsearch indexes by enforcing limits on the following:\n\n * Number of keys: 100 per \n * Size of value: 2kb per value\n * Total size of key and values: 40kb per \n\nCadence reserves keys like DomainID, WorkflowID, and RunID. These can only be used in list . The values are not updatable.\n\n# Upsert Search Attributes in Workflow\nUpsertSearchAttributes is used to add or update search attributes from within the code.\n\nGo samples for search attributes can be found at github.com/uber-common/cadence-samples.\n\nUpsertSearchAttributes will merge attributes to the existing map in the . Consider this example code:\n\nfunc MyWorkflow(ctx workflow.Context, input string) error {\n\n    attr1 := map[string]interface{}{\n        "CustomIntField": 1,\n        "CustomBoolField": true,\n    }\n    workflow.UpsertSearchAttributes(ctx, attr1)\n\n    attr2 := map[string]interface{}{\n        "CustomIntField": 2,\n        "CustomKeywordField": "seattle",\n    }\n    workflow.UpsertSearchAttributes(ctx, attr2)\n}\n\n\nAfter the second call to UpsertSearchAttributes, the map will contain:\n\nmap[string]interface{}{\n    "CustomIntField": 2,\n    "CustomBoolField": true,\n    "CustomKeywordField": "seattle",\n}\n\n\nThere is no support for removing a field. To achieve a similar effect, set the field to a sentinel value. For example, to remove “CustomKeywordField”, update it to “impossibleVal”. Then searching CustomKeywordField != ‘impossibleVal’ will match with CustomKeywordField not equal to "impossibleVal", which includes without the CustomKeywordField set.\n\nUse workflow.GetInfo to get current search attributes.\n\n# ContinueAsNew and Cron\nWhen performing a ContinueAsNew or using Cron, search attributes (and memo) will be carried over to the new run by default.\n\n# Query Capabilities\n by using a SQL-like where clause when listing workflows from the CLI or using the list APIs (Go, Java).\n\nNote that you will only see from one domain when .\n\n# Supported Operators\n * AND, OR, ()\n * =, !=, >, >=, <, <=\n * IN\n * BETWEEN ... AND\n * ORDER BY\n\n# Default Attributes\nMore and more default attributes are added in newer versions. Please get the by using the get-search-attr command or the GetSearchAttributes API. Some names and types are as follows:\n\nKEY                   VALUE TYPE   \nCloseStatus           INT          \nCloseTime             INT          \nCustomBoolField       DOUBLE       \nCustomDatetimeField   DATETIME     \nCustomDomain          KEYWORD      \nCustomDoubleField     BOOL         \nCustomIntField        INT          \nCustomKeywordField    KEYWORD      \nCustomStringField     STRING       \nDomainID              KEYWORD      \nExecutionTime         INT          \nHistoryLength         INT          \nRunID                 KEYWORD      \nStartTime             INT          \nWorkflowID            KEYWORD      \nWorkflowType          KEYWORD      \nTasklist              KEYWORD      \n\nThere are some special considerations for these attributes:\n\n * CloseStatus, CloseTime, DomainID, ExecutionTime, HistoryLength, RunID, StartTime, WorkflowID, WorkflowType are reserved by Cadence and are read-only\n * CloseStatus is a mapping of int to state: * 0 = completed\n    * 1 = failed\n    * 2 = canceled\n    * 3 = terminated\n    * 4 = continuedasnew\n    * 5 = timedout\n   \n   \n * StartTime, CloseTime and ExecutionTime are stored as INT, but support using both EpochTime in nanoseconds, and string in RFC3339 format (ex. "2006-01-02T15:04:05+07:00")\n * CloseTime, CloseStatus, HistoryLength are only present in closed \n * ExecutionTime is for Retry/Cron user to a that will run in the future\n\nTo list only open , add CloseTime = missing to the end of the .\n\nIf you use retry or the cron feature to that will start execution in a certain time range, you can add predicates on ExecutionTime. For example: ExecutionTime > 2019-01-01T10:00:00-07:00. Note that if predicates on ExecutionTime are included, only cron or a that needs to retry will be returned.\n\n# General Notes About Queries\n * Pagesize default is 1000, and cannot be larger than 10k\n * Range on Cadence timestamp (StartTime, CloseTime, ExecutionTime) cannot be larger than 9223372036854775807 (maxInt64 - 1001)\n *  by time range will have 1ms resolution\n *  column names are case sensitive\n * ListWorkflow may take longer when retrieving a large number of (10M+)\n * To retrieve a large number of without caring about order, use the ScanWorkflow API\n * To efficiently count the number of , use the CountWorkflow API\n\n# Tools Support\n# CLI\nSupport for search attributes is available as of version 0.6.0 of the Cadence server. You can also use the from the latest CLI Docker image (supported on 0.6.4 or later).\n\n# Start Workflow with Search Attributes\ncadence --do samples-domain workflow start --tl helloWorldGroup --wt main.Workflow --et 60 --dt 10 -i \'"vancexu"\' -search_attr_key \'CustomIntField | CustomKeywordField | CustomStringField |  CustomBoolField | CustomDatetimeField\' -search_attr_value \'5 | keyword1 | vancexu test | true | 2019-06-07T16:16:36-08:00\'\n\n\n# Search Workflows with List API\ncadence --do samples-domain wf list -q \'(CustomKeywordField = "keyword1" and CustomIntField >= 5) or CustomKeywordField = "keyword2"\' -psa\n\n\ncadence --do samples-domain wf list -q \'CustomKeywordField in ("keyword2", "keyword1") and CustomIntField >= 5 and CloseTime between "2018-06-07T16:16:36-08:00" and "2019-06-07T16:46:34-08:00" order by CustomDatetimeField desc\' -psa\n\n\nTo list only open , add CloseTime = missing to the end of the .\n\nNote that can support more than one type of filter:\n\ncadence --do samples-domain wf list -q \'WorkflowType = "main.Workflow" and (WorkflowID = "1645a588-4772-4dab-b276-5f9db108b3a8" or RunID = "be66519b-5f09-40cd-b2e8-20e4106244dc")\'\n\n\ncadence --do samples-domain wf list -q \'WorkflowType = "main.Workflow" StartTime > "2019-06-07T16:46:34-08:00" and CloseTime = missing\'\n\n\n# Web UI Support\n are supported in Cadence Web as of release 3.4.0. Use the "Basic/Advanced" button to switch to "Advanced" mode and type the in the search box.\n\n# Running Locally\n 1. Increase Docker memory to higher than 6GB. Navigate to Docker -> Preferences -> Advanced -> Memory\n 2. Get the Cadence Docker compose file. Run curl -O https://raw.githubusercontent.com/uber/cadence/master/docker/docker-compose-es.yml\n 3. Start Cadence Docker (which contains Apache Kafka, Apache Zookeeper, and Elasticsearch) using docker-compose -f docker-compose-es.yml up\n 4. From the Docker output log, make sure Elasticsearch and Cadence started correctly. If you encounter an insufficient disk space error, try docker system prune -a --volumes\n 5. Register a local domain and start using it. cadence --do samples-domain d re\n 6. Add the key to ElasticSearch And also allowlist search attributes. cadence --do domain adm cl asa --search_attr_key NewKey --search_attr_type 1\n\n# Running in Production\nTo enable this feature in a Cadence cluster:\n\n * Register index schema on ElasticSearch. Run two CURL commands following this script. * Create a index template by using the schema , choose v6/v7 based on your ElasticSearch version\n    * Create an index follow the index template, remember the name\n   \n   \n * Register topic on Kafka, and remember the name * Set up the right number of partitions based on your expected throughput(can be scaled up later)\n   \n   \n * Configure Cadence for ElasticSearch + Kafka like this documentationBased on the full static config, you may add some other fields like AuthN. Similarly for Kafka.\n\nTo add new search attributes:\n\n 1. Add the key to ElasticSearch cadence --do domain adm cl asa --search_attr_key NewKey --search_attr_type 1\n 2. Update the dynamic configuration to allowlist the new attribute\n\nNote: starting a with search attributes but without advanced visibility feature will succeed as normal, but will not be searchable and will not be shown in list results.',normalizedContent:'# searching workflows(advanced visibility)\n# introduction\ncadence supports creating with customized key-value pairs, updating the information within the code, and then listing/searching with a sql-like . for example, you can create with keys city and age, then search all with city = seattle and age > 22.\n\nalso note that normal properties like start time and type can be queried as well. for example, the following could be specified when listing workflows from the cli or using the list apis (go, java):\n\nworkflowtype = "main.workflow" and closestatus != 0 and (starttime > "2019-06-07t16:46:34-08:00" or closetime > "2019-06-07t16:46:34-08:00" order by starttime desc)\n\n\nin other places, this is also called as advanced visibility. while basic visibility is referred to basic listing without being able to search.\n\n# memo vs search attributes\ncadence offers two methods for creating with key-value pairs: memo and search attributes. memo can only be provided on start. also, memo data are not indexed, and are therefore not searchable. memo data are visible when listing using the list apis. search attributes data are indexed so you can search by on these attributes. however, search attributes require the use of elasticsearch.\n\nmemo and search attributes are available in the go client in startworkflowoptions.\n\ntype startworkflowoptions struct {\n    // ...\n\n    // memo - optional non-indexed info that will be shown in list workflow.\n    memo map[string]interface{}\n\n    // searchattributes - optional indexed info that can be used in query of list/scan/count workflow apis (only\n    // supported when cadence server is using elasticsearch). the key and value type must be registered on cadence server side.\n    // use getsearchattributes api to get valid key and corresponding value type.\n    searchattributes map[string]interface{}\n}\n\n\nin the java client, the workflowoptions.builder has similar methods for memo and search attributes.\n\nsome important distinctions between memo and search attributes:\n\n * memo can support all data types because it is not indexed. search attributes only support basic data types (including string, int, float, bool, datetime) because it is indexed by elasticsearch.\n * memo does not restrict on key names. search attributes require that keys are allowlisted before using them because elasticsearch has a limit on indexed keys.\n * memo doesn\'t require cadence clusters to depend on elasticsearch while search attributes only works with elasticsearch.\n\n# search attributes (go client usage)\nwhen using the cadence go client, provide key-value pairs as searchattributes in startworkflowoptions.\n\nsearchattributes is map[string]interface{} where the keys need to be allowlisted so that cadence knows the attribute key name and value type. the value provided in the map must be the same type as registered.\n\n# allow listing search attributes\nstart by the list of search attributes using the \n\n$ cadence --domain samples-domain cl get-search-attr\n+---------------------+------------+\n|         key         | value type |\n+---------------------+------------+\n| closestatus         | int        |\n| closetime           | int        |\n| customboolfield     | double     |\n| customdatetimefield | datetime   |\n| customdomain        | keyword    |\n| customdoublefield   | bool       |\n| customintfield      | int        |\n| customkeywordfield  | keyword    |\n| customstringfield   | string     |\n| domainid            | keyword    |\n| executiontime       | int        |\n| historylength       | int        |\n| runid               | keyword    |\n| starttime           | int        |\n| workflowid          | keyword    |\n| workflowtype        | keyword    |\n+---------------------+------------+\n\n\nuse the admin to add a new search attribute:\n\ncadence --domain samples-domain adm cl asa --search_attr_key newkey --search_attr_type 1\n\n\nthe numbers for the attribute types map as follows:\n\n * 0 = string\n * 1 = keyword\n * 2 = int\n * 3 = double\n * 4 = bool\n * 5 = datetime\n\n# keyword vs string\nnote that keyword and string are concepts taken from elasticsearch. each word in a string is considered a searchable keyword. for a uuid, that can be problematic as elasticsearch will index each portion of the uuid separately. to have the whole string considered as a searchable keyword, use the keyword type.\n\nfor example, key runid with value "2dd29ab7-2dd8-4668-83e0-89cae261cfb1"\n\n * as a keyword will only be matched by runid = "2dd29ab7-2dd8-4668-83e0-89cae261cfb1" (or in the future with regular expressions)\n * as a string will be matched by runid = "2dd8", which may cause unwanted matches\n\nnote: string type can not be used in order by .\n\nthere are some pre-allowlisted search attributes that are handy for testing:\n\n * customkeywordfield\n * customintfield\n * customdoublefield\n * customboolfield\n * customdatetimefield\n * customstringfield\n\ntheir types are indicated in their names.\n\n# value types\nhere are the search attribute value types and their correspondent golang types:\n\n * keyword = string\n * int = int64\n * double = float64\n * bool = bool\n * datetime = time.time\n * string = string\n\n# limit\nwe recommend limiting the number of elasticsearch indexes by enforcing limits on the following:\n\n * number of keys: 100 per \n * size of value: 2kb per value\n * total size of key and values: 40kb per \n\ncadence reserves keys like domainid, workflowid, and runid. these can only be used in list . the values are not updatable.\n\n# upsert search attributes in workflow\nupsertsearchattributes is used to add or update search attributes from within the code.\n\ngo samples for search attributes can be found at github.com/uber-common/cadence-samples.\n\nupsertsearchattributes will merge attributes to the existing map in the . consider this example code:\n\nfunc myworkflow(ctx workflow.context, input string) error {\n\n    attr1 := map[string]interface{}{\n        "customintfield": 1,\n        "customboolfield": true,\n    }\n    workflow.upsertsearchattributes(ctx, attr1)\n\n    attr2 := map[string]interface{}{\n        "customintfield": 2,\n        "customkeywordfield": "seattle",\n    }\n    workflow.upsertsearchattributes(ctx, attr2)\n}\n\n\nafter the second call to upsertsearchattributes, the map will contain:\n\nmap[string]interface{}{\n    "customintfield": 2,\n    "customboolfield": true,\n    "customkeywordfield": "seattle",\n}\n\n\nthere is no support for removing a field. to achieve a similar effect, set the field to a sentinel value. for example, to remove “customkeywordfield”, update it to “impossibleval”. then searching customkeywordfield != ‘impossibleval’ will match with customkeywordfield not equal to "impossibleval", which includes without the customkeywordfield set.\n\nuse workflow.getinfo to get current search attributes.\n\n# continueasnew and cron\nwhen performing a continueasnew or using cron, search attributes (and memo) will be carried over to the new run by default.\n\n# query capabilities\n by using a sql-like where clause when listing workflows from the cli or using the list apis (go, java).\n\nnote that you will only see from one domain when .\n\n# supported operators\n * and, or, ()\n * =, !=, >, >=, <, <=\n * in\n * between ... and\n * order by\n\n# default attributes\nmore and more default attributes are added in newer versions. please get the by using the get-search-attr command or the getsearchattributes api. some names and types are as follows:\n\nkey                   value type   \nclosestatus           int          \nclosetime             int          \ncustomboolfield       double       \ncustomdatetimefield   datetime     \ncustomdomain          keyword      \ncustomdoublefield     bool         \ncustomintfield        int          \ncustomkeywordfield    keyword      \ncustomstringfield     string       \ndomainid              keyword      \nexecutiontime         int          \nhistorylength         int          \nrunid                 keyword      \nstarttime             int          \nworkflowid            keyword      \nworkflowtype          keyword      \ntasklist              keyword      \n\nthere are some special considerations for these attributes:\n\n * closestatus, closetime, domainid, executiontime, historylength, runid, starttime, workflowid, workflowtype are reserved by cadence and are read-only\n * closestatus is a mapping of int to state: * 0 = completed\n    * 1 = failed\n    * 2 = canceled\n    * 3 = terminated\n    * 4 = continuedasnew\n    * 5 = timedout\n   \n   \n * starttime, closetime and executiontime are stored as int, but support using both epochtime in nanoseconds, and string in rfc3339 format (ex. "2006-01-02t15:04:05+07:00")\n * closetime, closestatus, historylength are only present in closed \n * executiontime is for retry/cron user to a that will run in the future\n\nto list only open , add closetime = missing to the end of the .\n\nif you use retry or the cron feature to that will start execution in a certain time range, you can add predicates on executiontime. for example: executiontime > 2019-01-01t10:00:00-07:00. note that if predicates on executiontime are included, only cron or a that needs to retry will be returned.\n\n# general notes about queries\n * pagesize default is 1000, and cannot be larger than 10k\n * range on cadence timestamp (starttime, closetime, executiontime) cannot be larger than 9223372036854775807 (maxint64 - 1001)\n *  by time range will have 1ms resolution\n *  column names are case sensitive\n * listworkflow may take longer when retrieving a large number of (10m+)\n * to retrieve a large number of without caring about order, use the scanworkflow api\n * to efficiently count the number of , use the countworkflow api\n\n# tools support\n# cli\nsupport for search attributes is available as of version 0.6.0 of the cadence server. you can also use the from the latest cli docker image (supported on 0.6.4 or later).\n\n# start workflow with search attributes\ncadence --do samples-domain workflow start --tl helloworldgroup --wt main.workflow --et 60 --dt 10 -i \'"vancexu"\' -search_attr_key \'customintfield | customkeywordfield | customstringfield |  customboolfield | customdatetimefield\' -search_attr_value \'5 | keyword1 | vancexu test | true | 2019-06-07t16:16:36-08:00\'\n\n\n# search workflows with list api\ncadence --do samples-domain wf list -q \'(customkeywordfield = "keyword1" and customintfield >= 5) or customkeywordfield = "keyword2"\' -psa\n\n\ncadence --do samples-domain wf list -q \'customkeywordfield in ("keyword2", "keyword1") and customintfield >= 5 and closetime between "2018-06-07t16:16:36-08:00" and "2019-06-07t16:46:34-08:00" order by customdatetimefield desc\' -psa\n\n\nto list only open , add closetime = missing to the end of the .\n\nnote that can support more than one type of filter:\n\ncadence --do samples-domain wf list -q \'workflowtype = "main.workflow" and (workflowid = "1645a588-4772-4dab-b276-5f9db108b3a8" or runid = "be66519b-5f09-40cd-b2e8-20e4106244dc")\'\n\n\ncadence --do samples-domain wf list -q \'workflowtype = "main.workflow" starttime > "2019-06-07t16:46:34-08:00" and closetime = missing\'\n\n\n# web ui support\n are supported in cadence web as of release 3.4.0. use the "basic/advanced" button to switch to "advanced" mode and type the in the search box.\n\n# running locally\n 1. increase docker memory to higher than 6gb. navigate to docker -> preferences -> advanced -> memory\n 2. get the cadence docker compose file. run curl -o https://raw.githubusercontent.com/uber/cadence/master/docker/docker-compose-es.yml\n 3. start cadence docker (which contains apache kafka, apache zookeeper, and elasticsearch) using docker-compose -f docker-compose-es.yml up\n 4. from the docker output log, make sure elasticsearch and cadence started correctly. if you encounter an insufficient disk space error, try docker system prune -a --volumes\n 5. register a local domain and start using it. cadence --do samples-domain d re\n 6. add the key to elasticsearch and also allowlist search attributes. cadence --do domain adm cl asa --search_attr_key newkey --search_attr_type 1\n\n# running in production\nto enable this feature in a cadence cluster:\n\n * register index schema on elasticsearch. run two curl commands following this script. * create a index template by using the schema , choose v6/v7 based on your elasticsearch version\n    * create an index follow the index template, remember the name\n   \n   \n * register topic on kafka, and remember the name * set up the right number of partitions based on your expected throughput(can be scaled up later)\n   \n   \n * configure cadence for elasticsearch + kafka like this documentationbased on the full static config, you may add some other fields like authn. similarly for kafka.\n\nto add new search attributes:\n\n 1. add the key to elasticsearch cadence --do domain adm cl asa --search_attr_key newkey --search_attr_type 1\n 2. update the dynamic configuration to allowlist the new attribute\n\nnote: starting a with search attributes but without advanced visibility feature will succeed as normal, but will not be searchable and will not be shown in list results.',charsets:{cjk:!0}},{title:"Introduction",frontmatter:{layout:"default",title:"Introduction",permalink:"/docs/concepts",readingShow:"top"},regularPath:"/docs/03-concepts/",relativePath:"docs/03-concepts/index.md",key:"v-496bb1df",path:"/docs/concepts/",headersStr:null,content:"# Concepts\nCadence is a new developer friendly way to develop distributed applications.\n\nIt borrows the core terminology from the workflow-automation space. So its concepts include workflows and activities. can react to events and return internal state through queries.\n\nThe deployment topology explains how all these concepts are mapped to deployable software components.",normalizedContent:"# concepts\ncadence is a new developer friendly way to develop distributed applications.\n\nit borrows the core terminology from the workflow-automation space. so its concepts include workflows and activities. can react to events and return internal state through queries.\n\nthe deployment topology explains how all these concepts are mapped to deployable software components.",charsets:{}},{title:"Client SDK Overview",frontmatter:{layout:"default",title:"Client SDK Overview",permalink:"/docs/java-client/client-overview",readingShow:"top"},regularPath:"/docs/04-java-client/01-client-overview.html",relativePath:"docs/04-java-client/01-client-overview.md",key:"v-211cf634",path:"/docs/java-client/client-overview/",headers:[{level:2,title:"JavaDoc Packages",slug:"javadoc-packages",normalizedTitle:"javadoc packages",charIndex:167},{level:3,title:"com.uber.cadence.activity",slug:"com-uber-cadence-activity",normalizedTitle:"com.uber.cadence.activity",charIndex:186},{level:3,title:"com.uber.cadence.client",slug:"com-uber-cadence-client",normalizedTitle:"com.uber.cadence.client",charIndex:290},{level:3,title:"com.uber.cadence.workflow",slug:"com-uber-cadence-workflow",normalizedTitle:"com.uber.cadence.workflow",charIndex:438},{level:3,title:"com.uber.cadence.worker",slug:"com-uber-cadence-worker",normalizedTitle:"com.uber.cadence.worker",charIndex:496},{level:3,title:"com.uber.cadence.testing",slug:"com-uber-cadence-testing",normalizedTitle:"com.uber.cadence.testing",charIndex:560},{level:2,title:"Samples",slug:"samples",normalizedTitle:"samples",charIndex:25},{level:3,title:"com.uber.cadence.samples.hello",slug:"com-uber-cadence-samples-hello",normalizedTitle:"com.uber.cadence.samples.hello",charIndex:638},{level:3,title:"com.uber.cadence.samples.bookingsaga",slug:"com-uber-cadence-samples-bookingsaga",normalizedTitle:"com.uber.cadence.samples.bookingsaga",charIndex:825},{level:3,title:"com.uber.cadence.samples.fileprocessing",slug:"com-uber-cadence-samples-fileprocessing",normalizedTitle:"com.uber.cadence.samples.fileprocessing",charIndex:922}],headersStr:"JavaDoc Packages com.uber.cadence.activity com.uber.cadence.client com.uber.cadence.workflow com.uber.cadence.worker com.uber.cadence.testing Samples com.uber.cadence.samples.hello com.uber.cadence.samples.bookingsaga com.uber.cadence.samples.fileprocessing",content:"# Client SDK Overview\n * Samples: https://github.com/uber/cadence-java-samples\n * JavaDoc documentation: https://www.javadoc.io/doc/com.uber.cadence/cadence-client\n\n# JavaDoc Packages\n# com.uber.cadence.activity\nAPIs to implement activity: accessing activity info, or sending heartbeat.\n\n# com.uber.cadence.client\nAPIs for external application code to interact with Cadence workflows: start workflows, send signals or query workflows.\n\n# com.uber.cadence.workflow\nAPIs to implement workflows.\n\n# com.uber.cadence.worker\nAPIs to configure and start workers.\n\n# com.uber.cadence.testing\nAPIs to write unit tests for workflows.\n\n# Samples\n# com.uber.cadence.samples.hello\nSamples of how to use the basic feature: activity, local activity, ChildWorkflow, Query, etc. This is the most important package you need to start with.\n\n# com.uber.cadence.samples.bookingsaga\nAn end-to-end example to write workflow using SAGA APIs.\n\n# com.uber.cadence.samples.fileprocessing\nAn end-to-end example to write workflows to download a file, zips it, and uploads it to a destination.\n\nAn important requirement for such a workflow is that while a first activity can run on any host, the second and third must run on the same host as the first one. This is achieved through use of a host specific task list. The first activity returns the name of the host specific task list and all other activities are dispatched using the stub that is configured with it. This assumes that FileProcessingWorker has a worker running on the same task list.",normalizedContent:"# client sdk overview\n * samples: https://github.com/uber/cadence-java-samples\n * javadoc documentation: https://www.javadoc.io/doc/com.uber.cadence/cadence-client\n\n# javadoc packages\n# com.uber.cadence.activity\napis to implement activity: accessing activity info, or sending heartbeat.\n\n# com.uber.cadence.client\napis for external application code to interact with cadence workflows: start workflows, send signals or query workflows.\n\n# com.uber.cadence.workflow\napis to implement workflows.\n\n# com.uber.cadence.worker\napis to configure and start workers.\n\n# com.uber.cadence.testing\napis to write unit tests for workflows.\n\n# samples\n# com.uber.cadence.samples.hello\nsamples of how to use the basic feature: activity, local activity, childworkflow, query, etc. this is the most important package you need to start with.\n\n# com.uber.cadence.samples.bookingsaga\nan end-to-end example to write workflow using saga apis.\n\n# com.uber.cadence.samples.fileprocessing\nan end-to-end example to write workflows to download a file, zips it, and uploads it to a destination.\n\nan important requirement for such a workflow is that while a first activity can run on any host, the second and third must run on the same host as the first one. this is achieved through use of a host specific task list. the first activity returns the name of the host specific task list and all other activities are dispatched using the stub that is configured with it. this assumes that fileprocessingworker has a worker running on the same task list.",charsets:{}},{title:"Workflow interface",frontmatter:{layout:"default",title:"Workflow interface",permalink:"/docs/java-client/workflow-interface",readingShow:"top"},regularPath:"/docs/04-java-client/02-workflow-interface.html",relativePath:"docs/04-java-client/02-workflow-interface.md",key:"v-3d11f802",path:"/docs/java-client/workflow-interface/",headersStr:null,content:'# Workflow interface\n encapsulates the orchestration of and child . It can also answer synchronous and receive external (also known as ).\n\nA must define an interface class. All of its methods must have one of the following annotations:\n\n * @WorkflowMethod indicates an entry point to a . It contains parameters such as timeouts and a . Required parameters (such as executionStartToCloseTimeoutSeconds) that are not specified through the annotation must be provided at runtime.\n * @SignalMethod indicates a method that reacts to external . It must have a void return type.\n * @QueryMethod indicates a method that reacts to synchronous requests.\n\nYou can have more than one method with the same annotation (except @WorkflowMethod). For example:\n\npublic interface FileProcessingWorkflow {\n\n    @WorkflowMethod(executionStartToCloseTimeoutSeconds = 10, taskList = "file-processing")\n    String processFile(Arguments args);\n\n    @QueryMethod(name="history")\n    List<String> getHistory();\n\n    @QueryMethod(name="status")\n    String getStatus();\n\n    @SignalMethod\n    void retryNow();\n\n    @SignalMethod\n    void abandon();\n}\n\n\nWe recommended that you use a single value type argument for methods. In this way, adding new arguments as fields to the value type is a backwards-compatible change.',normalizedContent:'# workflow interface\n encapsulates the orchestration of and child . it can also answer synchronous and receive external (also known as ).\n\na must define an interface class. all of its methods must have one of the following annotations:\n\n * @workflowmethod indicates an entry point to a . it contains parameters such as timeouts and a . required parameters (such as executionstarttoclosetimeoutseconds) that are not specified through the annotation must be provided at runtime.\n * @signalmethod indicates a method that reacts to external . it must have a void return type.\n * @querymethod indicates a method that reacts to synchronous requests.\n\nyou can have more than one method with the same annotation (except @workflowmethod). for example:\n\npublic interface fileprocessingworkflow {\n\n    @workflowmethod(executionstarttoclosetimeoutseconds = 10, tasklist = "file-processing")\n    string processfile(arguments args);\n\n    @querymethod(name="history")\n    list<string> gethistory();\n\n    @querymethod(name="status")\n    string getstatus();\n\n    @signalmethod\n    void retrynow();\n\n    @signalmethod\n    void abandon();\n}\n\n\nwe recommended that you use a single value type argument for methods. in this way, adding new arguments as fields to the value type is a backwards-compatible change.',charsets:{}},{title:"Implementing workflows",frontmatter:{layout:"default",title:"Implementing workflows",permalink:"/docs/java-client/implementing-workflows",readingShow:"top"},regularPath:"/docs/04-java-client/03-implementing-workflows.html",relativePath:"docs/04-java-client/03-implementing-workflows.md",key:"v-264670c2",path:"/docs/java-client/implementing-workflows/",headers:[{level:2,title:"Calling Activities",slug:"calling-activities",normalizedTitle:"calling activities",charIndex:513},{level:2,title:"Calling Activities Asynchronously",slug:"calling-activities-asynchronously",normalizedTitle:"calling activities asynchronously",charIndex:2715},{level:2,title:"Workflow Implementation Constraints",slug:"workflow-implementation-constraints",normalizedTitle:"workflow implementation constraints",charIndex:5577}],headersStr:"Calling Activities Calling Activities Asynchronously Workflow Implementation Constraints",content:"# Implementing workflows\nA implementation implements a interface. Each time a new is started, a new instance of the implementation object is created. Then, one of the methods (depending on which type has been started) annotated with @WorkflowMethod is invoked. As soon as this method returns, the is closed. While is open, it can receive calls to and methods. No additional calls to methods are allowed. The object is stateful, so and methods can communicate with the other parts of the through object fields.\n\n# Calling Activities\nWorkflow.newActivityStub returns a client-side stub that implements an interface. It takes type and options as arguments. options are needed only if some of the required timeouts are not specified through the @ActivityMethod annotation.\n\nCalling a method on this interface invokes an that implements this method. An invocation synchronously blocks until the completes, fails, or times out. Even if execution takes a few months, the code still sees it as a single synchronous invocation. It doesn't matter what happens to the processes that host the . The business logic code just sees a single method call.\n\npublic class FileProcessingWorkflowImpl implements FileProcessingWorkflow {\n\n    private final FileProcessingActivities activities;\n\n    public FileProcessingWorkflowImpl() {\n        this.activities = Workflow.newActivityStub(FileProcessingActivities.class);\n    }\n\n    @Override\n    public void processFile(Arguments args) {\n        String localName = null;\n        String processedName = null;\n        try {\n            localName = activities.download(args.getSourceBucketName(), args.getSourceFilename());\n            processedName = activities.processFile(localName);\n            activities.upload(args.getTargetBucketName(), args.getTargetFilename(), processedName);\n        } finally {\n            if (localName != null) { // File was downloaded.\n                activities.deleteLocalFile(localName);\n            }\n            if (processedName != null) { // File was processed.\n                activities.deleteLocalFile(processedName);\n            }\n        }\n    }\n    ...\n}\n\n\nIf different need different options, like timeouts or a , multiple client-side stubs can be created with different options.\n\npublic FileProcessingWorkflowImpl() {\n    ActivityOptions options1 = new ActivityOptions.Builder()\n             .setTaskList(\"taskList1\")\n             .build();\n    this.store1 = Workflow.newActivityStub(FileProcessingActivities.class, options1);\n\n    ActivityOptions options2 = new ActivityOptions.Builder()\n             .setTaskList(\"taskList2\")\n             .build();\n    this.store2 = Workflow.newActivityStub(FileProcessingActivities.class, options2);\n}\n\n\n# Calling Activities Asynchronously\nSometimes need to perform certain operations in parallel. The Async class static methods allow you to invoke any asynchronously. The calls return a Promise result immediately.Promise is similar to both Java Future and CompletionStage. The Promise get blocks until a result is available. It also exposes the thenApply and handle methods. See the Promise JavaDoc for technical details about differences with Future.\n\nTo convert a synchronous call:\n\nString localName = activities.download(sourceBucket, sourceFile);\n\n\nTo asynchronous style, the method reference is passed to Async.function or Async.procedurefollowed by arguments:\n\nPromise<String> localNamePromise = Async.function(activities::download, sourceBucket, sourceFile);\n\n\nThen to wait synchronously for the result:\n\nString localName = localNamePromise.get();\n\n\nHere is the above example rewritten to call download and upload in parallel on multiple files:\n\npublic void processFile(Arguments args) {\n    List<Promise<String>> localNamePromises = new ArrayList<>();\n    List<String> processedNames = null;\n    try {\n        // Download all files in parallel.\n        for (String sourceFilename : args.getSourceFilenames()) {\n            Promise<String> localName = Async.function(activities::download,\n                args.getSourceBucketName(), sourceFilename);\n            localNamePromises.add(localName);\n        }\n        // allOf converts a list of promises to a single promise that contains a list\n        // of each promise value.\n        Promise<List<String>> localNamesPromise = Promise.allOf(localNamePromises);\n\n        // All code until the next line wasn't blocking.\n        // The promise get is a blocking call.\n        List<String> localNames = localNamesPromise.get();\n        processedNames = activities.processFiles(localNames);\n\n        // Upload all results in parallel.\n        List<Promise<Void>> uploadedList = new ArrayList<>();\n        for (String processedName : processedNames) {\n            Promise<Void> uploaded = Async.procedure(activities::upload,\n                args.getTargetBucketName(), args.getTargetFilename(), processedName);\n            uploadedList.add(uploaded);\n        }\n        // Wait for all uploads to complete.\n        Promise<?> allUploaded = Promise.allOf(uploadedList);\n        allUploaded.get(); // blocks until all promises are ready.\n    } finally {\n        for (Promise<String> localNamePromise : localNamePromises) {\n            // Skip files that haven't completed downloading.\n            if (localNamePromise.isCompleted()) {\n                activities.deleteLocalFile(localNamePromise.get());\n            }\n        }\n        if (processedNames != null) {\n            for (String processedName : processedNames) {\n                activities.deleteLocalFile(processedName);\n            }\n        }\n    }\n}\n\n\n# Workflow Implementation Constraints\nCadence uses the Microsoft Azure Event Sourcing pattern to recover the state of a object including its threads and local variable values. In essence, every time a state has to be restored, its code is re-executed from the beginning. When replaying, side effects (such as invocations) are ignored because they are already recorded in the . When writing logic, the replay is not visible, so the code should be written since it executes only once. This design puts the following constraints on the implementation:\n\n * Do not use any mutable global variables because multiple instances of are executed in parallel.\n * Do not call any non-deterministic functions like non seeded random or UUID.randomUUID() directly from the code.\n\nAlways do the following in :\n\n * Don’t perform any IO or service calls as they are not usually deterministic. Use for this.\n * Only use Workflow.currentTimeMillis() to get the current time inside a .\n * Do not use native Java Thread or any other multi-threaded classes like ThreadPoolExecutor. Use Async.function or Async.procedureto execute code asynchronously.\n * Don't use any synchronization, locks, and other standard Java blocking concurrency-related classes besides those provided by the Workflow class. There is no need in explicit synchronization because multi-threaded code inside a is executed one thread at a time and under a global lock. * Call WorkflowThread.sleep instead of Thread.sleep.\n    * Use Promise and CompletablePromise instead of Future and CompletableFuture.\n    * Use WorkflowQueue instead of BlockingQueue.\n   \n   \n * Use Workflow.getVersion when making any changes to the code. Without this, any deployment of updated code might break already open .\n * Don’t access configuration APIs directly from a because changes in the configuration might affect a path. Pass it as an argument to a function or use an to load it.\n\n method arguments and return values are serializable to a byte array using the providedDataConverterinterface. The default implementation uses JSON serializer, but you can use any alternative serialization mechanism.\n\nThe values passed to through invocation parameters or returned through a result value are recorded in the execution history. The entire execution history is transferred from the Cadence service to with every that the logic needs to process. A large execution history can thus adversely impact the performance of your . Therefore, be mindful of the amount of data that you transfer via invocation parameters or return values. Otherwise, no additional limitations exist on implementations.",normalizedContent:"# implementing workflows\na implementation implements a interface. each time a new is started, a new instance of the implementation object is created. then, one of the methods (depending on which type has been started) annotated with @workflowmethod is invoked. as soon as this method returns, the is closed. while is open, it can receive calls to and methods. no additional calls to methods are allowed. the object is stateful, so and methods can communicate with the other parts of the through object fields.\n\n# calling activities\nworkflow.newactivitystub returns a client-side stub that implements an interface. it takes type and options as arguments. options are needed only if some of the required timeouts are not specified through the @activitymethod annotation.\n\ncalling a method on this interface invokes an that implements this method. an invocation synchronously blocks until the completes, fails, or times out. even if execution takes a few months, the code still sees it as a single synchronous invocation. it doesn't matter what happens to the processes that host the . the business logic code just sees a single method call.\n\npublic class fileprocessingworkflowimpl implements fileprocessingworkflow {\n\n    private final fileprocessingactivities activities;\n\n    public fileprocessingworkflowimpl() {\n        this.activities = workflow.newactivitystub(fileprocessingactivities.class);\n    }\n\n    @override\n    public void processfile(arguments args) {\n        string localname = null;\n        string processedname = null;\n        try {\n            localname = activities.download(args.getsourcebucketname(), args.getsourcefilename());\n            processedname = activities.processfile(localname);\n            activities.upload(args.gettargetbucketname(), args.gettargetfilename(), processedname);\n        } finally {\n            if (localname != null) { // file was downloaded.\n                activities.deletelocalfile(localname);\n            }\n            if (processedname != null) { // file was processed.\n                activities.deletelocalfile(processedname);\n            }\n        }\n    }\n    ...\n}\n\n\nif different need different options, like timeouts or a , multiple client-side stubs can be created with different options.\n\npublic fileprocessingworkflowimpl() {\n    activityoptions options1 = new activityoptions.builder()\n             .settasklist(\"tasklist1\")\n             .build();\n    this.store1 = workflow.newactivitystub(fileprocessingactivities.class, options1);\n\n    activityoptions options2 = new activityoptions.builder()\n             .settasklist(\"tasklist2\")\n             .build();\n    this.store2 = workflow.newactivitystub(fileprocessingactivities.class, options2);\n}\n\n\n# calling activities asynchronously\nsometimes need to perform certain operations in parallel. the async class static methods allow you to invoke any asynchronously. the calls return a promise result immediately.promise is similar to both java future and completionstage. the promise get blocks until a result is available. it also exposes the thenapply and handle methods. see the promise javadoc for technical details about differences with future.\n\nto convert a synchronous call:\n\nstring localname = activities.download(sourcebucket, sourcefile);\n\n\nto asynchronous style, the method reference is passed to async.function or async.procedurefollowed by arguments:\n\npromise<string> localnamepromise = async.function(activities::download, sourcebucket, sourcefile);\n\n\nthen to wait synchronously for the result:\n\nstring localname = localnamepromise.get();\n\n\nhere is the above example rewritten to call download and upload in parallel on multiple files:\n\npublic void processfile(arguments args) {\n    list<promise<string>> localnamepromises = new arraylist<>();\n    list<string> processednames = null;\n    try {\n        // download all files in parallel.\n        for (string sourcefilename : args.getsourcefilenames()) {\n            promise<string> localname = async.function(activities::download,\n                args.getsourcebucketname(), sourcefilename);\n            localnamepromises.add(localname);\n        }\n        // allof converts a list of promises to a single promise that contains a list\n        // of each promise value.\n        promise<list<string>> localnamespromise = promise.allof(localnamepromises);\n\n        // all code until the next line wasn't blocking.\n        // the promise get is a blocking call.\n        list<string> localnames = localnamespromise.get();\n        processednames = activities.processfiles(localnames);\n\n        // upload all results in parallel.\n        list<promise<void>> uploadedlist = new arraylist<>();\n        for (string processedname : processednames) {\n            promise<void> uploaded = async.procedure(activities::upload,\n                args.gettargetbucketname(), args.gettargetfilename(), processedname);\n            uploadedlist.add(uploaded);\n        }\n        // wait for all uploads to complete.\n        promise<?> alluploaded = promise.allof(uploadedlist);\n        alluploaded.get(); // blocks until all promises are ready.\n    } finally {\n        for (promise<string> localnamepromise : localnamepromises) {\n            // skip files that haven't completed downloading.\n            if (localnamepromise.iscompleted()) {\n                activities.deletelocalfile(localnamepromise.get());\n            }\n        }\n        if (processednames != null) {\n            for (string processedname : processednames) {\n                activities.deletelocalfile(processedname);\n            }\n        }\n    }\n}\n\n\n# workflow implementation constraints\ncadence uses the microsoft azure event sourcing pattern to recover the state of a object including its threads and local variable values. in essence, every time a state has to be restored, its code is re-executed from the beginning. when replaying, side effects (such as invocations) are ignored because they are already recorded in the . when writing logic, the replay is not visible, so the code should be written since it executes only once. this design puts the following constraints on the implementation:\n\n * do not use any mutable global variables because multiple instances of are executed in parallel.\n * do not call any non-deterministic functions like non seeded random or uuid.randomuuid() directly from the code.\n\nalways do the following in :\n\n * don’t perform any io or service calls as they are not usually deterministic. use for this.\n * only use workflow.currenttimemillis() to get the current time inside a .\n * do not use native java thread or any other multi-threaded classes like threadpoolexecutor. use async.function or async.procedureto execute code asynchronously.\n * don't use any synchronization, locks, and other standard java blocking concurrency-related classes besides those provided by the workflow class. there is no need in explicit synchronization because multi-threaded code inside a is executed one thread at a time and under a global lock. * call workflowthread.sleep instead of thread.sleep.\n    * use promise and completablepromise instead of future and completablefuture.\n    * use workflowqueue instead of blockingqueue.\n   \n   \n * use workflow.getversion when making any changes to the code. without this, any deployment of updated code might break already open .\n * don’t access configuration apis directly from a because changes in the configuration might affect a path. pass it as an argument to a function or use an to load it.\n\n method arguments and return values are serializable to a byte array using the provideddataconverterinterface. the default implementation uses json serializer, but you can use any alternative serialization mechanism.\n\nthe values passed to through invocation parameters or returned through a result value are recorded in the execution history. the entire execution history is transferred from the cadence service to with every that the logic needs to process. a large execution history can thus adversely impact the performance of your . therefore, be mindful of the amount of data that you transfer via invocation parameters or return values. otherwise, no additional limitations exist on implementations.",charsets:{}},{title:"Starting workflows",frontmatter:{layout:"default",title:"Starting workflows",permalink:"/docs/java-client/starting-workflow-executions",readingShow:"top"},regularPath:"/docs/04-java-client/04-starting-workflow-executions.html",relativePath:"docs/04-java-client/04-starting-workflow-executions.md",key:"v-11997e3c",path:"/docs/java-client/starting-workflow-executions/",headersStr:null,content:'# Starting workflow executions\nA interface that executes a requires initializing a WorkflowClient instance, creating a client side stub to the , and then calling a method annotated with @WorkflowMethod.\n\nWorkflowClient workflowClient =\n        WorkflowClient.newInstance(\n            new WorkflowServiceTChannel(\n                ClientOptions.newBuilder().setHost(cadenceServiceHost).setPort(cadenceServicePort).build()),\n            WorkflowClientOptions.newBuilder().setDomain(domain).build());\n// Create a workflow stub.\nFileProcessingWorkflow workflow = workflowClient.newWorkflowStub(FileProcessingWorkflow.class);\n\n\nIf you are using version prior to 3.0.0\n\nWorkflowClient workflowClient = WorkflowClient.newClient(cadenceServiceHost, cadenceServicePort, domain);\n// Create a workflow stub.\nFileProcessingWorkflow workflow = workflowClient.newWorkflowStub(FileProcessingWorkflow.class);\n\n\nThere are two ways to start asynchronously and synchronously. Asynchronous start initiates a and immediately returns to the caller. This is the most common way to start in production code. Synchronous invocation starts a and then waits for its completion. If the process that started the crashes or stops waiting, the continues executing. Because are potentially long running, and crashes of clients happen, this is not very commonly found in production use.\n\nAsynchronous start:\n\n// Returns as soon as the workflow starts.\nWorkflowExecution workflowExecution = WorkflowClient.start(workflow::processFile, workflowArgs);\n\nSystem.out.println("Started process file workflow with workflowId=\\"" + workflowExecution.getWorkflowId()\n                    + "\\" and runId=\\"" + workflowExecution.getRunId() + "\\"");\n\n\nSynchronous start:\n\n// Start a workflow and then wait for a result.\n// Note that if the waiting process is killed, the workflow will continue execution.\nString result = workflow.processFile(workflowArgs);\n\n\nIf you need to wait for a completion after an asynchronous start, the most straightforward way is to call the blocking version again. If WorkflowOptions.WorkflowIdReusePolicy is not AllowDuplicate, then instead of throwing DuplicateWorkflowException, it reconnects to an existing and waits for its completion. The following example shows how to do this from a different process than the one that started the . All this process needs is a WorkflowID.\n\nWorkflowExecution execution = new WorkflowExecution().setWorkflowId(workflowId);\nFileProcessingWorkflow workflow = workflowClient.newWorkflowStub(execution);\n// Returns result potentially waiting for workflow to complete.\nString result = workflow.processFile(workflowArgs);',normalizedContent:'# starting workflow executions\na interface that executes a requires initializing a workflowclient instance, creating a client side stub to the , and then calling a method annotated with @workflowmethod.\n\nworkflowclient workflowclient =\n        workflowclient.newinstance(\n            new workflowservicetchannel(\n                clientoptions.newbuilder().sethost(cadenceservicehost).setport(cadenceserviceport).build()),\n            workflowclientoptions.newbuilder().setdomain(domain).build());\n// create a workflow stub.\nfileprocessingworkflow workflow = workflowclient.newworkflowstub(fileprocessingworkflow.class);\n\n\nif you are using version prior to 3.0.0\n\nworkflowclient workflowclient = workflowclient.newclient(cadenceservicehost, cadenceserviceport, domain);\n// create a workflow stub.\nfileprocessingworkflow workflow = workflowclient.newworkflowstub(fileprocessingworkflow.class);\n\n\nthere are two ways to start asynchronously and synchronously. asynchronous start initiates a and immediately returns to the caller. this is the most common way to start in production code. synchronous invocation starts a and then waits for its completion. if the process that started the crashes or stops waiting, the continues executing. because are potentially long running, and crashes of clients happen, this is not very commonly found in production use.\n\nasynchronous start:\n\n// returns as soon as the workflow starts.\nworkflowexecution workflowexecution = workflowclient.start(workflow::processfile, workflowargs);\n\nsystem.out.println("started process file workflow with workflowid=\\"" + workflowexecution.getworkflowid()\n                    + "\\" and runid=\\"" + workflowexecution.getrunid() + "\\"");\n\n\nsynchronous start:\n\n// start a workflow and then wait for a result.\n// note that if the waiting process is killed, the workflow will continue execution.\nstring result = workflow.processfile(workflowargs);\n\n\nif you need to wait for a completion after an asynchronous start, the most straightforward way is to call the blocking version again. if workflowoptions.workflowidreusepolicy is not allowduplicate, then instead of throwing duplicateworkflowexception, it reconnects to an existing and waits for its completion. the following example shows how to do this from a different process than the one that started the . all this process needs is a workflowid.\n\nworkflowexecution execution = new workflowexecution().setworkflowid(workflowid);\nfileprocessingworkflow workflow = workflowclient.newworkflowstub(execution);\n// returns result potentially waiting for workflow to complete.\nstring result = workflow.processfile(workflowargs);',charsets:{}},{title:"Activity interface",frontmatter:{layout:"default",title:"Activity interface",permalink:"/docs/java-client/activity-interface",readingShow:"top"},regularPath:"/docs/04-java-client/05-activity-interface.html",relativePath:"docs/04-java-client/05-activity-interface.md",key:"v-5a20c23c",path:"/docs/java-client/activity-interface/",headersStr:null,content:"# Activity interface\nAn is a manifestation of a particular in the business logic.\n\n are defined as methods of a plain Java interface. Each method defines a single type. A single can use more than one interface and call more that one method from the same interface. The only requirement is that method arguments and return values are serializable to a byte array using the providedDataConverterinterface. The default implementation uses a JSON serializer, but an alternative implementation can be easily configured.\n\nFollowing is an example of an interface that defines four activities:\n\npublic interface FileProcessingActivities {\n\n    void upload(String bucketName, String localName, String targetName);\n\n    String download(String bucketName, String remoteName);\n\n    @ActivityMethod(scheduleToCloseTimeoutSeconds = 2)\n    String processFile(String localName);\n\n    void deleteLocalFile(String fileName);\n}\n\n\n\nWe recommend to use a single value type argument for methods. In this way, adding new arguments as fields to the value type is a backwards-compatible change.\n\nAn optional @ActivityMethod annotation can be used to specify options like timeouts or a . Required options that are not specified through the annotation must be specified at runtime.",normalizedContent:"# activity interface\nan is a manifestation of a particular in the business logic.\n\n are defined as methods of a plain java interface. each method defines a single type. a single can use more than one interface and call more that one method from the same interface. the only requirement is that method arguments and return values are serializable to a byte array using the provideddataconverterinterface. the default implementation uses a json serializer, but an alternative implementation can be easily configured.\n\nfollowing is an example of an interface that defines four activities:\n\npublic interface fileprocessingactivities {\n\n    void upload(string bucketname, string localname, string targetname);\n\n    string download(string bucketname, string remotename);\n\n    @activitymethod(scheduletoclosetimeoutseconds = 2)\n    string processfile(string localname);\n\n    void deletelocalfile(string filename);\n}\n\n\n\nwe recommend to use a single value type argument for methods. in this way, adding new arguments as fields to the value type is a backwards-compatible change.\n\nan optional @activitymethod annotation can be used to specify options like timeouts or a . required options that are not specified through the annotation must be specified at runtime.",charsets:{}},{title:"Implementing activities",frontmatter:{layout:"default",title:"Implementing activities",permalink:"/docs/java-client/implementing-activities",readingShow:"top"},regularPath:"/docs/04-java-client/06-implementing-activities.html",relativePath:"docs/04-java-client/06-implementing-activities.md",key:"v-056557ea",path:"/docs/java-client/implementing-activities/",headers:[{level:2,title:"Accessing Activity Info",slug:"accessing-activity-info",normalizedTitle:"accessing activity info",charIndex:1517},{level:2,title:"Asynchronous Activity Completion",slug:"asynchronous-activity-completion",normalizedTitle:"asynchronous activity completion",charIndex:2510},{level:2,title:"Activity Heart Beating",slug:"activity-heart-beating",normalizedTitle:"activity heart beating",charIndex:3924}],headersStr:"Accessing Activity Info Asynchronous Activity Completion Activity Heart Beating",content:'# Implementing activities\n implementation is an implementation of an interface. A single instance of the implementation is shared across multiple simultaneous invocations. Therefore, the implementation code must be thread safe.\n\nThe values passed to through invocation parameters or returned through a result value are recorded in the execution history. The entire execution history is transferred from the Cadence service to when a state needs to recover. A large execution history can thus adversely impact the performance of your . Therefore, be mindful of the amount of data you transfer via invocation parameters or return values. Otherwise, no additional limitations exist on implementations.\n\npublic class FileProcessingActivitiesImpl implements FileProcessingActivities {\n\n    private final AmazonS3 s3Client;\n\n    private final String localDirectory;\n\n    void upload(String bucketName, String localName, String targetName) {\n        File f = new File(localName);\n        s3Client.putObject(bucket, remoteName, f);\n    }\n\n    String download(String bucketName, String remoteName, String localName) {\n        // Implementation omitted for brevity.\n        return downloadFileFromS3(bucketName, remoteName, localDirectory + localName);\n    }\n\n    String processFile(String localName) {\n        // Implementation omitted for brevity.\n        return compressFile(localName);\n    }\n\n    void deleteLocalFile(String fileName) {\n        File f = new File(localDirectory + fileName);\n        f.delete();\n    }\n}\n\n\n# Accessing Activity Info\nThe Activityclass provides static getters to access information about the that invoked it. Note that this information is stored in a thread local variable. Therefore, calls to accessors succeed only in the thread that invoked the function.\n\npublic class FileProcessingActivitiesImpl implements FileProcessingActivities {\n\n    @Override\n    public String download(String bucketName, String remoteName, String localName) {\n        log.info("domain=" +  Activity.getDomain());\n        WorkflowExecution execution = Activity.getWorkflowExecution();\n        log.info("workflowId=" + execution.getWorkflowId());\n        log.info("runId=" + execution.getRunId());\n        ActivityTask activityTask = Activity.getTask();\n        log.info("activityId=" + activityTask.getActivityId());\n        log.info("activityTimeout=" + activityTask.getStartToCloseTimeoutSeconds());\n        return downloadFileFromS3(bucketName, remoteName, localDirectory + localName);\n    }\n    ...\n}\n\n\n# Asynchronous Activity Completion\nSometimes an lifecycle goes beyond a synchronous method invocation. For example, a request can be put in a queue and later a reply comes and is picked up by a different process. The whole request-reply interaction can be modeled as a single Cadence .\n\nTo indicate that an should not be completed upon its method return, call Activity.doNotCompleteOnReturn() from the original thread. Then later, when replies come, complete the using ActivityCompletionClient. To correlate invocation with completion, use either TaskToken or and IDs.\n\npublic class FileProcessingActivitiesImpl implements FileProcessingActivities {\n\n    public String download(String bucketName, String remoteName, String localName) {\n        byte[] taskToken = Activity.getTaskToken(); // Used to correlate reply.\n        asyncDownloadFileFromS3(taskToken, bucketName, remoteName, localDirectory + localName);\n        Activity.doNotCompleteOnReturn();\n        return "ignored"; // Return value is ignored when doNotCompleteOnReturn was called.\n    }\n    ...\n}\n\n\nWhen the download is complete, the download service potentially calls back from a different process:\n\npublic <R> void completeActivity(byte[] taskToken, R result) {\n    completionClient.complete(taskToken, result);\n}\n\npublic void failActivity(byte[] taskToken, Exception failure) {\n    completionClient.completeExceptionally(taskToken, failure);\n}\n\n\n# Activity Heart Beating\nSome are long running. To react to a crash quickly, use a heartbeat mechanism. The Activity.heartbeat function lets the Cadence service know that the is still alive. You can piggybackdetails on an heartbeat. If an times out, the last value of details is included in the ActivityTimeoutException delivered to a . Then the can pass the details to the next invocation. This acts as a periodic checkpoint mechanism for the progress of an .\n\npublic class FileProcessingActivitiesImpl implements FileProcessingActivities {\n\n    @Override\n    public String download(String bucketName, String remoteName, String localName) {\n        InputStream inputStream = openInputStream(file);\n        try {\n            byte[] bytes = new byte[MAX_BUFFER_SIZE];\n            while ((read = inputStream.read(bytes)) != -1) {\n                totalRead += read;\n                f.write(bytes, 0, read);\n                /*\n                 * Let the service know about the download progress.\n                 */\n                Activity.heartbeat(totalRead);\n            }\n        } finally {\n            inputStream.close();\n        }\n    }\n    ...\n}',normalizedContent:'# implementing activities\n implementation is an implementation of an interface. a single instance of the implementation is shared across multiple simultaneous invocations. therefore, the implementation code must be thread safe.\n\nthe values passed to through invocation parameters or returned through a result value are recorded in the execution history. the entire execution history is transferred from the cadence service to when a state needs to recover. a large execution history can thus adversely impact the performance of your . therefore, be mindful of the amount of data you transfer via invocation parameters or return values. otherwise, no additional limitations exist on implementations.\n\npublic class fileprocessingactivitiesimpl implements fileprocessingactivities {\n\n    private final amazons3 s3client;\n\n    private final string localdirectory;\n\n    void upload(string bucketname, string localname, string targetname) {\n        file f = new file(localname);\n        s3client.putobject(bucket, remotename, f);\n    }\n\n    string download(string bucketname, string remotename, string localname) {\n        // implementation omitted for brevity.\n        return downloadfilefroms3(bucketname, remotename, localdirectory + localname);\n    }\n\n    string processfile(string localname) {\n        // implementation omitted for brevity.\n        return compressfile(localname);\n    }\n\n    void deletelocalfile(string filename) {\n        file f = new file(localdirectory + filename);\n        f.delete();\n    }\n}\n\n\n# accessing activity info\nthe activityclass provides static getters to access information about the that invoked it. note that this information is stored in a thread local variable. therefore, calls to accessors succeed only in the thread that invoked the function.\n\npublic class fileprocessingactivitiesimpl implements fileprocessingactivities {\n\n    @override\n    public string download(string bucketname, string remotename, string localname) {\n        log.info("domain=" +  activity.getdomain());\n        workflowexecution execution = activity.getworkflowexecution();\n        log.info("workflowid=" + execution.getworkflowid());\n        log.info("runid=" + execution.getrunid());\n        activitytask activitytask = activity.gettask();\n        log.info("activityid=" + activitytask.getactivityid());\n        log.info("activitytimeout=" + activitytask.getstarttoclosetimeoutseconds());\n        return downloadfilefroms3(bucketname, remotename, localdirectory + localname);\n    }\n    ...\n}\n\n\n# asynchronous activity completion\nsometimes an lifecycle goes beyond a synchronous method invocation. for example, a request can be put in a queue and later a reply comes and is picked up by a different process. the whole request-reply interaction can be modeled as a single cadence .\n\nto indicate that an should not be completed upon its method return, call activity.donotcompleteonreturn() from the original thread. then later, when replies come, complete the using activitycompletionclient. to correlate invocation with completion, use either tasktoken or and ids.\n\npublic class fileprocessingactivitiesimpl implements fileprocessingactivities {\n\n    public string download(string bucketname, string remotename, string localname) {\n        byte[] tasktoken = activity.gettasktoken(); // used to correlate reply.\n        asyncdownloadfilefroms3(tasktoken, bucketname, remotename, localdirectory + localname);\n        activity.donotcompleteonreturn();\n        return "ignored"; // return value is ignored when donotcompleteonreturn was called.\n    }\n    ...\n}\n\n\nwhen the download is complete, the download service potentially calls back from a different process:\n\npublic <r> void completeactivity(byte[] tasktoken, r result) {\n    completionclient.complete(tasktoken, result);\n}\n\npublic void failactivity(byte[] tasktoken, exception failure) {\n    completionclient.completeexceptionally(tasktoken, failure);\n}\n\n\n# activity heart beating\nsome are long running. to react to a crash quickly, use a heartbeat mechanism. the activity.heartbeat function lets the cadence service know that the is still alive. you can piggybackdetails on an heartbeat. if an times out, the last value of details is included in the activitytimeoutexception delivered to a . then the can pass the details to the next invocation. this acts as a periodic checkpoint mechanism for the progress of an .\n\npublic class fileprocessingactivitiesimpl implements fileprocessingactivities {\n\n    @override\n    public string download(string bucketname, string remotename, string localname) {\n        inputstream inputstream = openinputstream(file);\n        try {\n            byte[] bytes = new byte[max_buffer_size];\n            while ((read = inputstream.read(bytes)) != -1) {\n                totalread += read;\n                f.write(bytes, 0, read);\n                /*\n                 * let the service know about the download progress.\n                 */\n                activity.heartbeat(totalread);\n            }\n        } finally {\n            inputstream.close();\n        }\n    }\n    ...\n}',charsets:{}},{title:"Versioning",frontmatter:{layout:"default",title:"Versioning",permalink:"/docs/java-client/versioning",readingShow:"top"},regularPath:"/docs/04-java-client/07-versioning.html",relativePath:"docs/04-java-client/07-versioning.md",key:"v-ee26987c",path:"/docs/java-client/versioning/",headersStr:null,content:'# Versioning\nAs outlined in the Workflow Implementation Constraints section, code has to be deterministic by taking the same code path when replaying history . Any code change that affects the order in which are generated breaks this assumption. The solution that allows updating code of already running is to keep both the old and new code. When replaying, use the code version that the were generated with and when executing a new code path, always take the new code.\n\nUse the Workflow.getVersion function to return a version of the code that should be executed and then use the returned value to pick a correct branch. Let\'s look at an example.\n\npublic void processFile(Arguments args) {\n    String localName = null;\n    String processedName = null;\n    try {\n        localName = activities.download(args.getSourceBucketName(), args.getSourceFilename());\n        processedName = activities.processFile(localName);\n        activities.upload(args.getTargetBucketName(), args.getTargetFilename(), processedName);\n    } finally {\n        if (localName != null) { // File was downloaded.\n            activities.deleteLocalFile(localName);\n        }\n        if (processedName != null) { // File was processed.\n            activities.deleteLocalFile(processedName);\n        }\n    }\n}\n\n\nNow we decide to calculate the processed file checksum and pass it to upload. The correct way to implement this change is:\n\npublic void processFile(Arguments args) {\n    String localName = null;\n    String processedName = null;\n    try {\n        localName = activities.download(args.getSourceBucketName(), args.getSourceFilename());\n        processedName = activities.processFile(localName);\n        int version = Workflow.getVersion("checksumAdded", Workflow.DEFAULT_VERSION, 1);\n        if (version == Workflow.DEFAULT_VERSION) {\n            activities.upload(args.getTargetBucketName(), args.getTargetFilename(), processedName);\n        } else {\n            long checksum = activities.calculateChecksum(processedName);\n            activities.uploadWithChecksum(\n                args.getTargetBucketName(), args.getTargetFilename(), processedName, checksum);\n        }\n    } finally {\n        if (localName != null) { // File was downloaded.\n            activities.deleteLocalFile(localName);\n        }\n        if (processedName != null) { // File was processed.\n            activities.deleteLocalFile(processedName);\n        }\n    }\n}\n\n\nLater, when all that use the old version are completed, the old branch can be removed.\n\npublic void processFile(Arguments args) {\n    String localName = null;\n    String processedName = null;\n    try {\n        localName = activities.download(args.getSourceBucketName(), args.getSourceFilename());\n        processedName = activities.processFile(localName);\n        // getVersion call is left here to ensure that any attempt to replay history\n        // for a different version fails. It can be removed later when there is no possibility\n        // of this happening.\n        Workflow.getVersion("checksumAdded", 1, 1);\n        long checksum = activities.calculateChecksum(processedName);\n        activities.uploadWithChecksum(\n            args.getTargetBucketName(), args.getTargetFilename(), processedName, checksum);\n    } finally {\n        if (localName != null) { // File was downloaded.\n            activities.deleteLocalFile(localName);\n        }\n        if (processedName != null) { // File was processed.\n            activities.deleteLocalFile(processedName);\n        }\n    }\n}\n\n\nThe ID that is passed to the getVersion call identifies the change. Each change is expected to have its own ID. But if a change spawns multiple places in the code and the new code should be either executed in all of them or in none of them, then they have to share the ID.',normalizedContent:'# versioning\nas outlined in the workflow implementation constraints section, code has to be deterministic by taking the same code path when replaying history . any code change that affects the order in which are generated breaks this assumption. the solution that allows updating code of already running is to keep both the old and new code. when replaying, use the code version that the were generated with and when executing a new code path, always take the new code.\n\nuse the workflow.getversion function to return a version of the code that should be executed and then use the returned value to pick a correct branch. let\'s look at an example.\n\npublic void processfile(arguments args) {\n    string localname = null;\n    string processedname = null;\n    try {\n        localname = activities.download(args.getsourcebucketname(), args.getsourcefilename());\n        processedname = activities.processfile(localname);\n        activities.upload(args.gettargetbucketname(), args.gettargetfilename(), processedname);\n    } finally {\n        if (localname != null) { // file was downloaded.\n            activities.deletelocalfile(localname);\n        }\n        if (processedname != null) { // file was processed.\n            activities.deletelocalfile(processedname);\n        }\n    }\n}\n\n\nnow we decide to calculate the processed file checksum and pass it to upload. the correct way to implement this change is:\n\npublic void processfile(arguments args) {\n    string localname = null;\n    string processedname = null;\n    try {\n        localname = activities.download(args.getsourcebucketname(), args.getsourcefilename());\n        processedname = activities.processfile(localname);\n        int version = workflow.getversion("checksumadded", workflow.default_version, 1);\n        if (version == workflow.default_version) {\n            activities.upload(args.gettargetbucketname(), args.gettargetfilename(), processedname);\n        } else {\n            long checksum = activities.calculatechecksum(processedname);\n            activities.uploadwithchecksum(\n                args.gettargetbucketname(), args.gettargetfilename(), processedname, checksum);\n        }\n    } finally {\n        if (localname != null) { // file was downloaded.\n            activities.deletelocalfile(localname);\n        }\n        if (processedname != null) { // file was processed.\n            activities.deletelocalfile(processedname);\n        }\n    }\n}\n\n\nlater, when all that use the old version are completed, the old branch can be removed.\n\npublic void processfile(arguments args) {\n    string localname = null;\n    string processedname = null;\n    try {\n        localname = activities.download(args.getsourcebucketname(), args.getsourcefilename());\n        processedname = activities.processfile(localname);\n        // getversion call is left here to ensure that any attempt to replay history\n        // for a different version fails. it can be removed later when there is no possibility\n        // of this happening.\n        workflow.getversion("checksumadded", 1, 1);\n        long checksum = activities.calculatechecksum(processedname);\n        activities.uploadwithchecksum(\n            args.gettargetbucketname(), args.gettargetfilename(), processedname, checksum);\n    } finally {\n        if (localname != null) { // file was downloaded.\n            activities.deletelocalfile(localname);\n        }\n        if (processedname != null) { // file was processed.\n            activities.deletelocalfile(processedname);\n        }\n    }\n}\n\n\nthe id that is passed to the getversion call identifies the change. each change is expected to have its own id. but if a change spawns multiple places in the code and the new code should be either executed in all of them or in none of them, then they have to share the id.',charsets:{}},{title:"Distributed CRON",frontmatter:{layout:"default",title:"Distributed CRON",permalink:"/docs/java-client/distributed-cron",readingShow:"top"},regularPath:"/docs/04-java-client/08-distributed-cron.html",relativePath:"docs/04-java-client/08-distributed-cron.md",key:"v-5951033c",path:"/docs/java-client/distributed-cron/",headers:[{level:2,title:"Convert an existing cron workflow",slug:"convert-an-existing-cron-workflow",normalizedTitle:"convert an existing cron workflow",charIndex:1998},{level:2,title:"Retrieve last successful result",slug:"retrieve-last-successful-result",normalizedTitle:"retrieve last successful result",charIndex:2460}],headersStr:"Convert an existing cron workflow Retrieve last successful result",content:"# Distributed CRON\nIt is relatively straightforward to turn any Cadence into a Cron . All you need is to supply a cron schedule when starting the using the CronSchedule parameter ofStartWorkflowOptions.\n\nYou can also start a using the Cadence with an optional cron schedule using the --cron argument.\n\nFor with CronSchedule:\n\n * CronSchedule is based on UTC time. For example cron schedule \"15 8 * * *\" will run daily at 8:15am UTC.\n * If a failed and a RetryPolicy is supplied to the StartWorkflowOptions as well, the will retry based on the RetryPolicy. While the is retrying, the server will not schedule the next cron run.\n * Cadence server only schedules the next cron run after the current run is completed. If the next schedule is due while a is running (or retrying), then it will skip that schedule.\n * Cron will not stop until they are terminated or cancelled.\n\nCadence supports the standard cron spec:\n\n// CronSchedule - Optional cron schedule for workflow. If a cron schedule is specified, the workflow will run\n// as a cron based on the schedule. The scheduling will be based on UTC time. The schedule for the next run only happens\n// after the current run is completed/failed/timeout. If a RetryPolicy is also supplied, and the workflow failed\n// or timed out, the workflow will be retried based on the retry policy. While the workflow is retrying, it won't\n// schedule its next run. If the next schedule is due while the workflow is running (or retrying), then it will skip that\n// schedule. Cron workflow will not stop until it is terminated or cancelled (by returning cadence.CanceledError).\n// The cron spec is as follows:\n// ┌───────────── minute (0 - 59)\n// │ ┌───────────── hour (0 - 23)\n// │ │ ┌───────────── day of the month (1 - 31)\n// │ │ │ ┌───────────── month (1 - 12)\n// │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday)\n// │ │ │ │ │\n// │ │ │ │ │\n// * * * * *\nCronSchedule string\n\n\nThe crontab guru site is useful for testing your cron expressions.\n\n# Convert an existing cron workflow\nBefore CronSchedule was available, the previous approach to implementing cron was to use a delay timer as the last step and then returnContinueAsNew. One problem with that implementation is that if the fails or times out, the cron would stop.\n\nTo convert those to make use of Cadence CronSchedule, all you need is to remove the delay timer and return without usingContinueAsNew. Then start the with the desired CronSchedule.\n\n# Retrieve last successful result\nSometimes it is useful to obtain the progress of previous successful runs. This is supported by two new APIs in the client library:HasLastCompletionResult and GetLastCompletionResult. Below is an example of how to use this in Java:\n\npublic String cronWorkflow() {\n    String lastProcessedFileName = Workflow.getLastCompletionResult(String.class);\n\n    // Process work starting from the lastProcessedFileName.\n    // Business logic implementation goes here.\n    // Updates lastProcessedFileName to the new value.\n\n    return lastProcessedFileName;\n}\n\n\nNote that this works even if one of the cron schedule runs failed. The next schedule will still get the last successful result if it ever successfully completed at least once. For example, for a daily cron , if the first day run succeeds and the second day fails, then the third day run will still get the result from first day's run using these APIs.",normalizedContent:"# distributed cron\nit is relatively straightforward to turn any cadence into a cron . all you need is to supply a cron schedule when starting the using the cronschedule parameter ofstartworkflowoptions.\n\nyou can also start a using the cadence with an optional cron schedule using the --cron argument.\n\nfor with cronschedule:\n\n * cronschedule is based on utc time. for example cron schedule \"15 8 * * *\" will run daily at 8:15am utc.\n * if a failed and a retrypolicy is supplied to the startworkflowoptions as well, the will retry based on the retrypolicy. while the is retrying, the server will not schedule the next cron run.\n * cadence server only schedules the next cron run after the current run is completed. if the next schedule is due while a is running (or retrying), then it will skip that schedule.\n * cron will not stop until they are terminated or cancelled.\n\ncadence supports the standard cron spec:\n\n// cronschedule - optional cron schedule for workflow. if a cron schedule is specified, the workflow will run\n// as a cron based on the schedule. the scheduling will be based on utc time. the schedule for the next run only happens\n// after the current run is completed/failed/timeout. if a retrypolicy is also supplied, and the workflow failed\n// or timed out, the workflow will be retried based on the retry policy. while the workflow is retrying, it won't\n// schedule its next run. if the next schedule is due while the workflow is running (or retrying), then it will skip that\n// schedule. cron workflow will not stop until it is terminated or cancelled (by returning cadence.cancelederror).\n// the cron spec is as follows:\n// ┌───────────── minute (0 - 59)\n// │ ┌───────────── hour (0 - 23)\n// │ │ ┌───────────── day of the month (1 - 31)\n// │ │ │ ┌───────────── month (1 - 12)\n// │ │ │ │ ┌───────────── day of the week (0 - 6) (sunday to saturday)\n// │ │ │ │ │\n// │ │ │ │ │\n// * * * * *\ncronschedule string\n\n\nthe crontab guru site is useful for testing your cron expressions.\n\n# convert an existing cron workflow\nbefore cronschedule was available, the previous approach to implementing cron was to use a delay timer as the last step and then returncontinueasnew. one problem with that implementation is that if the fails or times out, the cron would stop.\n\nto convert those to make use of cadence cronschedule, all you need is to remove the delay timer and return without usingcontinueasnew. then start the with the desired cronschedule.\n\n# retrieve last successful result\nsometimes it is useful to obtain the progress of previous successful runs. this is supported by two new apis in the client library:haslastcompletionresult and getlastcompletionresult. below is an example of how to use this in java:\n\npublic string cronworkflow() {\n    string lastprocessedfilename = workflow.getlastcompletionresult(string.class);\n\n    // process work starting from the lastprocessedfilename.\n    // business logic implementation goes here.\n    // updates lastprocessedfilename to the new value.\n\n    return lastprocessedfilename;\n}\n\n\nnote that this works even if one of the cron schedule runs failed. the next schedule will still get the last successful result if it ever successfully completed at least once. for example, for a daily cron , if the first day run succeeds and the second day fails, then the third day run will still get the result from first day's run using these apis.",charsets:{}},{title:"Worker service",frontmatter:{layout:"default",title:"Worker service",permalink:"/docs/java-client/workers",readingShow:"top"},regularPath:"/docs/04-java-client/09-workers.html",relativePath:"docs/04-java-client/09-workers.md",key:"v-8f63b5a0",path:"/docs/java-client/workers/",headersStr:null,content:"# Worker service\nA or service is a service that hosts the and implementations. The polls the Cadence service for , performs those , and communicates execution results back to the Cadence service. services are developed, deployed, and operated by Cadence customers.\n\nYou can run a Cadence in a new or an existing service. Use the framework APIs to start the Cadence and link in all and implementations that you require the service to execute.\n\n  WorkerFactory factory = WorkerFactory.newInstance(workflowClient,\n          WorkerFactoryOptions.newBuilder()\n                  .setMaxWorkflowThreadCount(1000)\n                  .setStickyCacheSize(100)\n                  .setDisableStickyExecution(false)\n                  .build());\n  Worker worker = factory.newWorker(TASK_LIST,\n          WorkerOptions.newBuilder()\n                  .setMaxConcurrentActivityExecutionSize(100)\n                  .setMaxConcurrentWorkflowExecutionSize(100)\n                  .build());\n                  \n    // Workflows are stateful. So you need a type to create instances.\n    worker.registerWorkflowImplementationTypes(GreetingWorkflowImpl.class);\n    // Activities are stateless and thread safe. So a shared instance is used.\n    worker.registerActivitiesImplementations(new GreetingActivitiesImpl());\n    // Start listening to the workflow and activity task lists.\n    factory.start();\n\n\nThe code is slightly different if you are using client version prior to 3.0.0:\n\nWorker.Factory factory = new Worker.Factory(DOMAIN,\n            new Worker.FactoryOptions.Builder()\n                    .setMaxWorkflowThreadCount(1000)\n                    .setCacheMaximumSize(100)\n                    .setDisableStickyExecution(false)\n                    .build());\n    Worker worker = factory.newWorker(TASK_LIST,\n            new WorkerOptions.Builder()\n                    .setMaxConcurrentActivityExecutionSize(100)\n                    .setMaxConcurrentWorkflowExecutionSize(100)\n                    .build());\n    // Workflows are stateful. So you need a type to create instances.\n    worker.registerWorkflowImplementationTypes(GreetingWorkflowImpl.class);\n    // Activities are stateless and thread safe. So a shared instance is used.\n    worker.registerActivitiesImplementations(new GreetingActivitiesImpl());\n    // Start listening to the workflow and activity task lists.\n    factory.start();\n\n\nThe WorkerFactoryOptions includes those that need to be shared across workers on the hosts like thread pool, sticky cache.\n\nIn WorkerOptions you can customize things like pollerOptions, activities per second.",normalizedContent:"# worker service\na or service is a service that hosts the and implementations. the polls the cadence service for , performs those , and communicates execution results back to the cadence service. services are developed, deployed, and operated by cadence customers.\n\nyou can run a cadence in a new or an existing service. use the framework apis to start the cadence and link in all and implementations that you require the service to execute.\n\n  workerfactory factory = workerfactory.newinstance(workflowclient,\n          workerfactoryoptions.newbuilder()\n                  .setmaxworkflowthreadcount(1000)\n                  .setstickycachesize(100)\n                  .setdisablestickyexecution(false)\n                  .build());\n  worker worker = factory.newworker(task_list,\n          workeroptions.newbuilder()\n                  .setmaxconcurrentactivityexecutionsize(100)\n                  .setmaxconcurrentworkflowexecutionsize(100)\n                  .build());\n                  \n    // workflows are stateful. so you need a type to create instances.\n    worker.registerworkflowimplementationtypes(greetingworkflowimpl.class);\n    // activities are stateless and thread safe. so a shared instance is used.\n    worker.registeractivitiesimplementations(new greetingactivitiesimpl());\n    // start listening to the workflow and activity task lists.\n    factory.start();\n\n\nthe code is slightly different if you are using client version prior to 3.0.0:\n\nworker.factory factory = new worker.factory(domain,\n            new worker.factoryoptions.builder()\n                    .setmaxworkflowthreadcount(1000)\n                    .setcachemaximumsize(100)\n                    .setdisablestickyexecution(false)\n                    .build());\n    worker worker = factory.newworker(task_list,\n            new workeroptions.builder()\n                    .setmaxconcurrentactivityexecutionsize(100)\n                    .setmaxconcurrentworkflowexecutionsize(100)\n                    .build());\n    // workflows are stateful. so you need a type to create instances.\n    worker.registerworkflowimplementationtypes(greetingworkflowimpl.class);\n    // activities are stateless and thread safe. so a shared instance is used.\n    worker.registeractivitiesimplementations(new greetingactivitiesimpl());\n    // start listening to the workflow and activity task lists.\n    factory.start();\n\n\nthe workerfactoryoptions includes those that need to be shared across workers on the hosts like thread pool, sticky cache.\n\nin workeroptions you can customize things like polleroptions, activities per second.",charsets:{}},{title:"Signals",frontmatter:{layout:"default",title:"Signals",permalink:"/docs/java-client/signals",readingShow:"top"},regularPath:"/docs/04-java-client/10-signals.html",relativePath:"docs/04-java-client/10-signals.md",key:"v-3e031ed8",path:"/docs/java-client/signals/",headers:[{level:2,title:"Implement Signal Handler in Workflow",slug:"implement-signal-handler-in-workflow",normalizedTitle:"implement signal handler in workflow",charIndex:1012},{level:2,title:"Signal From Command Line",slug:"signal-from-command-line",normalizedTitle:"signal from command line",charIndex:2492},{level:2,title:"SignalWithStart From Command Line",slug:"signalwithstart-from-command-line",normalizedTitle:"signalwithstart from command line",charIndex:6180},{level:2,title:"Signal from user/applicaiton code",slug:"signal-from-user-applicaiton-code",normalizedTitle:"signal from user/applicaiton code",charIndex:6846}],headersStr:"Implement Signal Handler in Workflow Signal From Command Line SignalWithStart From Command Line Signal from user/applicaiton code",content:'# Signals\n provide a mechanism to send data directly to a running . Previously, you had two options for passing data to the implementation:\n\n * Via start parameters\n * As return values from \n\nWith start parameters, we could only pass in values before began.\n\nReturn values from allowed us to pass information to a running , but this approach comes with its own complications. One major drawback is reliance on polling. This means that the data needs to be stored in a third-party location until it\'s ready to be picked up by the . Further, the lifecycle of this requires management, and the requires manual restart if it fails before acquiring the data.\n\n, on the other hand, provide a fully asynchronous and durable mechanism for providing data to a running . When a is received for a running , Cadence persists the and the payload in the history. The can then process the at any time afterwards without the risk of losing the information. The also has the option to stop execution by blocking on a channel.\n\n# Implement Signal Handler in Workflow\nSee the below example from sample.\n\npublic interface HelloWorld {\n    @WorkflowMethod\n    void sayHello(String name);\n\n    @SignalMethod\n    void updateGreeting(String greeting);\n}\n\npublic static class HelloWorldImpl implements HelloWorld {\n\n    private String greeting = "Hello";\n\n    @Override\n    public void sayHello(String name) {\n        int count = 0;\n        while (!"Bye".equals(greeting)) {\n            logger.info(++count + ": " + greeting + " " + name + "!");\n            String oldGreeting = greeting;\n            Workflow.await(() -> !Objects.equals(greeting, oldGreeting));\n        }\n        logger.info(++count + ": " + greeting + " " + name + "!");\n    }\n\n    @Override\n    public void updateGreeting(String greeting) {\n        this.greeting = greeting;\n    }\n}\n\n\nThe interface now has a new method annotated with @SignalMethod. It is a callback method that is invoked every time a new of "HelloWorldupdateGreeting" is delivered to a . The interface can have only one @WorkflowMethod which is a main function of the and as many methods as needed.\n\nThe updated implementation demonstrates a few important Cadence concepts. The first is that is stateful and can have fields of any complex type. Another is that the Workflow.await function that blocks until the function it receives as a parameter evaluates to true. The condition is going to be evaluated only on state changes, so it is not a busy wait in traditional sense.\n\n# Signal From Command Line\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start  --workflow_id "HelloSignal" --tasklist HelloWorldTaskList --workflow_type HelloWorld::sayHello --execution_timeout 3600 --input \\"World\\"\nStarted Workflow Id: HelloSignal, run Id: 6fa204cb-f478-469a-9432-78060b83b6cd\n\n\nProgram output:\n\n16:53:56.120 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 1: Hello World!\n\n\nLet\'s send a using \n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloSignal" --name "HelloWorld::updateGreeting" --input \\"Hi\\"\nSignal workflow succeeded.\n\n\nProgram output:\n\n16:53:56.120 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 1: Hello World!\n16:54:57.901 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 2: Hi World!\n\n\nTry sending the same with the same input again. Note that the output doesn\'t change. This happens because the await condition doesn\'t unblock when it sees the same value. But a new greeting unblocks it:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloSignal" --name "HelloWorld::updateGreeting" --input \\"Welcome\\"\nSignal workflow succeeded.\n\n\nProgram output:\n\n16:53:56.120 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 1: Hello World!\n16:54:57.901 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 2: Hi World!\n16:56:24.400 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 3: Welcome World!\n\n\nNow shut down the and send the same again:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloSignal" --name "HelloWorld::updateGreeting" --input \\"Welcome\\"\nSignal workflow succeeded.\n\n\nNote that sending as well as starting does not need a running. The requests are queued inside the Cadence service.\n\nNow bring the back. Note that it doesn\'t log anything besides the standard startup messages. This occurs because it ignores the queued that contains the same input as the current value of greeting. Note that the restart of the didn\'t affect the . It is still blocked on the same line of code as before the failure. This is the most important feature of Cadence. The code doesn\'t need to deal with failures at all. Its state is fully recovered to its current state that includes all the local variables and threads.\n\nLet\'s look at the line where the is blocked:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow stack --workflow_id "Hello2"\nQuery result:\n"workflow-root: (BLOCKED on await)\ncom.uber.cadence.internal.sync.SyncDecisionContext.await(SyncDecisionContext.java:546)\ncom.uber.cadence.internal.sync.WorkflowInternal.await(WorkflowInternal.java:243)\ncom.uber.cadence.workflow.Workflow.await(Workflow.java:611)\ncom.uber.cadence.samples.hello.GettingStarted$HelloWorldImpl.sayHello(GettingStarted.java:32)\nsun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)"\n\n\nYes, indeed the is blocked on await. This feature works for any open , greatly simplifying troubleshooting in production. Let\'s complete the by sending a with a "Bye" greeting:\n\n16:58:22.962 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 4: Bye World!\n\n\nNote that the value of the count variable was not lost during the restart.\n\nAlso note that while a single instance is used for this walkthrough, any real production deployment has multiple instances running. So any failure or restart does not delay any because it is just migrated to any other available .\n\n# SignalWithStart From Command Line\nYou may not know if a is running and can accept a . The signalWithStart feature allows you to send a to the current instance if one exists or to create a new run and then send the . SignalWithStartWorkflow therefore doesn\'t take a as a parameter.\n\nLearn more from the --help manual:\n\ndocker run --network=host --rm ubercadence/cli:master --do test-domain workflow signalwithstart -h\nNAME:\n   cadence workflow signalwithstart - signal the current open workflow if exists, or attempt to start a new run based on IDResuePolicy and signals it\n\nUSAGE:\n   cadence workflow signalwithstart [command options] [arguments...]\n...\n...\n...\n\n\n# Signal from user/applicaiton code\nYou may want to signal workflows without running the command line.\n\nTheWorkflowClient API allows you to send signal (or SignalWithStartWorkflow) from outside of the workflow to send a to the current .\n\nNote that when using newWorkflowStub to signal a workflow, you MUST NOT passing WorkflowOptions.\n\nThe WorkflowStub with WorkflowOptions is only for starting workflows.\n\nThe WorkflowStub without WorkflowOptions is for signal or query',normalizedContent:'# signals\n provide a mechanism to send data directly to a running . previously, you had two options for passing data to the implementation:\n\n * via start parameters\n * as return values from \n\nwith start parameters, we could only pass in values before began.\n\nreturn values from allowed us to pass information to a running , but this approach comes with its own complications. one major drawback is reliance on polling. this means that the data needs to be stored in a third-party location until it\'s ready to be picked up by the . further, the lifecycle of this requires management, and the requires manual restart if it fails before acquiring the data.\n\n, on the other hand, provide a fully asynchronous and durable mechanism for providing data to a running . when a is received for a running , cadence persists the and the payload in the history. the can then process the at any time afterwards without the risk of losing the information. the also has the option to stop execution by blocking on a channel.\n\n# implement signal handler in workflow\nsee the below example from sample.\n\npublic interface helloworld {\n    @workflowmethod\n    void sayhello(string name);\n\n    @signalmethod\n    void updategreeting(string greeting);\n}\n\npublic static class helloworldimpl implements helloworld {\n\n    private string greeting = "hello";\n\n    @override\n    public void sayhello(string name) {\n        int count = 0;\n        while (!"bye".equals(greeting)) {\n            logger.info(++count + ": " + greeting + " " + name + "!");\n            string oldgreeting = greeting;\n            workflow.await(() -> !objects.equals(greeting, oldgreeting));\n        }\n        logger.info(++count + ": " + greeting + " " + name + "!");\n    }\n\n    @override\n    public void updategreeting(string greeting) {\n        this.greeting = greeting;\n    }\n}\n\n\nthe interface now has a new method annotated with @signalmethod. it is a callback method that is invoked every time a new of "helloworldupdategreeting" is delivered to a . the interface can have only one @workflowmethod which is a main function of the and as many methods as needed.\n\nthe updated implementation demonstrates a few important cadence concepts. the first is that is stateful and can have fields of any complex type. another is that the workflow.await function that blocks until the function it receives as a parameter evaluates to true. the condition is going to be evaluated only on state changes, so it is not a busy wait in traditional sense.\n\n# signal from command line\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start  --workflow_id "hellosignal" --tasklist helloworldtasklist --workflow_type helloworld::sayhello --execution_timeout 3600 --input \\"world\\"\nstarted workflow id: hellosignal, run id: 6fa204cb-f478-469a-9432-78060b83b6cd\n\n\nprogram output:\n\n16:53:56.120 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 1: hello world!\n\n\nlet\'s send a using \n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "hellosignal" --name "helloworld::updategreeting" --input \\"hi\\"\nsignal workflow succeeded.\n\n\nprogram output:\n\n16:53:56.120 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 1: hello world!\n16:54:57.901 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 2: hi world!\n\n\ntry sending the same with the same input again. note that the output doesn\'t change. this happens because the await condition doesn\'t unblock when it sees the same value. but a new greeting unblocks it:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "hellosignal" --name "helloworld::updategreeting" --input \\"welcome\\"\nsignal workflow succeeded.\n\n\nprogram output:\n\n16:53:56.120 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 1: hello world!\n16:54:57.901 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 2: hi world!\n16:56:24.400 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 3: welcome world!\n\n\nnow shut down the and send the same again:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "hellosignal" --name "helloworld::updategreeting" --input \\"welcome\\"\nsignal workflow succeeded.\n\n\nnote that sending as well as starting does not need a running. the requests are queued inside the cadence service.\n\nnow bring the back. note that it doesn\'t log anything besides the standard startup messages. this occurs because it ignores the queued that contains the same input as the current value of greeting. note that the restart of the didn\'t affect the . it is still blocked on the same line of code as before the failure. this is the most important feature of cadence. the code doesn\'t need to deal with failures at all. its state is fully recovered to its current state that includes all the local variables and threads.\n\nlet\'s look at the line where the is blocked:\n\n> docker run --network=host --rm ubercadence/cli:master --do test-domain workflow stack --workflow_id "hello2"\nquery result:\n"workflow-root: (blocked on await)\ncom.uber.cadence.internal.sync.syncdecisioncontext.await(syncdecisioncontext.java:546)\ncom.uber.cadence.internal.sync.workflowinternal.await(workflowinternal.java:243)\ncom.uber.cadence.workflow.workflow.await(workflow.java:611)\ncom.uber.cadence.samples.hello.gettingstarted$helloworldimpl.sayhello(gettingstarted.java:32)\nsun.reflect.nativemethodaccessorimpl.invoke0(native method)\nsun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)"\n\n\nyes, indeed the is blocked on await. this feature works for any open , greatly simplifying troubleshooting in production. let\'s complete the by sending a with a "bye" greeting:\n\n16:58:22.962 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 4: bye world!\n\n\nnote that the value of the count variable was not lost during the restart.\n\nalso note that while a single instance is used for this walkthrough, any real production deployment has multiple instances running. so any failure or restart does not delay any because it is just migrated to any other available .\n\n# signalwithstart from command line\nyou may not know if a is running and can accept a . the signalwithstart feature allows you to send a to the current instance if one exists or to create a new run and then send the . signalwithstartworkflow therefore doesn\'t take a as a parameter.\n\nlearn more from the --help manual:\n\ndocker run --network=host --rm ubercadence/cli:master --do test-domain workflow signalwithstart -h\nname:\n   cadence workflow signalwithstart - signal the current open workflow if exists, or attempt to start a new run based on idresuepolicy and signals it\n\nusage:\n   cadence workflow signalwithstart [command options] [arguments...]\n...\n...\n...\n\n\n# signal from user/applicaiton code\nyou may want to signal workflows without running the command line.\n\ntheworkflowclient api allows you to send signal (or signalwithstartworkflow) from outside of the workflow to send a to the current .\n\nnote that when using newworkflowstub to signal a workflow, you must not passing workflowoptions.\n\nthe workflowstub with workflowoptions is only for starting workflows.\n\nthe workflowstub without workflowoptions is for signal or query',charsets:{}},{title:"Queries",frontmatter:{layout:"default",title:"Queries",permalink:"/docs/java-client/queries",readingShow:"top"},regularPath:"/docs/04-java-client/11-queries.html",relativePath:"docs/04-java-client/11-queries.md",key:"v-8e66acc0",path:"/docs/java-client/queries/",headers:[{level:2,title:"Built-in Query: Stack Trace",slug:"built-in-query-stack-trace",normalizedTitle:"built-in query: stack trace",charIndex:549},{level:2,title:"Customized Query",slug:"customized-query",normalizedTitle:"customized query",charIndex:1051},{level:2,title:"Run Query from Command Line",slug:"run-query-from-command-line",normalizedTitle:"run query from command line",charIndex:2682},{level:2,title:"Run Query from external application code",slug:"run-query-from-external-application-code",normalizedTitle:"run query from external application code",charIndex:4686},{level:2,title:"Consistent Query",slug:"consistent-query",normalizedTitle:"consistent query",charIndex:4794}],headersStr:"Built-in Query: Stack Trace Customized Query Run Query from Command Line Run Query from external application code Consistent Query",content:'# Queries\nQuery is to expose this internal state to the external world Cadence provides a synchronous feature. From the implementer point of view the is exposed as a synchronous callback that is invoked by external entities. Multiple such callbacks can be provided per type exposing different information to different external systems.\n\n callbacks must be read-only not mutating the state in any way. The other limitation is that the callback cannot contain any blocking code. Both above limitations rule out ability to invoke from the handlers.\n\n# Built-in Query: Stack Trace\nIf a has been stuck at a state for longer than an expected period of time, you might want to the current call stack. You can use the Cadence to perform this . For example:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt __stack_trace\n\nThis command uses __stack_trace, which is a built-in type supported by the Cadence client library. You can add custom types to handle such as the current state of a, or how many the has completed.\n\n# Customized Query\nCadence provides a feature that supports synchronously returning any information from a to an external caller.\n\nInterface QueryMethod indicates that the method is a query method. Query method can be used to query a workflow state by external process at any time during its execution. This annotation applies only to workflow interface methods.\n\nSee the example code :\n\npublic interface HelloWorld {\n    @WorkflowMethod\n    void sayHello(String name);\n\n    @SignalMethod\n    void updateGreeting(String greeting);\n\n    @QueryMethod\n    int getCount();\n}\n\npublic static class HelloWorldImpl implements HelloWorld {\n\n    private String greeting = "Hello";\n    private int count = 0;\n\n    @Override\n    public void sayHello(String name) {\n        while (!"Bye".equals(greeting)) {\n            logger.info(++count + ": " + greeting + " " + name + "!");\n            String oldGreeting = greeting;\n            Workflow.await(() -> !Objects.equals(greeting, oldGreeting));\n        }\n        logger.info(++count + ": " + greeting + " " + name + "!");\n    }\n\n    @Override\n    public void updateGreeting(String greeting) {\n        this.greeting = greeting;\n    }\n\n    @Override\n    public int getCount() {\n        return count;\n    }\n}\n\n\nThe new getCount method annotated with @QueryMethod was added to the interface definition. It is allowed to have multiple methods per interface.\n\nThe main restriction on the implementation of the method is that it is not allowed to modify state in any form. It also is not allowed to block its thread in any way. It usually just returns a value derived from the fields of the object.\n\n# Run Query from Command Line\nLet\'s run the updated and send a couple to it:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start  --workflow_id "HelloQuery" --tasklist HelloWorldTaskList --workflow_type HelloWorld::sayHello --execution_timeout 3600 --input \\"World\\"\nStarted Workflow Id: HelloQuery, run Id: 1925f668-45b5-4405-8cba-74f7c68c3135\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloQuery" --name "HelloWorld::updateGreeting" --input \\"Hi\\"\nSignal workflow succeeded.\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloQuery" --name "HelloWorld::updateGreeting" --input \\"Welcome\\"\nSignal workflow succeeded.\n\n\nThe output:\n\n17:35:50.485 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 1: Hello World!\n17:36:10.483 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 2: Hi World!\n17:36:16.204 [workflow-root] INFO  c.u.c.samples.hello.GettingStarted - 3: Welcome World!\n\n\nNow let\'s the using the \n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow query --workflow_id "HelloQuery" --query_type "HelloWorld::getCount"\n:query:Query: result as JSON:\n3\n\n\nOne limitation of the is that it requires a process running because it is executing callback code. An interesting feature of the is that it works for completed as well. Let\'s complete the by sending "Bye" and it.\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "HelloQuery" --name "HelloWorld::updateGreeting" --input \\"Bye\\"\nSignal workflow succeeded.\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow query --workflow_id "HelloQuery" --query_type "HelloWorld::getCount"\n:query:Query: result as JSON:\n4\n\n\nThe method can accept parameters. This might be useful if only part of the state should be returned.\n\n# Run Query from external application code\nThe WorkflowStub without WorkflowOptions is for signal or query\n\n# Consistent Query\n has two consistency levels, eventual and strong. Consider if you were to a and then immediately the \n\ncadence-cli --domain samples-domain workflow signal -w my_workflow_id -r my_run_id -n signal_name -if ./input.json\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nIn this example if were to change state, may or may not see that state update reflected in the result. This is what it means for to be eventually consistent.\n\n has another consistency level called strong consistency. A strongly consistent is guaranteed to be based on state which includes all that came before the was issued. An is considered to have come before a if the call creating the external returned success before the was issued. External which are created while the is outstanding may or may not be reflected in the state the result is based on.\n\nIn order to run consistent through the do the following:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state --qcl strong\n\nIn order to run a using application code, you need to use service client.\n\nWhen using strongly consistent you should expect higher latency than eventually consistent .',normalizedContent:'# queries\nquery is to expose this internal state to the external world cadence provides a synchronous feature. from the implementer point of view the is exposed as a synchronous callback that is invoked by external entities. multiple such callbacks can be provided per type exposing different information to different external systems.\n\n callbacks must be read-only not mutating the state in any way. the other limitation is that the callback cannot contain any blocking code. both above limitations rule out ability to invoke from the handlers.\n\n# built-in query: stack trace\nif a has been stuck at a state for longer than an expected period of time, you might want to the current call stack. you can use the cadence to perform this . for example:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt __stack_trace\n\nthis command uses __stack_trace, which is a built-in type supported by the cadence client library. you can add custom types to handle such as the current state of a, or how many the has completed.\n\n# customized query\ncadence provides a feature that supports synchronously returning any information from a to an external caller.\n\ninterface querymethod indicates that the method is a query method. query method can be used to query a workflow state by external process at any time during its execution. this annotation applies only to workflow interface methods.\n\nsee the example code :\n\npublic interface helloworld {\n    @workflowmethod\n    void sayhello(string name);\n\n    @signalmethod\n    void updategreeting(string greeting);\n\n    @querymethod\n    int getcount();\n}\n\npublic static class helloworldimpl implements helloworld {\n\n    private string greeting = "hello";\n    private int count = 0;\n\n    @override\n    public void sayhello(string name) {\n        while (!"bye".equals(greeting)) {\n            logger.info(++count + ": " + greeting + " " + name + "!");\n            string oldgreeting = greeting;\n            workflow.await(() -> !objects.equals(greeting, oldgreeting));\n        }\n        logger.info(++count + ": " + greeting + " " + name + "!");\n    }\n\n    @override\n    public void updategreeting(string greeting) {\n        this.greeting = greeting;\n    }\n\n    @override\n    public int getcount() {\n        return count;\n    }\n}\n\n\nthe new getcount method annotated with @querymethod was added to the interface definition. it is allowed to have multiple methods per interface.\n\nthe main restriction on the implementation of the method is that it is not allowed to modify state in any form. it also is not allowed to block its thread in any way. it usually just returns a value derived from the fields of the object.\n\n# run query from command line\nlet\'s run the updated and send a couple to it:\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow start  --workflow_id "helloquery" --tasklist helloworldtasklist --workflow_type helloworld::sayhello --execution_timeout 3600 --input \\"world\\"\nstarted workflow id: helloquery, run id: 1925f668-45b5-4405-8cba-74f7c68c3135\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "helloquery" --name "helloworld::updategreeting" --input \\"hi\\"\nsignal workflow succeeded.\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "helloquery" --name "helloworld::updategreeting" --input \\"welcome\\"\nsignal workflow succeeded.\n\n\nthe output:\n\n17:35:50.485 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 1: hello world!\n17:36:10.483 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 2: hi world!\n17:36:16.204 [workflow-root] info  c.u.c.samples.hello.gettingstarted - 3: welcome world!\n\n\nnow let\'s the using the \n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow query --workflow_id "helloquery" --query_type "helloworld::getcount"\n:query:query: result as json:\n3\n\n\none limitation of the is that it requires a process running because it is executing callback code. an interesting feature of the is that it works for completed as well. let\'s complete the by sending "bye" and it.\n\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow signal --workflow_id "helloquery" --name "helloworld::updategreeting" --input \\"bye\\"\nsignal workflow succeeded.\ncadence: docker run --network=host --rm ubercadence/cli:master --do test-domain workflow query --workflow_id "helloquery" --query_type "helloworld::getcount"\n:query:query: result as json:\n4\n\n\nthe method can accept parameters. this might be useful if only part of the state should be returned.\n\n# run query from external application code\nthe workflowstub without workflowoptions is for signal or query\n\n# consistent query\n has two consistency levels, eventual and strong. consider if you were to a and then immediately the \n\ncadence-cli --domain samples-domain workflow signal -w my_workflow_id -r my_run_id -n signal_name -if ./input.json\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nin this example if were to change state, may or may not see that state update reflected in the result. this is what it means for to be eventually consistent.\n\n has another consistency level called strong consistency. a strongly consistent is guaranteed to be based on state which includes all that came before the was issued. an is considered to have come before a if the call creating the external returned success before the was issued. external which are created while the is outstanding may or may not be reflected in the state the result is based on.\n\nin order to run consistent through the do the following:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state --qcl strong\n\nin order to run a using application code, you need to use service client.\n\nwhen using strongly consistent you should expect higher latency than eventually consistent .',charsets:{}},{title:"Retries",frontmatter:{layout:"default",title:"Retries",permalink:"/docs/java-client/retries",readingShow:"top"},regularPath:"/docs/04-java-client/12-retries.html",relativePath:"docs/04-java-client/12-retries.md",key:"v-596ec15e",path:"/docs/java-client/retries/",headers:[{level:2,title:"RetryOptions",slug:"retryoptions",normalizedTitle:"retryoptions",charIndex:282},{level:3,title:"InitialInterval",slug:"initialinterval",normalizedTitle:"initialinterval",charIndex:337},{level:3,title:"BackoffCoefficient",slug:"backoffcoefficient",normalizedTitle:"backoffcoefficient",charIndex:477},{level:3,title:"MaximumInterval",slug:"maximuminterval",normalizedTitle:"maximuminterval",charIndex:676},{level:3,title:"ExpirationInterval",slug:"expirationinterval",normalizedTitle:"expirationinterval",charIndex:861},{level:3,title:"MaximumAttempts",slug:"maximumattempts",normalizedTitle:"maximumattempts",charIndex:932},{level:3,title:"NonRetriableErrorReasons(via setDoNotRetry)",slug:"nonretriableerrorreasons-via-setdonotretry",normalizedTitle:"nonretriableerrorreasons(via setdonotretry)",charIndex:1286},{level:2,title:"Activity Timeout Usage",slug:"activity-timeout-usage",normalizedTitle:"activity timeout usage",charIndex:1931},{level:2,title:"Activity Timeout Internals",slug:"activity-timeout-internals",normalizedTitle:"activity timeout internals",charIndex:3282},{level:3,title:"Basics without Retry",slug:"basics-without-retry",normalizedTitle:"basics without retry",charIndex:3311},{level:3,title:"Heartbeat timeout",slug:"heartbeat-timeout",normalizedTitle:"heartbeat timeout",charIndex:2336},{level:3,title:"RetryOptions and Activity with Retry",slug:"retryoptions-and-activity-with-retry",normalizedTitle:"retryoptions and activity with retry",charIndex:5993}],headersStr:"RetryOptions InitialInterval BackoffCoefficient MaximumInterval ExpirationInterval MaximumAttempts NonRetriableErrorReasons(via setDoNotRetry) Activity Timeout Usage Activity Timeout Internals Basics without Retry Heartbeat timeout RetryOptions and Activity with Retry",content:"# Activity and workflow retries\n and can fail due to various intermediate conditions. In those cases, we want to retry the failed or child or even the parent . This can be achieved by supplying an optional retry options.\n\n> Note that sometimes it's also referred as RetryPolicy\n\n\n# RetryOptions\nA RetryOptions includes the following.\n\n# InitialInterval\nBackoff interval for the first retry. If coefficient is 1.0 then it is used for all retries. Required, no default value.\n\n# BackoffCoefficient\nCoefficient used to calculate the next retry backoff interval. The next retry interval is previous interval multiplied by this coefficient. Must be 1 or larger. Default is 2.0.\n\n# MaximumInterval\nMaximum backoff interval between retries. Exponential backoff leads to interval increase. This value is the cap of the interval. Default is 100x of initial interval.\n\n# ExpirationInterval\nMaximum time to retry. Either ExpirationInterval or MaximumAttempts is required. When exceeded the retries stop even if maximum retries is not reached yet.\n\n# MaximumAttempts\nMaximum number of attempts. When exceeded the retries stop even if not expired yet. If not set or set to 0, it means unlimited, and relies on ExpirationInterval to stop. Either MaximumAttempts or ExpirationInterval is required.\n\n# NonRetriableErrorReasons(via setDoNotRetry)\nNon-Retriable errors. This is optional. Cadence server will stop retry if error reason matches this list. When matching an exact match is used. So adding RuntimeException.class to this list is going to include only RuntimeException itself, not all of its subclasses. The reason for such behaviour is to be able to support server side retries without knowledge of Java exception hierarchy. When considering an exception type a cause of ActivityFailureException and ChildWorkflowFailureException is looked at. Error and CancellationException are never retried and are not even passed to this filter.\n\n# Activity Timeout Usage\nIt's probably too complicated to learn how to set those timeouts by reading the above. There is an easy way to deal with it.\n\nLocalActivity without retry: Use ScheduleToClose for overall timeout\n\nRegular Activity without retry:\n\n 1. Use ScheduleToClose for overall timeout\n 2. Leave ScheduleToStart and StartToClose empty\n 3. If ScheduleToClose is too large(like 10 mins), then set Heartbeat timeout to a smaller value like 10s. Call heartbeat API inside activity regularly.\n\nLocalActivity with retry:\n\n 1. Use ScheduleToClose as timeout of each attempt.\n 2. Use retryOptions.InitialInterval, retryOptions.BackoffCoefficient, retryOptions.MaximumInterval to control backoff.\n 3. Use retryOptions.ExperiationInterval as overall timeout of all attempts.\n 4. Leave retryOptions.MaximumAttempts empty.\n\nRegular Activity with retry:\n\n 1. Use ScheduleToClose as timeout of each attempt\n 2. Leave ScheduleToStart and StartToClose empty\n 3. If ScheduleToClose is too large(like 10 mins), then set Heartbeat timeout to a smaller value like 10s. Call heartbeat API inside activity regularly.\n 4. Use retryOptions.InitialInterval, retryOptions.BackoffCoefficient, retryOptions.MaximumInterval to control backoff.\n 5. Use retryOptions.ExperiationInterval as overall timeout of all attempts.\n 6. Leave retryOptions.MaximumAttempts empty.\n\n# Activity Timeout Internals\n# Basics without Retry\nThings are easier to understand in the world without retry. Because Cadence started from it.\n\n * ScheduleToClose timeout is the overall end-to-end timeout from a workflow's perspective.\n   \n   \n * ScheduleToStart timeout is the time that activity worker needed to start an activity. Exceeding this timeout, activity will return an ScheduleToStart timeout error/exception to workflow\n   \n   \n * StartToClose timeout is the time that an activity needed to run. Exceeding this will return StartToClose to workflow.\n   \n   \n * Requirement and defaults:\n   \n    * Either ScheduleToClose is provided or both of ScheduleToStart and StartToClose are provided.\n    * If only ScheduleToClose, then ScheduleToStart and StartToClose are default to it.\n    * If only ScheduleToStart and StartToClose are provided, then ScheduleToClose = ScheduleToStart + StartToClose.\n    * All of them are capped by workflowTimeout. (e.g. if workflowTimeout is 1hour, set 2 hour for ScheduleToClose will still get 1 hour :ScheduleToClose=Min(ScheduleToClose, workflowTimeout) )\n   \n   \n\nSo why are they?\n\nYou may notice that ScheduleToClose is only useful whenScheduleToClose < ScheduleToStart + StartToClose. Because if ScheduleToClose >= ScheduleToStart+StartToClose the ScheduleToClose timeout is already enforced by the combination of the other two, and it become meaningless.\n\nSo the main use case of ScheduleToClose being less than the sum of two is that people want to limit the overall timeout of the activity but give more timeout for scheduleToStart or startToClose. This is extremely rare use case.\n\nAlso the main use case that people want to distinguish ScheduleToStart and StartToClose is that the workflow may need to do some special handling for ScheduleToStart timeout error. This is also very rare use case.\n\nTherefore, you can understand why in TL;DR that I recommend only using ScheduleToClose but leave the other two empty. Because only in some rare cases you may need it. If you can't think of the use case, then you do not need it.\n\nLocalActivity doesn't have ScheduleToStart/StartToClose because it's started directly inside workflow worker without server scheduling involved.\n\n# Heartbeat timeout\nHeartbeat is very important for long running activity, to prevent it from getting stuck. Not only bugs can cause activity getting stuck, regular deployment/host restart/failure could also cause it. Because without heartbeat, Cadence server couldn't know whether or not the activity is still being worked on. See more details about here https://stackoverflow.com/questions/65118584/solutions-to-stuck-timers-activities-in-cadence-swf-stepfunctions/65118585#65118585\n\n# RetryOptions and Activity with Retry\nFirst of all, here RetryOptions is for server side backoff retry -- meaning that the retry is managed automatically by Cadence without interacting with workflows. Because retry is managed by Cadence, the activity has to be specially handled in Cadence history that the started event can not written until the activity is closed. Here is some reference: https://stackoverflow.com/questions/65113363/why-an-activity-task-is-scheduled-but-not-started/65113365#65113365\n\nIn fact, workflow can do client side retry on their own. This means workflow will be managing the retry logic. You can write your own retry function, or there is some helper function in SDK, like Workflow.retry in Cadence-java-client. Client side retry will show all start events immediately, but there will be many events in the history when retrying for a single activity. It's not recommended because of performance issue.\n\nSo what do the options mean:\n\n * ExpirationInterval:\n   \n    * It replaces the ScheduleToClose timeout to become the actual overall timeout of the activity for all attempts.\n    * It's also capped to workflow timeout like other three timeout options. ScheduleToClose = Min(ScheduleToClose, workflowTimeout)\n    * The timeout of each attempt is StartToClose, but StartToClose defaults to ScheduleToClose like explanation above.\n    * ScheduleToClose will be extended to ExpirationInterval:ScheduleToClose = Max(ScheduleToClose, ExpirationInterval), and this happens before ScheduleToClose is copied to ScheduleToClose and StartToClose.\n   \n   \n * InitialInterval: the interval of first retry\n   \n   \n * BackoffCoefficient: self explained\n   \n   \n * MaximumInterval: maximum of the interval during retry\n   \n   \n * MaximumAttempts: the maximum attempts. If existing with ExpirationInterval, then retry stops when either one of them is exceeded.\n   \n   \n * Requirements and defaults:\n   \n   \n * Either MaximumAttempts or ExpirationInterval is required. ExpirationInterval is set to workflowTimeout if not provided.\n   \n   \n\nSince ExpirationInterval is always there, and in fact it's more useful. And I think it's quite confusing to use MaximumAttempts, so I would recommend just use ExpirationInterval. Unless you really need it.",normalizedContent:"# activity and workflow retries\n and can fail due to various intermediate conditions. in those cases, we want to retry the failed or child or even the parent . this can be achieved by supplying an optional retry options.\n\n> note that sometimes it's also referred as retrypolicy\n\n\n# retryoptions\na retryoptions includes the following.\n\n# initialinterval\nbackoff interval for the first retry. if coefficient is 1.0 then it is used for all retries. required, no default value.\n\n# backoffcoefficient\ncoefficient used to calculate the next retry backoff interval. the next retry interval is previous interval multiplied by this coefficient. must be 1 or larger. default is 2.0.\n\n# maximuminterval\nmaximum backoff interval between retries. exponential backoff leads to interval increase. this value is the cap of the interval. default is 100x of initial interval.\n\n# expirationinterval\nmaximum time to retry. either expirationinterval or maximumattempts is required. when exceeded the retries stop even if maximum retries is not reached yet.\n\n# maximumattempts\nmaximum number of attempts. when exceeded the retries stop even if not expired yet. if not set or set to 0, it means unlimited, and relies on expirationinterval to stop. either maximumattempts or expirationinterval is required.\n\n# nonretriableerrorreasons(via setdonotretry)\nnon-retriable errors. this is optional. cadence server will stop retry if error reason matches this list. when matching an exact match is used. so adding runtimeexception.class to this list is going to include only runtimeexception itself, not all of its subclasses. the reason for such behaviour is to be able to support server side retries without knowledge of java exception hierarchy. when considering an exception type a cause of activityfailureexception and childworkflowfailureexception is looked at. error and cancellationexception are never retried and are not even passed to this filter.\n\n# activity timeout usage\nit's probably too complicated to learn how to set those timeouts by reading the above. there is an easy way to deal with it.\n\nlocalactivity without retry: use scheduletoclose for overall timeout\n\nregular activity without retry:\n\n 1. use scheduletoclose for overall timeout\n 2. leave scheduletostart and starttoclose empty\n 3. if scheduletoclose is too large(like 10 mins), then set heartbeat timeout to a smaller value like 10s. call heartbeat api inside activity regularly.\n\nlocalactivity with retry:\n\n 1. use scheduletoclose as timeout of each attempt.\n 2. use retryoptions.initialinterval, retryoptions.backoffcoefficient, retryoptions.maximuminterval to control backoff.\n 3. use retryoptions.experiationinterval as overall timeout of all attempts.\n 4. leave retryoptions.maximumattempts empty.\n\nregular activity with retry:\n\n 1. use scheduletoclose as timeout of each attempt\n 2. leave scheduletostart and starttoclose empty\n 3. if scheduletoclose is too large(like 10 mins), then set heartbeat timeout to a smaller value like 10s. call heartbeat api inside activity regularly.\n 4. use retryoptions.initialinterval, retryoptions.backoffcoefficient, retryoptions.maximuminterval to control backoff.\n 5. use retryoptions.experiationinterval as overall timeout of all attempts.\n 6. leave retryoptions.maximumattempts empty.\n\n# activity timeout internals\n# basics without retry\nthings are easier to understand in the world without retry. because cadence started from it.\n\n * scheduletoclose timeout is the overall end-to-end timeout from a workflow's perspective.\n   \n   \n * scheduletostart timeout is the time that activity worker needed to start an activity. exceeding this timeout, activity will return an scheduletostart timeout error/exception to workflow\n   \n   \n * starttoclose timeout is the time that an activity needed to run. exceeding this will return starttoclose to workflow.\n   \n   \n * requirement and defaults:\n   \n    * either scheduletoclose is provided or both of scheduletostart and starttoclose are provided.\n    * if only scheduletoclose, then scheduletostart and starttoclose are default to it.\n    * if only scheduletostart and starttoclose are provided, then scheduletoclose = scheduletostart + starttoclose.\n    * all of them are capped by workflowtimeout. (e.g. if workflowtimeout is 1hour, set 2 hour for scheduletoclose will still get 1 hour :scheduletoclose=min(scheduletoclose, workflowtimeout) )\n   \n   \n\nso why are they?\n\nyou may notice that scheduletoclose is only useful whenscheduletoclose < scheduletostart + starttoclose. because if scheduletoclose >= scheduletostart+starttoclose the scheduletoclose timeout is already enforced by the combination of the other two, and it become meaningless.\n\nso the main use case of scheduletoclose being less than the sum of two is that people want to limit the overall timeout of the activity but give more timeout for scheduletostart or starttoclose. this is extremely rare use case.\n\nalso the main use case that people want to distinguish scheduletostart and starttoclose is that the workflow may need to do some special handling for scheduletostart timeout error. this is also very rare use case.\n\ntherefore, you can understand why in tl;dr that i recommend only using scheduletoclose but leave the other two empty. because only in some rare cases you may need it. if you can't think of the use case, then you do not need it.\n\nlocalactivity doesn't have scheduletostart/starttoclose because it's started directly inside workflow worker without server scheduling involved.\n\n# heartbeat timeout\nheartbeat is very important for long running activity, to prevent it from getting stuck. not only bugs can cause activity getting stuck, regular deployment/host restart/failure could also cause it. because without heartbeat, cadence server couldn't know whether or not the activity is still being worked on. see more details about here https://stackoverflow.com/questions/65118584/solutions-to-stuck-timers-activities-in-cadence-swf-stepfunctions/65118585#65118585\n\n# retryoptions and activity with retry\nfirst of all, here retryoptions is for server side backoff retry -- meaning that the retry is managed automatically by cadence without interacting with workflows. because retry is managed by cadence, the activity has to be specially handled in cadence history that the started event can not written until the activity is closed. here is some reference: https://stackoverflow.com/questions/65113363/why-an-activity-task-is-scheduled-but-not-started/65113365#65113365\n\nin fact, workflow can do client side retry on their own. this means workflow will be managing the retry logic. you can write your own retry function, or there is some helper function in sdk, like workflow.retry in cadence-java-client. client side retry will show all start events immediately, but there will be many events in the history when retrying for a single activity. it's not recommended because of performance issue.\n\nso what do the options mean:\n\n * expirationinterval:\n   \n    * it replaces the scheduletoclose timeout to become the actual overall timeout of the activity for all attempts.\n    * it's also capped to workflow timeout like other three timeout options. scheduletoclose = min(scheduletoclose, workflowtimeout)\n    * the timeout of each attempt is starttoclose, but starttoclose defaults to scheduletoclose like explanation above.\n    * scheduletoclose will be extended to expirationinterval:scheduletoclose = max(scheduletoclose, expirationinterval), and this happens before scheduletoclose is copied to scheduletoclose and starttoclose.\n   \n   \n * initialinterval: the interval of first retry\n   \n   \n * backoffcoefficient: self explained\n   \n   \n * maximuminterval: maximum of the interval during retry\n   \n   \n * maximumattempts: the maximum attempts. if existing with expirationinterval, then retry stops when either one of them is exceeded.\n   \n   \n * requirements and defaults:\n   \n   \n * either maximumattempts or expirationinterval is required. expirationinterval is set to workflowtimeout if not provided.\n   \n   \n\nsince expirationinterval is always there, and in fact it's more useful. and i think it's quite confusing to use maximumattempts, so i would recommend just use expirationinterval. unless you really need it.",charsets:{}},{title:"Child workflows",frontmatter:{layout:"default",title:"Child workflows",permalink:"/docs/java-client/child-workflows",readingShow:"top"},regularPath:"/docs/04-java-client/13-child-workflows.html",relativePath:"docs/04-java-client/13-child-workflows.md",key:"v-5fdea0a2",path:"/docs/java-client/child-workflows/",headersStr:null,content:'# Child workflows\nBesides , a can also orchestrate other .\n\nworkflow.ExecuteChildWorkflow enables the scheduling of other from within a \'s implementation. The parent has the ability to monitor and impact the lifecycle of the child, similar to the way it does for an that it invoked.\n\npublic static class GreetingWorkflowImpl implements GreetingWorkflow {\n\n  @Override\n  public String getGreeting(String name) {\n    // Workflows are stateful. So a new stub must be created for each new child.\n    GreetingChild child = Workflow.newChildWorkflowStub(GreetingChild.class);\n\n    // This is a non blocking call that returns immediately.\n    // Use child.composeGreeting("Hello", name) to call synchronously.\n    Promise<String> greeting = Async.function(child::composeGreeting, "Hello", name);\n    // Do something else here.\n    return greeting.get(); // blocks waiting for the child to complete.\n  }\n\n  // This example shows how parent workflow return right after starting a child workflow,\n  // and let the child run itself.\n  private String demoAsyncChildRun(String name) {\n    GreetingChild child = Workflow.newChildWorkflowStub(GreetingChild.class);\n    // non blocking call that initiated child workflow\n    Async.function(child::composeGreeting, "Hello", name);\n    // instead of using greeting.get() to block till child complete,\n    // sometimes we just want to return parent immediately and keep child running\n    Promise<WorkflowExecution> childPromise = Workflow.getWorkflowExecution(child);\n    childPromise.get(); // block until child started,\n    // otherwise child may not start because parent complete first.\n    return "let child run, parent just return";\n  }\n}\n\n\nWorkflow.newChildWorkflowStub returns a client-side stub that implements a child interface. It takes a child type and optional child options as arguments. options may be needed to override the timeouts and if they differ from the ones defined in the @WorkflowMethod annotation or parent .\n\nThe first call to the child stub must always be to a method annotated with @WorkflowMethod. Similar to , a call can be made synchronous or asynchronous by using Async#function or Async#procedure. The synchronous call blocks until a child completes. The asynchronous call returns a Promise that can be used to wait for the completion. After an async call returns the stub, it can be used to send to the child by calling methods annotated with @SignalMethod. a child by calling methods annotated with @QueryMethodfrom within code is not supported. However, can be done from using the provided WorkflowClient stub.\n\nRunning two children in parallel:\n\npublic static class GreetingWorkflowImpl implements GreetingWorkflow {\n\n    @Override\n    public String getGreeting(String name) {\n\n        // Workflows are stateful, so a new stub must be created for each new child.\n        GreetingChild child1 = Workflow.newChildWorkflowStub(GreetingChild.class);\n        Promise<String> greeting1 = Async.function(child1::composeGreeting, "Hello", name);\n\n        // Both children will run concurrently.\n        GreetingChild child2 = Workflow.newChildWorkflowStub(GreetingChild.class);\n        Promise<String> greeting2 = Async.function(child2::composeGreeting, "Bye", name);\n\n        // Do something else here.\n        ...\n        return "First: " + greeting1.get() + ", second: " + greeting2.get();\n    }\n}\n\n\nTo send a to a child, call a method annotated with @SignalMethod:\n\npublic interface GreetingChild {\n    @WorkflowMethod\n    String composeGreeting(String greeting, String name);\n\n    @SignalMethod\n    void updateName(String name);\n}\n\npublic static class GreetingWorkflowImpl implements GreetingWorkflow {\n\n    @Override\n    public String getGreeting(String name) {\n        GreetingChild child = Workflow.newChildWorkflowStub(GreetingChild.class);\n        Promise<String> greeting = Async.function(child::composeGreeting, "Hello", name);\n        child.updateName("Cadence");\n        return greeting.get();\n    }\n}\n\n\nCalling methods annotated with @QueryMethod is not allowed from within code.',normalizedContent:'# child workflows\nbesides , a can also orchestrate other .\n\nworkflow.executechildworkflow enables the scheduling of other from within a \'s implementation. the parent has the ability to monitor and impact the lifecycle of the child, similar to the way it does for an that it invoked.\n\npublic static class greetingworkflowimpl implements greetingworkflow {\n\n  @override\n  public string getgreeting(string name) {\n    // workflows are stateful. so a new stub must be created for each new child.\n    greetingchild child = workflow.newchildworkflowstub(greetingchild.class);\n\n    // this is a non blocking call that returns immediately.\n    // use child.composegreeting("hello", name) to call synchronously.\n    promise<string> greeting = async.function(child::composegreeting, "hello", name);\n    // do something else here.\n    return greeting.get(); // blocks waiting for the child to complete.\n  }\n\n  // this example shows how parent workflow return right after starting a child workflow,\n  // and let the child run itself.\n  private string demoasyncchildrun(string name) {\n    greetingchild child = workflow.newchildworkflowstub(greetingchild.class);\n    // non blocking call that initiated child workflow\n    async.function(child::composegreeting, "hello", name);\n    // instead of using greeting.get() to block till child complete,\n    // sometimes we just want to return parent immediately and keep child running\n    promise<workflowexecution> childpromise = workflow.getworkflowexecution(child);\n    childpromise.get(); // block until child started,\n    // otherwise child may not start because parent complete first.\n    return "let child run, parent just return";\n  }\n}\n\n\nworkflow.newchildworkflowstub returns a client-side stub that implements a child interface. it takes a child type and optional child options as arguments. options may be needed to override the timeouts and if they differ from the ones defined in the @workflowmethod annotation or parent .\n\nthe first call to the child stub must always be to a method annotated with @workflowmethod. similar to , a call can be made synchronous or asynchronous by using async#function or async#procedure. the synchronous call blocks until a child completes. the asynchronous call returns a promise that can be used to wait for the completion. after an async call returns the stub, it can be used to send to the child by calling methods annotated with @signalmethod. a child by calling methods annotated with @querymethodfrom within code is not supported. however, can be done from using the provided workflowclient stub.\n\nrunning two children in parallel:\n\npublic static class greetingworkflowimpl implements greetingworkflow {\n\n    @override\n    public string getgreeting(string name) {\n\n        // workflows are stateful, so a new stub must be created for each new child.\n        greetingchild child1 = workflow.newchildworkflowstub(greetingchild.class);\n        promise<string> greeting1 = async.function(child1::composegreeting, "hello", name);\n\n        // both children will run concurrently.\n        greetingchild child2 = workflow.newchildworkflowstub(greetingchild.class);\n        promise<string> greeting2 = async.function(child2::composegreeting, "bye", name);\n\n        // do something else here.\n        ...\n        return "first: " + greeting1.get() + ", second: " + greeting2.get();\n    }\n}\n\n\nto send a to a child, call a method annotated with @signalmethod:\n\npublic interface greetingchild {\n    @workflowmethod\n    string composegreeting(string greeting, string name);\n\n    @signalmethod\n    void updatename(string name);\n}\n\npublic static class greetingworkflowimpl implements greetingworkflow {\n\n    @override\n    public string getgreeting(string name) {\n        greetingchild child = workflow.newchildworkflowstub(greetingchild.class);\n        promise<string> greeting = async.function(child::composegreeting, "hello", name);\n        child.updatename("cadence");\n        return greeting.get();\n    }\n}\n\n\ncalling methods annotated with @querymethod is not allowed from within code.',charsets:{}},{title:"Exception Handling",frontmatter:{layout:"default",title:"Exception Handling",permalink:"/docs/java-client/exception-handling",readingShow:"top"},regularPath:"/docs/04-java-client/14-exception-handling.html",relativePath:"docs/04-java-client/14-exception-handling.md",key:"v-e894b2bc",path:"/docs/java-client/exception-handling/",headersStr:null,content:'# Exception Handling\nBy default, Exceptions thrown by an activity are received by the workflow wrapped into an com.uber.cadence.workflow.ActivityFailureException,\n\nExceptions thrown by a child workflow are received by a parent workflow wrapped into a com.uber.cadence.workflow.ChildWorkflowFailureException\n\nExceptions thrown by a workflow are received by a workflow client wrapped into com.uber.cadence.client.WorkflowFailureException.\n\nIn this example a Workflow Client executes a workflow which executes a child workflow which executes an activity which throws an IOException. The resulting exception stack trace is:\n\n com.uber.cadence.client.WorkflowFailureException: WorkflowType="GreetingWorkflow::getGreeting", WorkflowID="38b9ce7a-e370-4cd8-a9f3-35e7295f7b3d", RunID="37ceb58c-9271-4fca-b5aa-ba06c5495214\n     at com.uber.cadence.internal.dispatcher.UntypedWorkflowStubImpl.getResult(UntypedWorkflowStubImpl.java:139)\n     at com.uber.cadence.internal.dispatcher.UntypedWorkflowStubImpl.getResult(UntypedWorkflowStubImpl.java:111)\n     at com.uber.cadence.internal.dispatcher.WorkflowExternalInvocationHandler.startWorkflow(WorkflowExternalInvocationHandler.java:187)\n     at com.uber.cadence.internal.dispatcher.WorkflowExternalInvocationHandler.invoke(WorkflowExternalInvocationHandler.java:113)\n     at com.sun.proxy.$Proxy2.getGreeting(Unknown Source)\n     at com.uber.cadence.samples.hello.HelloException.main(HelloException.java:117)\n Caused by: com.uber.cadence.workflow.ChildWorkflowFailureException: WorkflowType="GreetingChild::composeGreeting", ID="37ceb58c-9271-4fca-b5aa-ba06c5495214:1", RunID="47859b47-da4c-4225-876a-462421c98c72, EventID=10\n     at java.lang.Thread.getStackTrace(Thread.java:1559)\n     at com.uber.cadence.internal.dispatcher.ChildWorkflowInvocationHandler.executeChildWorkflow(ChildWorkflowInvocationHandler.java:114)\n     at com.uber.cadence.internal.dispatcher.ChildWorkflowInvocationHandler.invoke(ChildWorkflowInvocationHandler.java:71)\n     at com.sun.proxy.$Proxy5.composeGreeting(Unknown Source:0)\n     at com.uber.cadence.samples.hello.HelloException$GreetingWorkflowImpl.getGreeting(HelloException.java:70)\n     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method:0)\n     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n     at java.lang.reflect.Method.invoke(Method.java:498)\n     at com.uber.cadence.internal.worker.POJOWorkflowImplementationFactory$POJOWorkflowImplementation.execute(POJOWorkflowImplementationFactory.java:160)\n Caused by: com.uber.cadence.workflow.ActivityFailureException: ActivityType="GreetingActivities::composeGreeting" ActivityID="1", EventID=7\n     at java.lang.Thread.getStackTrace(Thread.java:1559)\n     at com.uber.cadence.internal.dispatcher.ActivityInvocationHandler.invoke(ActivityInvocationHandler.java:75)\n     at com.sun.proxy.$Proxy6.composeGreeting(Unknown Source:0)\n     at com.uber.cadence.samples.hello.HelloException$GreetingChildImpl.composeGreeting(HelloException.java:85)\n     ... 5 more\n Caused by: java.io.IOException: Hello World!\n     at com.uber.cadence.samples.hello.HelloException$GreetingActivitiesImpl.composeGreeting(HelloException.java:93)\n     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method:0)\n     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n     at java.lang.reflect.Method.invoke(Method.java:498)\n     at com.uber.cadence.internal.worker.POJOActivityImplementationFactory$POJOActivityImplementation.execute(POJOActivityImplementationFactory.java:162)\n\n\nNote that IOException is a checked exception. The standard Java way of adding throws IOException to method signature of activity, child and workflow interfaces is not going to help. It is because at all levels it is never received directly, but in wrapped form. Propagating it without wrapping would not allow adding additional context information like activity, child workflow and parent workflow types and IDs. The Cadence library solution is to provide a special wrapper method Workflow.wrap(Exception) which wraps a checked exception in a special runtime exception. It is special because the framework strips it when chaining exceptions across logical process boundaries. In this example IOException is directly attached to ActivityFailureException besides being wrapped when rethrown.\n\npublic class HelloException {\n\n  static final String TASK_LIST = "HelloException";\n\n  public interface GreetingWorkflow {\n    @WorkflowMethod\n    String getGreeting(String name);\n  }\n\n  public interface GreetingChild {\n    @WorkflowMethod\n    String composeGreeting(String greeting, String name);\n  }\n\n  public interface GreetingActivities {\n    String composeGreeting(String greeting, String name);\n  }\n\n  /** Parent implementation that calls GreetingChild#composeGreeting.**/\n  public static class GreetingWorkflowImpl implements GreetingWorkflow {\n\n    @Override\n    public String getGreeting(String name) {\n      GreetingChild child = Workflow.newChildWorkflowStub(GreetingChild.class);\n      return child.composeGreeting("Hello", name);\n    }\n  }\n\n  /** Child workflow implementation.**/\n  public static class GreetingChildImpl implements GreetingChild {\n    private final GreetingActivities activities =\n        Workflow.newActivityStub(\n            GreetingActivities.class,\n            new ActivityOptions.Builder()\n                .setScheduleToCloseTimeout(Duration.ofSeconds(10))\n                .build());\n\n    @Override\n    public String composeGreeting(String greeting, String name) {\n      return activities.composeGreeting(greeting, name);\n    }\n  }\n\n  static class GreetingActivitiesImpl implements GreetingActivities {\n    @Override\n    public String composeGreeting(String greeting, String name) {\n      try {\n        throw new IOException(greeting + " " + name + "!");\n      } catch (IOException e) {\n        // Wrapping the exception as checked exceptions in activity and workflow interface methods\n        // are prohibited.\n        // It will be unwrapped and attached as a cause to the ActivityFailureException.\n        throw Workflow.wrap(e);\n      }\n    }\n  }\n\n  public static void main(String[] args) {\n     // Get a new client\n     // NOTE: to set a different options, you can do like this:\n     // ClientOptions.newBuilder().setRpcTimeout(5 * 1000).build();\n     WorkflowClient workflowClient =\n         WorkflowClient.newInstance(\n             new WorkflowServiceTChannel(ClientOptions.defaultInstance()),\n             WorkflowClientOptions.newBuilder().setDomain(DOMAIN).build());\n     // Get worker to poll the task list.\n     WorkerFactory factory = WorkerFactory.newInstance(workflowClient);\n     Worker worker = factory.newWorker(TASK_LIST);\n     worker.registerWorkflowImplementationTypes(GreetingWorkflowImpl.class, GreetingChildImpl.class);\n     worker.registerActivitiesImplementations(new GreetingActivitiesImpl());\n     factory.start();\n\n     WorkflowOptions workflowOptions =\n         new WorkflowOptions.Builder()\n             .setTaskList(TASK_LIST)\n             .setExecutionStartToCloseTimeout(Duration.ofSeconds(30))\n             .build();\n     GreetingWorkflow workflow =\n         workflowClient.newWorkflowStub(GreetingWorkflow.class, workflowOptions);\n     try {\n       workflow.getGreeting("World");\n       throw new IllegalStateException("unreachable");\n     } catch (WorkflowException e) {\n       Throwable cause = Throwables.getRootCause(e);\n       // prints "Hello World!"\n       System.out.println(cause.getMessage());\n       System.out.println("\\nStack Trace:\\n" + Throwables.getStackTraceAsString(e));\n     }\n     System.exit(0);\n   }\n   \n}\n\n\nThe code is slightly different if you are using client version prior to 3.0.0:\n\npublic static void main(String[] args) {\n  Worker.Factory factory = new Worker.Factory(DOMAIN);\n  Worker worker = factory.newWorker(TASK_LIST);\n  worker.registerWorkflowImplementationTypes(GreetingWorkflowImpl.class, GreetingChildImpl.class);\n  worker.registerActivitiesImplementations(new GreetingActivitiesImpl());\n  factory.start();\n\n  WorkflowClient workflowClient = WorkflowClient.newInstance(DOMAIN);\n  WorkflowOptions workflowOptions =\n      new WorkflowOptions.Builder()\n          .setTaskList(TASK_LIST)\n          .setExecutionStartToCloseTimeout(Duration.ofSeconds(30))\n          .build();\n  GreetingWorkflow workflow =\n      workflowClient.newWorkflowStub(GreetingWorkflow.class, workflowOptions);\n  try {\n    workflow.getGreeting("World");\n    throw new IllegalStateException("unreachable");\n  } catch (WorkflowException e) {\n    Throwable cause = Throwables.getRootCause(e);\n    // prints "Hello World!"\n    System.out.println(cause.getMessage());\n    System.out.println("\\nStack Trace:\\n" + Throwables.getStackTraceAsString(e));\n  }\n  System.exit(0);\n}',normalizedContent:'# exception handling\nby default, exceptions thrown by an activity are received by the workflow wrapped into an com.uber.cadence.workflow.activityfailureexception,\n\nexceptions thrown by a child workflow are received by a parent workflow wrapped into a com.uber.cadence.workflow.childworkflowfailureexception\n\nexceptions thrown by a workflow are received by a workflow client wrapped into com.uber.cadence.client.workflowfailureexception.\n\nin this example a workflow client executes a workflow which executes a child workflow which executes an activity which throws an ioexception. the resulting exception stack trace is:\n\n com.uber.cadence.client.workflowfailureexception: workflowtype="greetingworkflow::getgreeting", workflowid="38b9ce7a-e370-4cd8-a9f3-35e7295f7b3d", runid="37ceb58c-9271-4fca-b5aa-ba06c5495214\n     at com.uber.cadence.internal.dispatcher.untypedworkflowstubimpl.getresult(untypedworkflowstubimpl.java:139)\n     at com.uber.cadence.internal.dispatcher.untypedworkflowstubimpl.getresult(untypedworkflowstubimpl.java:111)\n     at com.uber.cadence.internal.dispatcher.workflowexternalinvocationhandler.startworkflow(workflowexternalinvocationhandler.java:187)\n     at com.uber.cadence.internal.dispatcher.workflowexternalinvocationhandler.invoke(workflowexternalinvocationhandler.java:113)\n     at com.sun.proxy.$proxy2.getgreeting(unknown source)\n     at com.uber.cadence.samples.hello.helloexception.main(helloexception.java:117)\n caused by: com.uber.cadence.workflow.childworkflowfailureexception: workflowtype="greetingchild::composegreeting", id="37ceb58c-9271-4fca-b5aa-ba06c5495214:1", runid="47859b47-da4c-4225-876a-462421c98c72, eventid=10\n     at java.lang.thread.getstacktrace(thread.java:1559)\n     at com.uber.cadence.internal.dispatcher.childworkflowinvocationhandler.executechildworkflow(childworkflowinvocationhandler.java:114)\n     at com.uber.cadence.internal.dispatcher.childworkflowinvocationhandler.invoke(childworkflowinvocationhandler.java:71)\n     at com.sun.proxy.$proxy5.composegreeting(unknown source:0)\n     at com.uber.cadence.samples.hello.helloexception$greetingworkflowimpl.getgreeting(helloexception.java:70)\n     at sun.reflect.nativemethodaccessorimpl.invoke0(native method:0)\n     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)\n     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)\n     at java.lang.reflect.method.invoke(method.java:498)\n     at com.uber.cadence.internal.worker.pojoworkflowimplementationfactory$pojoworkflowimplementation.execute(pojoworkflowimplementationfactory.java:160)\n caused by: com.uber.cadence.workflow.activityfailureexception: activitytype="greetingactivities::composegreeting" activityid="1", eventid=7\n     at java.lang.thread.getstacktrace(thread.java:1559)\n     at com.uber.cadence.internal.dispatcher.activityinvocationhandler.invoke(activityinvocationhandler.java:75)\n     at com.sun.proxy.$proxy6.composegreeting(unknown source:0)\n     at com.uber.cadence.samples.hello.helloexception$greetingchildimpl.composegreeting(helloexception.java:85)\n     ... 5 more\n caused by: java.io.ioexception: hello world!\n     at com.uber.cadence.samples.hello.helloexception$greetingactivitiesimpl.composegreeting(helloexception.java:93)\n     at sun.reflect.nativemethodaccessorimpl.invoke0(native method:0)\n     at sun.reflect.nativemethodaccessorimpl.invoke(nativemethodaccessorimpl.java:62)\n     at sun.reflect.delegatingmethodaccessorimpl.invoke(delegatingmethodaccessorimpl.java:43)\n     at java.lang.reflect.method.invoke(method.java:498)\n     at com.uber.cadence.internal.worker.pojoactivityimplementationfactory$pojoactivityimplementation.execute(pojoactivityimplementationfactory.java:162)\n\n\nnote that ioexception is a checked exception. the standard java way of adding throws ioexception to method signature of activity, child and workflow interfaces is not going to help. it is because at all levels it is never received directly, but in wrapped form. propagating it without wrapping would not allow adding additional context information like activity, child workflow and parent workflow types and ids. the cadence library solution is to provide a special wrapper method workflow.wrap(exception) which wraps a checked exception in a special runtime exception. it is special because the framework strips it when chaining exceptions across logical process boundaries. in this example ioexception is directly attached to activityfailureexception besides being wrapped when rethrown.\n\npublic class helloexception {\n\n  static final string task_list = "helloexception";\n\n  public interface greetingworkflow {\n    @workflowmethod\n    string getgreeting(string name);\n  }\n\n  public interface greetingchild {\n    @workflowmethod\n    string composegreeting(string greeting, string name);\n  }\n\n  public interface greetingactivities {\n    string composegreeting(string greeting, string name);\n  }\n\n  /** parent implementation that calls greetingchild#composegreeting.**/\n  public static class greetingworkflowimpl implements greetingworkflow {\n\n    @override\n    public string getgreeting(string name) {\n      greetingchild child = workflow.newchildworkflowstub(greetingchild.class);\n      return child.composegreeting("hello", name);\n    }\n  }\n\n  /** child workflow implementation.**/\n  public static class greetingchildimpl implements greetingchild {\n    private final greetingactivities activities =\n        workflow.newactivitystub(\n            greetingactivities.class,\n            new activityoptions.builder()\n                .setscheduletoclosetimeout(duration.ofseconds(10))\n                .build());\n\n    @override\n    public string composegreeting(string greeting, string name) {\n      return activities.composegreeting(greeting, name);\n    }\n  }\n\n  static class greetingactivitiesimpl implements greetingactivities {\n    @override\n    public string composegreeting(string greeting, string name) {\n      try {\n        throw new ioexception(greeting + " " + name + "!");\n      } catch (ioexception e) {\n        // wrapping the exception as checked exceptions in activity and workflow interface methods\n        // are prohibited.\n        // it will be unwrapped and attached as a cause to the activityfailureexception.\n        throw workflow.wrap(e);\n      }\n    }\n  }\n\n  public static void main(string[] args) {\n     // get a new client\n     // note: to set a different options, you can do like this:\n     // clientoptions.newbuilder().setrpctimeout(5 * 1000).build();\n     workflowclient workflowclient =\n         workflowclient.newinstance(\n             new workflowservicetchannel(clientoptions.defaultinstance()),\n             workflowclientoptions.newbuilder().setdomain(domain).build());\n     // get worker to poll the task list.\n     workerfactory factory = workerfactory.newinstance(workflowclient);\n     worker worker = factory.newworker(task_list);\n     worker.registerworkflowimplementationtypes(greetingworkflowimpl.class, greetingchildimpl.class);\n     worker.registeractivitiesimplementations(new greetingactivitiesimpl());\n     factory.start();\n\n     workflowoptions workflowoptions =\n         new workflowoptions.builder()\n             .settasklist(task_list)\n             .setexecutionstarttoclosetimeout(duration.ofseconds(30))\n             .build();\n     greetingworkflow workflow =\n         workflowclient.newworkflowstub(greetingworkflow.class, workflowoptions);\n     try {\n       workflow.getgreeting("world");\n       throw new illegalstateexception("unreachable");\n     } catch (workflowexception e) {\n       throwable cause = throwables.getrootcause(e);\n       // prints "hello world!"\n       system.out.println(cause.getmessage());\n       system.out.println("\\nstack trace:\\n" + throwables.getstacktraceasstring(e));\n     }\n     system.exit(0);\n   }\n   \n}\n\n\nthe code is slightly different if you are using client version prior to 3.0.0:\n\npublic static void main(string[] args) {\n  worker.factory factory = new worker.factory(domain);\n  worker worker = factory.newworker(task_list);\n  worker.registerworkflowimplementationtypes(greetingworkflowimpl.class, greetingchildimpl.class);\n  worker.registeractivitiesimplementations(new greetingactivitiesimpl());\n  factory.start();\n\n  workflowclient workflowclient = workflowclient.newinstance(domain);\n  workflowoptions workflowoptions =\n      new workflowoptions.builder()\n          .settasklist(task_list)\n          .setexecutionstarttoclosetimeout(duration.ofseconds(30))\n          .build();\n  greetingworkflow workflow =\n      workflowclient.newworkflowstub(greetingworkflow.class, workflowoptions);\n  try {\n    workflow.getgreeting("world");\n    throw new illegalstateexception("unreachable");\n  } catch (workflowexception e) {\n    throwable cause = throwables.getrootcause(e);\n    // prints "hello world!"\n    system.out.println(cause.getmessage());\n    system.out.println("\\nstack trace:\\n" + throwables.getstacktraceasstring(e));\n  }\n  system.exit(0);\n}',charsets:{}},{title:"Continue As New",frontmatter:{layout:"default",title:"Continue As New",permalink:"/docs/java-client/continue-as-new",readingShow:"top"},regularPath:"/docs/04-java-client/15-continue-as-new.html",relativePath:"docs/04-java-client/15-continue-as-new.md",key:"v-0463910e",path:"/docs/java-client/continue-as-new/",headersStr:null,content:'# Continue as new\n that need to rerun periodically could naively be implemented as a big for loop with a sleep where the entire logic of the is inside the body of the for loop. The problem with this approach is that the history for that will keep growing to a point where it reaches the maximum size enforced by the service.\n\nContinueAsNewis the low level construct that enables implementing such without the risk of failures down the road. The operation atomically completes the current execution and starts a new execution of the with the same . The new execution will not carry over any history from the old execution.\n\n@Override\npublic void greet(String name) {\n  activities.greet("Hello " + name + "!");\n  Workflow.continueAsNew(name);\n}',normalizedContent:'# continue as new\n that need to rerun periodically could naively be implemented as a big for loop with a sleep where the entire logic of the is inside the body of the for loop. the problem with this approach is that the history for that will keep growing to a point where it reaches the maximum size enforced by the service.\n\ncontinueasnewis the low level construct that enables implementing such without the risk of failures down the road. the operation atomically completes the current execution and starts a new execution of the with the same . the new execution will not carry over any history from the old execution.\n\n@override\npublic void greet(string name) {\n  activities.greet("hello " + name + "!");\n  workflow.continueasnew(name);\n}',charsets:{}},{title:"Side Effect",frontmatter:{layout:"default",title:"Side Effect",permalink:"/docs/java-client/side-effect",readingShow:"top"},regularPath:"/docs/04-java-client/16-side-effect.html",relativePath:"docs/04-java-client/16-side-effect.md",key:"v-5ee76854",path:"/docs/java-client/side-effect/",headers:[{level:2,title:"Mutable Side Effect",slug:"mutable-side-effect",normalizedTitle:"mutable side effect",charIndex:1561}],headersStr:"Mutable Side Effect",content:"# Side Effect\nSide Effect allow workflow executes the provided function once, records its result into the workflow history. The recorded result on history will be returned without executing the provided function during replay. This guarantees the deterministic requirement for workflow as the exact same result will be returned in replay. Common use case is to run some short non-deterministic code in workflow, like getting random number. The only way to fail SideEffect is to panic which causes decision task failure. The decision task after timeout is rescheduled and re-executed giving SideEffect another chance to succeed.\n\n!!Caution: do not use sideEffect function to modify any workflow state. Only use the SideEffect's return value. For example this code is BROKEN:\n\nBad example:\n\n AtomicInteger random = new AtomicInteger();\n Workflow.sideEffect(() -> {\n        random.set(random.nextInt(100));\n        return null;\n });\n // random will always be 0 in replay, thus this code is non-deterministic\n if random.get() < 50 {\n        ....\n } else {\n        ....\n }\n\n\nOn replay the provided function is not executed, the random will always be 0, and the workflow could takes a different path breaking the determinism.\n\nHere is the correct way to use sideEffect:\n\nGood example:\n\n int random = Workflow.sideEffect(Integer.class, () -> random.nextInt(100));\n if random < 50 {\n        ....\n } else {\n        ....\n }\n\n\nIf function throws any exception it is not delivered to the workflow code. It is wrapped in an Error causing failure of the current decision.\n\n# Mutable Side Effect\nMutableSideEffect is similar to sideEffect, in allowing calls of non-deterministic functions from workflow code. The difference is that every sideEffect call in non-replay mode results in a new marker event recorded into the history. However, mutableSideEffect only records a new marker if a value has changed. During the replay, mutableSideEffect will not execute the function again, but it will return the exact same value as it was returning during the non-replay run.\n\nOne good use case of mutableSideEffect is to access a dynamically changing config without breaking determinism. Even if called very frequently the config value is recorded only when it changes not causing any performance degradation due to a large history size.\n\n!!Caution: do not use mutableSideEffect function to modify any workflow sate. Only use the mutableSideEffect's return value.\n\nIf function throws any exception it is not delivered to the workflow code. It is wrapped in an Error causing failure of the current decision.",normalizedContent:"# side effect\nside effect allow workflow executes the provided function once, records its result into the workflow history. the recorded result on history will be returned without executing the provided function during replay. this guarantees the deterministic requirement for workflow as the exact same result will be returned in replay. common use case is to run some short non-deterministic code in workflow, like getting random number. the only way to fail sideeffect is to panic which causes decision task failure. the decision task after timeout is rescheduled and re-executed giving sideeffect another chance to succeed.\n\n!!caution: do not use sideeffect function to modify any workflow state. only use the sideeffect's return value. for example this code is broken:\n\nbad example:\n\n atomicinteger random = new atomicinteger();\n workflow.sideeffect(() -> {\n        random.set(random.nextint(100));\n        return null;\n });\n // random will always be 0 in replay, thus this code is non-deterministic\n if random.get() < 50 {\n        ....\n } else {\n        ....\n }\n\n\non replay the provided function is not executed, the random will always be 0, and the workflow could takes a different path breaking the determinism.\n\nhere is the correct way to use sideeffect:\n\ngood example:\n\n int random = workflow.sideeffect(integer.class, () -> random.nextint(100));\n if random < 50 {\n        ....\n } else {\n        ....\n }\n\n\nif function throws any exception it is not delivered to the workflow code. it is wrapped in an error causing failure of the current decision.\n\n# mutable side effect\nmutablesideeffect is similar to sideeffect, in allowing calls of non-deterministic functions from workflow code. the difference is that every sideeffect call in non-replay mode results in a new marker event recorded into the history. however, mutablesideeffect only records a new marker if a value has changed. during the replay, mutablesideeffect will not execute the function again, but it will return the exact same value as it was returning during the non-replay run.\n\none good use case of mutablesideeffect is to access a dynamically changing config without breaking determinism. even if called very frequently the config value is recorded only when it changes not causing any performance degradation due to a large history size.\n\n!!caution: do not use mutablesideeffect function to modify any workflow sate. only use the mutablesideeffect's return value.\n\nif function throws any exception it is not delivered to the workflow code. it is wrapped in an error causing failure of the current decision.",charsets:{}},{title:"Testing",frontmatter:{layout:"default",title:"Testing",permalink:"/docs/java-client/testing",readingShow:"top"},regularPath:"/docs/04-java-client/17-testing.html",relativePath:"docs/04-java-client/17-testing.md",key:"v-45b94840",path:"/docs/java-client/testing/",headers:[{level:2,title:"Workflow Test Environment",slug:"workflow-test-environment",normalizedTitle:"workflow test environment",charIndex:807}],headersStr:"Workflow Test Environment",content:'# Activity Test Environment\nTestActivityEnvironment is the helper class for unit testing activity implementations. Supports calls to Activity methods from the tested activities. An example test:\n\n\n   public interface TestActivity {\n     String activity1(String input);\n   }\n\n   private static class ActivityImpl implements TestActivity {\n     @Override\n     public String activity1(String input) {\n       return Activity.getTask().getActivityType().getName() + "-" + input;\n     }\n   }\n\n   @Test\n   public void testSuccess() {\n     testEnvironment.registerActivitiesImplementations(new ActivityImpl());\n     TestActivity activity = testEnvironment.newActivityStub(TestActivity.class);\n     String result = activity.activity1("input1");\n     assertEquals("TestActivity::activity1-input1", result);\n   }\n\n\n\n# Workflow Test Environment\nTestWorkflowEnvironment provides workflow unit testing capabilities.\n\nTesting the workflow code is hard as it might be potentially very long running. The included in-memory implementation of the Cadence service supports an automatic time skipping. Anytime a workflow under the test as well as the unit test code are waiting on a timer (or sleep) the internal service time is automatically advanced to the nearest time that unblocks one of the waiting threads. This way a workflow that runs in production for months is unit tested in milliseconds. Here is an example of a test that executes in a few milliseconds instead of over two hours that are needed for the workflow to complete:\n\npublic class SignaledWorkflowImpl implements SignaledWorkflow {\n  private String signalInput;\n\n  @Override\n  public String workflow1(String input) {\n    Workflow.sleep(Duration.ofHours(1));\n    Workflow.await(() -> signalInput != null);\n    Workflow.sleep(Duration.ofHours(1));\n    return signalInput + "-" + input;\n  }\n\n  @Override\n  public void processSignal(String input) {\n    signalInput = input;\n }\n}\n\n@Test\npublic void testSignal() throws ExecutionException, InterruptedException {\n TestWorkflowEnvironment testEnvironment = TestWorkflowEnvironment.newInstance();\n\n // Creates a worker that polls tasks from the service owned by the testEnvironment.\n Worker worker = testEnvironment.newWorker(TASK_LIST);\n worker.registerWorkflowImplementationTypes(SignaledWorkflowImpl.class);\n testEnvironment.start();\n\n // Creates a WorkflowClient that interacts with the server owned by the testEnvironment.\n WorkflowClient client = testEnvironment.newWorkflowClient();\n SignaledWorkflow workflow = client.newWorkflowStub(SignaledWorkflow.class);\n\n // Starts a workflow execution\n CompletableFuture result = WorkflowClient.execute(workflow::workflow1, "input1");\n\n // The sleep forwards the service clock for 65 minutes without blocking.\n // This ensures that the signal is sent after the one hour sleep in the workflow code.\n testEnvironment.sleep(Duration.ofMinutes(65));\n workflow.processSignal("signalInput");\n\n // Blocks until workflow is complete. Workflow sleep forwards clock for one hour and\n // this call returns almost immediately.\n assertEquals("signalInput-input1", result.get());\n\n // Closes workers and releases in-memory service.\n testEnvironment.close();\n}',normalizedContent:'# activity test environment\ntestactivityenvironment is the helper class for unit testing activity implementations. supports calls to activity methods from the tested activities. an example test:\n\n\n   public interface testactivity {\n     string activity1(string input);\n   }\n\n   private static class activityimpl implements testactivity {\n     @override\n     public string activity1(string input) {\n       return activity.gettask().getactivitytype().getname() + "-" + input;\n     }\n   }\n\n   @test\n   public void testsuccess() {\n     testenvironment.registeractivitiesimplementations(new activityimpl());\n     testactivity activity = testenvironment.newactivitystub(testactivity.class);\n     string result = activity.activity1("input1");\n     assertequals("testactivity::activity1-input1", result);\n   }\n\n\n\n# workflow test environment\ntestworkflowenvironment provides workflow unit testing capabilities.\n\ntesting the workflow code is hard as it might be potentially very long running. the included in-memory implementation of the cadence service supports an automatic time skipping. anytime a workflow under the test as well as the unit test code are waiting on a timer (or sleep) the internal service time is automatically advanced to the nearest time that unblocks one of the waiting threads. this way a workflow that runs in production for months is unit tested in milliseconds. here is an example of a test that executes in a few milliseconds instead of over two hours that are needed for the workflow to complete:\n\npublic class signaledworkflowimpl implements signaledworkflow {\n  private string signalinput;\n\n  @override\n  public string workflow1(string input) {\n    workflow.sleep(duration.ofhours(1));\n    workflow.await(() -> signalinput != null);\n    workflow.sleep(duration.ofhours(1));\n    return signalinput + "-" + input;\n  }\n\n  @override\n  public void processsignal(string input) {\n    signalinput = input;\n }\n}\n\n@test\npublic void testsignal() throws executionexception, interruptedexception {\n testworkflowenvironment testenvironment = testworkflowenvironment.newinstance();\n\n // creates a worker that polls tasks from the service owned by the testenvironment.\n worker worker = testenvironment.newworker(task_list);\n worker.registerworkflowimplementationtypes(signaledworkflowimpl.class);\n testenvironment.start();\n\n // creates a workflowclient that interacts with the server owned by the testenvironment.\n workflowclient client = testenvironment.newworkflowclient();\n signaledworkflow workflow = client.newworkflowstub(signaledworkflow.class);\n\n // starts a workflow execution\n completablefuture result = workflowclient.execute(workflow::workflow1, "input1");\n\n // the sleep forwards the service clock for 65 minutes without blocking.\n // this ensures that the signal is sent after the one hour sleep in the workflow code.\n testenvironment.sleep(duration.ofminutes(65));\n workflow.processsignal("signalinput");\n\n // blocks until workflow is complete. workflow sleep forwards clock for one hour and\n // this call returns almost immediately.\n assertequals("signalinput-input1", result.get());\n\n // closes workers and releases in-memory service.\n testenvironment.close();\n}',charsets:{}},{title:"Workflow Replay and Shadowing",frontmatter:{layout:"default",title:"Workflow Replay and Shadowing",permalink:"/docs/java-client/workflow-replay-shadowing",readingShow:"top"},regularPath:"/docs/04-java-client/18-workflow-replay-shadowing.html",relativePath:"docs/04-java-client/18-workflow-replay-shadowing.md",key:"v-389be5ec",path:"/docs/java-client/workflow-replay-shadowing/",headers:[{level:2,title:"Workflow Replayer",slug:"workflow-replayer",normalizedTitle:"workflow replayer",charIndex:468},{level:3,title:"Write a Replay Test",slug:"write-a-replay-test",normalizedTitle:"write a replay test",charIndex:820},{level:3,title:"Sample Replay Test",slug:"sample-replay-test",normalizedTitle:"sample replay test",charIndex:2155},{level:2,title:"Workflow Shadower",slug:"workflow-shadower",normalizedTitle:"workflow shadower",charIndex:490},{level:3,title:"Shadow Options",slug:"shadow-options",normalizedTitle:"shadow options",charIndex:3266},{level:3,title:"Local Shadowing Test",slug:"local-shadowing-test",normalizedTitle:"local shadowing test",charIndex:4957},{level:3,title:"Shadowing Worker",slug:"shadowing-worker",normalizedTitle:"shadowing worker",charIndex:6116}],headersStr:"Workflow Replayer Write a Replay Test Sample Replay Test Workflow Shadower Shadow Options Local Shadowing Test Shadowing Worker",content:"# Workflow Replay and Shadowing\nIn the Versioning section, we mentioned that incompatible changes to workflow definition code could cause non-deterministic issues when processing workflow tasks if versioning is not done correctly. However, it may be hard for you to tell if a particular change is incompatible or not and whether versioning logic is needed. To help you identify incompatible changes and catch them before production traffic is impacted, we implemented Workflow Replayer and Workflow Shadower.\n\n# Workflow Replayer\nWorkflow Replayer is a testing component for replaying existing workflow histories against a workflow definition. The replaying logic is the same as the one used for processing workflow tasks, so if there's any incompatible changes in the workflow definition, the replay test will fail.\n\n# Write a Replay Test\n# Step 1: Prepare workflow histories\nReplayer can read workflow history from a local json file or fetch it directly from the Cadence server. If you would like to use the first method, you can use the following CLI command, otherwise you can skip to the next step.\n\ncadence --do <domain> workflow show --wid <workflowID> --rid <runID> --of <output file name>\n\n\nThe dumped workflow history will be stored in the file at the path you specified in json format.\n\n# Step 2: Call the replay method\nOnce you have the workflow history or have the connection to Cadence server for fetching history, call one of the four replay methods to start the replay test.\n\n// if workflow history has been loaded into memory\nWorkflowReplayer.replayWorkflowExecution(history, MyWorkflowImpl.class);\n\n// if workflow history is stored in a json file\nWorkflowReplayer.replayWorkflowExecutionFromResource(\"workflowHistory.json\", MyWorkflowImpl.class);\n\n// if workflow history is read from a File\nWorkflowReplayer.replayWorkflowExecution(historyFileObject, MyWorkflowImpl.class);\n\n\n# Step 3: Catch returned exception\nIf an exception is returned from the replay method, it means there's a incompatible change in the workflow definition and the error message will contain more information regarding where the non-deterministic error happens.\n\n# Sample Replay Test\nThis sample is also available in our samples repo at here.\n\npublic class HelloActivityReplayTest {\n  @Test\n  public void testReplay() throws Exception {\n    WorkflowReplayer.replayWorkflowExecutionFromResource(\n        \"HelloActivity.json\", HelloActivity.GreetingWorkflowImpl.class);\n  }\n}\n\n\n# Workflow Shadower\nWorkflow Replayer works well when verifying the compatibility against a small number of workflows histories. If there are lots of workflows in production that need to be verified, dumping all histories manually clearly won't work. Directly fetching histories from cadence server might be a solution, but the time to replay all workflow histories might be too long for a test.\n\nWorkflow Shadower is built on top of Workflow Replayer to address this problem. The basic idea of shadowing is: scan workflows based on the filters you defined, fetch history for each workflow in the scan result from Cadence server and run the replay test. It can be run either as a test to serve local development purpose or as a workflow in your worker to continuously replay production workflows.\n\n# Shadow Options\nComplete documentation on shadow options which includes default values, accepted values, etc. can be found here. The following sections are just a brief description of each option.\n\n# Scan Filters\n * WorkflowQuery: If you are familiar with our advanced visibility query syntax, you can specify a query directly. If specified, all other scan filters must be left empty.\n * WorkflowTypes: A list of workflow Type names.\n * WorkflowStatuses: A list of workflow status.\n * WorkflowStartTimeFilter: Min and max timestamp for workflow start time.\n * WorkflowSamplingRate: Sampling workflows from the scan result before executing the replay test.\n\n# Shadow Exit Condition\n * ExpirationInterval: Shadowing will exit when the specified interval has passed.\n * ShadowCount: Shadowing will exit after this number of workflow has been replayed. Note: replay maybe skipped due to errors like can't fetch history, history too short, etc. Skipped workflows won't be taken into account for ShadowCount.\n\n# Shadow Mode\n * Normal: Shadowing will complete after all workflows matches WorkflowQuery (after sampling) have been replayed or when exit condition is met.\n * Continuous: A new round of shadowing will be started after all workflows matches WorkflowQuery have been replayed. There will be a 5 min wait period between each round, and currently this wait period is not configurable. Shadowing will complete only when ExitCondition is met. ExitCondition must be specified when using this mode.\n\n# Shadow Concurrency\n * Concurrency: workflow replay concurrency. If not specified, it will default to 1. For local shadowing, an error will be returned if a value higher than 1 is specified.\n\n# Local Shadowing Test\nLocal shadowing test is similar to the replay test. First create a workflow shadower with optional shadow and replay options, then register the workflow that needs to be shadowed. Finally, call the Run method to start the shadowing. The method will return if shadowing has finished or any non-deterministic error is found.\n\nHere's a simple example. The example is also available here.\n\npublic void testShadowing() throws Throwable {\n  IWorkflowService service = new WorkflowServiceTChannel(ClientOptions.defaultInstance());\n\n  ShadowingOptions options = ShadowingOptions\n          .newBuilder()\n          .setDomain(DOMAIN)\n          .setShadowMode(Mode.Normal)\n          .setWorkflowTypes(Lists.newArrayList(\"GreetingWorkflow::getGreeting\"))\n          .setWorkflowStatuses(Lists.newArrayList(WorkflowStatus.OPEN, WorkflowStatus.CLOSED))\n          .setExitCondition(new ExitCondition().setExpirationIntervalInSeconds(60))\n          .build();\n  WorkflowShadower shadower = new WorkflowShadower(service, options, TASK_LIST);\n  shadower.registerWorkflowImplementationTypes(HelloActivity.GreetingWorkflowImpl.class);\n\n  shadower.run();\n}\n\n\n# Shadowing Worker\nNOTE:\n\n * All shadow workflows are running in one Cadence system domain, and right now, every user domain can only have one shadow workflow at a time.\n * The Cadence server used for scanning and getting workflow history will also be the Cadence server for running your shadow workflow. Currently, there's no way to specify different Cadence servers for hosting the shadowing workflow and scanning/fetching workflow.\n\nYour worker can also be configured to run in shadow mode to run shadow tests as a workflow. This is useful if there's a number of workflows that need to be replayed. Using a workflow can make sure the shadowing won't accidentally fail in the middle and the replay load can be distributed by deploying more shadow mode workers. It can also be incorporated into your deployment process to make sure there's no failed replay checks before deploying your change to production workers.\n\nWhen running in shadow mode, the normal decision worker will be disabled so that it won't update any production workflows. A special shadow activity worker will be started to execute activities for scanning and replaying workflows. The actual shadow workflow logic is controlled by Cadence server and your worker is only responsible for scanning and replaying workflows.\n\nReplay succeed, skipped and failed metrics will be emitted by your worker when executing the shadow workflow and you can monitor those metrics to see if there's any incompatible changes.\n\nTo enable the shadow mode, you can initialize a shadowing worker and pass in the shadowing options.\n\nTo enable the shadowing worker, here is a example. The example is also available here:\n\nWorkflowClient workflowClient =\n  WorkflowClient.newInstance(\n          new WorkflowServiceTChannel(ClientOptions.defaultInstance()),\n          WorkflowClientOptions.newBuilder().setDomain(DOMAIN).build());\n  ShadowingOptions options = ShadowingOptions\n          .newBuilder()\n          .setDomain(DOMAIN)\n          .setShadowMode(Mode.Normal)\n          .setWorkflowTypes(Lists.newArrayList(\"GreetingWorkflow::getGreeting\"))\n          .setWorkflowStatuses(Lists.newArrayList(WorkflowStatus.OPEN, WorkflowStatus.CLOSED))\n          .setExitCondition(new ExitCondition().setExpirationIntervalInSeconds(60))\n          .build();\n\n  ShadowingWorker shadowingWorker = new ShadowingWorker(\n          workflowClient,\n          \"HelloActivity\",\n          WorkerOptions.defaultInstance(),\n          options);\n  shadowingWorker.registerWorkflowImplementationTypes(HelloActivity.GreetingWorkflowImpl.class);\n\tshadowingWorker.start();\n\n\nRegistered workflows will be forwarded to the underlying WorkflowReplayer. DataConverter, WorkflowInterceptorChainFactories, ContextPropagators, and Tracer specified in the worker.Options will also be used as ReplayOptions. Since all shadow workflows are running in one system domain, to avoid conflict, the actual task list name used will be domain-tasklist.",normalizedContent:"# workflow replay and shadowing\nin the versioning section, we mentioned that incompatible changes to workflow definition code could cause non-deterministic issues when processing workflow tasks if versioning is not done correctly. however, it may be hard for you to tell if a particular change is incompatible or not and whether versioning logic is needed. to help you identify incompatible changes and catch them before production traffic is impacted, we implemented workflow replayer and workflow shadower.\n\n# workflow replayer\nworkflow replayer is a testing component for replaying existing workflow histories against a workflow definition. the replaying logic is the same as the one used for processing workflow tasks, so if there's any incompatible changes in the workflow definition, the replay test will fail.\n\n# write a replay test\n# step 1: prepare workflow histories\nreplayer can read workflow history from a local json file or fetch it directly from the cadence server. if you would like to use the first method, you can use the following cli command, otherwise you can skip to the next step.\n\ncadence --do <domain> workflow show --wid <workflowid> --rid <runid> --of <output file name>\n\n\nthe dumped workflow history will be stored in the file at the path you specified in json format.\n\n# step 2: call the replay method\nonce you have the workflow history or have the connection to cadence server for fetching history, call one of the four replay methods to start the replay test.\n\n// if workflow history has been loaded into memory\nworkflowreplayer.replayworkflowexecution(history, myworkflowimpl.class);\n\n// if workflow history is stored in a json file\nworkflowreplayer.replayworkflowexecutionfromresource(\"workflowhistory.json\", myworkflowimpl.class);\n\n// if workflow history is read from a file\nworkflowreplayer.replayworkflowexecution(historyfileobject, myworkflowimpl.class);\n\n\n# step 3: catch returned exception\nif an exception is returned from the replay method, it means there's a incompatible change in the workflow definition and the error message will contain more information regarding where the non-deterministic error happens.\n\n# sample replay test\nthis sample is also available in our samples repo at here.\n\npublic class helloactivityreplaytest {\n  @test\n  public void testreplay() throws exception {\n    workflowreplayer.replayworkflowexecutionfromresource(\n        \"helloactivity.json\", helloactivity.greetingworkflowimpl.class);\n  }\n}\n\n\n# workflow shadower\nworkflow replayer works well when verifying the compatibility against a small number of workflows histories. if there are lots of workflows in production that need to be verified, dumping all histories manually clearly won't work. directly fetching histories from cadence server might be a solution, but the time to replay all workflow histories might be too long for a test.\n\nworkflow shadower is built on top of workflow replayer to address this problem. the basic idea of shadowing is: scan workflows based on the filters you defined, fetch history for each workflow in the scan result from cadence server and run the replay test. it can be run either as a test to serve local development purpose or as a workflow in your worker to continuously replay production workflows.\n\n# shadow options\ncomplete documentation on shadow options which includes default values, accepted values, etc. can be found here. the following sections are just a brief description of each option.\n\n# scan filters\n * workflowquery: if you are familiar with our advanced visibility query syntax, you can specify a query directly. if specified, all other scan filters must be left empty.\n * workflowtypes: a list of workflow type names.\n * workflowstatuses: a list of workflow status.\n * workflowstarttimefilter: min and max timestamp for workflow start time.\n * workflowsamplingrate: sampling workflows from the scan result before executing the replay test.\n\n# shadow exit condition\n * expirationinterval: shadowing will exit when the specified interval has passed.\n * shadowcount: shadowing will exit after this number of workflow has been replayed. note: replay maybe skipped due to errors like can't fetch history, history too short, etc. skipped workflows won't be taken into account for shadowcount.\n\n# shadow mode\n * normal: shadowing will complete after all workflows matches workflowquery (after sampling) have been replayed or when exit condition is met.\n * continuous: a new round of shadowing will be started after all workflows matches workflowquery have been replayed. there will be a 5 min wait period between each round, and currently this wait period is not configurable. shadowing will complete only when exitcondition is met. exitcondition must be specified when using this mode.\n\n# shadow concurrency\n * concurrency: workflow replay concurrency. if not specified, it will default to 1. for local shadowing, an error will be returned if a value higher than 1 is specified.\n\n# local shadowing test\nlocal shadowing test is similar to the replay test. first create a workflow shadower with optional shadow and replay options, then register the workflow that needs to be shadowed. finally, call the run method to start the shadowing. the method will return if shadowing has finished or any non-deterministic error is found.\n\nhere's a simple example. the example is also available here.\n\npublic void testshadowing() throws throwable {\n  iworkflowservice service = new workflowservicetchannel(clientoptions.defaultinstance());\n\n  shadowingoptions options = shadowingoptions\n          .newbuilder()\n          .setdomain(domain)\n          .setshadowmode(mode.normal)\n          .setworkflowtypes(lists.newarraylist(\"greetingworkflow::getgreeting\"))\n          .setworkflowstatuses(lists.newarraylist(workflowstatus.open, workflowstatus.closed))\n          .setexitcondition(new exitcondition().setexpirationintervalinseconds(60))\n          .build();\n  workflowshadower shadower = new workflowshadower(service, options, task_list);\n  shadower.registerworkflowimplementationtypes(helloactivity.greetingworkflowimpl.class);\n\n  shadower.run();\n}\n\n\n# shadowing worker\nnote:\n\n * all shadow workflows are running in one cadence system domain, and right now, every user domain can only have one shadow workflow at a time.\n * the cadence server used for scanning and getting workflow history will also be the cadence server for running your shadow workflow. currently, there's no way to specify different cadence servers for hosting the shadowing workflow and scanning/fetching workflow.\n\nyour worker can also be configured to run in shadow mode to run shadow tests as a workflow. this is useful if there's a number of workflows that need to be replayed. using a workflow can make sure the shadowing won't accidentally fail in the middle and the replay load can be distributed by deploying more shadow mode workers. it can also be incorporated into your deployment process to make sure there's no failed replay checks before deploying your change to production workers.\n\nwhen running in shadow mode, the normal decision worker will be disabled so that it won't update any production workflows. a special shadow activity worker will be started to execute activities for scanning and replaying workflows. the actual shadow workflow logic is controlled by cadence server and your worker is only responsible for scanning and replaying workflows.\n\nreplay succeed, skipped and failed metrics will be emitted by your worker when executing the shadow workflow and you can monitor those metrics to see if there's any incompatible changes.\n\nto enable the shadow mode, you can initialize a shadowing worker and pass in the shadowing options.\n\nto enable the shadowing worker, here is a example. the example is also available here:\n\nworkflowclient workflowclient =\n  workflowclient.newinstance(\n          new workflowservicetchannel(clientoptions.defaultinstance()),\n          workflowclientoptions.newbuilder().setdomain(domain).build());\n  shadowingoptions options = shadowingoptions\n          .newbuilder()\n          .setdomain(domain)\n          .setshadowmode(mode.normal)\n          .setworkflowtypes(lists.newarraylist(\"greetingworkflow::getgreeting\"))\n          .setworkflowstatuses(lists.newarraylist(workflowstatus.open, workflowstatus.closed))\n          .setexitcondition(new exitcondition().setexpirationintervalinseconds(60))\n          .build();\n\n  shadowingworker shadowingworker = new shadowingworker(\n          workflowclient,\n          \"helloactivity\",\n          workeroptions.defaultinstance(),\n          options);\n  shadowingworker.registerworkflowimplementationtypes(helloactivity.greetingworkflowimpl.class);\n\tshadowingworker.start();\n\n\nregistered workflows will be forwarded to the underlying workflowreplayer. dataconverter, workflowinterceptorchainfactories, contextpropagators, and tracer specified in the worker.options will also be used as replayoptions. since all shadow workflows are running in one system domain, to avoid conflict, the actual task list name used will be domain-tasklist.",charsets:{}},{title:"Introduction",frontmatter:{layout:"default",title:"Introduction",permalink:"/docs/java-client",readingShow:"top"},regularPath:"/docs/04-java-client/",relativePath:"docs/04-java-client/index.md",key:"v-651b4e0a",path:"/docs/java-client/",headersStr:null,content:"# Java client\nThe following are important links for the Cadence Java client:\n\n * GitHub project: https://github.com/uber/cadence-java-client\n * Samples: https://github.com/uber/cadence-java-samples\n * JavaDoc documentation: https://www.javadoc.io/doc/com.uber.cadence/cadence-client\n\nAdd cadence-client as a dependency to your pom.xml:\n\n<dependency>\n  <groupId>com.uber.cadence</groupId>\n  <artifactId>cadence-client</artifactId>\n  <version>LATEST.RELEASE.VERSION</version>\n</dependency>\n\n\nor to build.gradle:\n\ncompile group: 'com.uber.cadence', name: 'cadence-client', version: 'LATEST.RELEASE.VERSION'\n\n\nCheck the latest release version in Release page",normalizedContent:"# java client\nthe following are important links for the cadence java client:\n\n * github project: https://github.com/uber/cadence-java-client\n * samples: https://github.com/uber/cadence-java-samples\n * javadoc documentation: https://www.javadoc.io/doc/com.uber.cadence/cadence-client\n\nadd cadence-client as a dependency to your pom.xml:\n\n<dependency>\n  <groupid>com.uber.cadence</groupid>\n  <artifactid>cadence-client</artifactid>\n  <version>latest.release.version</version>\n</dependency>\n\n\nor to build.gradle:\n\ncompile group: 'com.uber.cadence', name: 'cadence-client', version: 'latest.release.version'\n\n\ncheck the latest release version in release page",charsets:{}},{title:"Worker service",frontmatter:{layout:"default",title:"Worker service",permalink:"/docs/go-client/workers",readingShow:"top"},regularPath:"/docs/05-go-client/01-workers.html",relativePath:"docs/05-go-client/01-workers.md",key:"v-e9a63714",path:"/docs/go-client/workers/",headersStr:null,content:'# Worker service\nA or service is a service that hosts the and implementations. The polls the Cadence service for , performs those , and communicates execution results back to the Cadence service. services are developed, deployed, and operated by Cadence customers.\n\nYou can run a Cadence in a new or an existing service. Use the framework APIs to start the Cadence and link in all and implementations that you require the service to execute.\n\npackage main\n\nimport (\n\n    "go.uber.org/cadence/.gen/go/cadence"\n    "go.uber.org/cadence/.gen/go/cadence/workflowserviceclient"\n    "go.uber.org/cadence/worker"\n\n    "github.com/uber-go/tally"\n    "go.uber.org/zap"\n    "go.uber.org/zap/zapcore"\n    "go.uber.org/yarpc"\n    "go.uber.org/yarpc/api/transport"\n    "go.uber.org/yarpc/transport/tchannel"\n)\n\nvar HostPort = "127.0.0.1:7933"\nvar Domain = "SimpleDomain"\nvar TaskListName = "SimpleWorker"\nvar ClientName = "SimpleWorker"\nvar CadenceService = "cadence-frontend"\n\nfunc main() {\n    startWorker(buildLogger(), buildCadenceClient())\n}\n\nfunc buildLogger() *zap.Logger {\n    config := zap.NewDevelopmentConfig()\n    config.Level.SetLevel(zapcore.InfoLevel)\n\n    var err error\n    logger, err := config.Build()\n    if err != nil {\n        panic("Failed to setup logger")\n    }\n\n    return logger\n}\n\nfunc buildCadenceClient() workflowserviceclient.Interface {\n    ch, err := tchannel.NewChannelTransport(tchannel.ServiceName(ClientName))\n    if err != nil {\n        panic("Failed to setup tchannel")\n    }\n    dispatcher := yarpc.NewDispatcher(yarpc.Config{\n        Name: ClientName,\n        Outbounds: yarpc.Outbounds{\n            CadenceService: {Unary: ch.NewSingleOutbound(HostPort)},\n        },\n    })\n    if err := dispatcher.Start(); err != nil {\n        panic("Failed to start dispatcher")\n    }\n\n    return workflowserviceclient.New(dispatcher.ClientConfig(CadenceService))\n}\n\nfunc startWorker(logger *zap.Logger, service workflowserviceclient.Interface) {\n    // TaskListName identifies set of client workflows, activities, and workers.\n    // It could be your group or client or application name.\n    workerOptions := worker.Options{\n        Logger:       logger,\n        MetricsScope: tally.NewTestScope(TaskListName, map[string]string{}),\n    }\n\n    worker := worker.New(\n        service,\n        Domain,\n        TaskListName,\n        workerOptions)\n    err := worker.Start()\n    if err != nil {\n        panic("Failed to start worker")\n    }\n\n    logger.Info("Started Worker.", zap.String("worker", TaskListName))\n}',normalizedContent:'# worker service\na or service is a service that hosts the and implementations. the polls the cadence service for , performs those , and communicates execution results back to the cadence service. services are developed, deployed, and operated by cadence customers.\n\nyou can run a cadence in a new or an existing service. use the framework apis to start the cadence and link in all and implementations that you require the service to execute.\n\npackage main\n\nimport (\n\n    "go.uber.org/cadence/.gen/go/cadence"\n    "go.uber.org/cadence/.gen/go/cadence/workflowserviceclient"\n    "go.uber.org/cadence/worker"\n\n    "github.com/uber-go/tally"\n    "go.uber.org/zap"\n    "go.uber.org/zap/zapcore"\n    "go.uber.org/yarpc"\n    "go.uber.org/yarpc/api/transport"\n    "go.uber.org/yarpc/transport/tchannel"\n)\n\nvar hostport = "127.0.0.1:7933"\nvar domain = "simpledomain"\nvar tasklistname = "simpleworker"\nvar clientname = "simpleworker"\nvar cadenceservice = "cadence-frontend"\n\nfunc main() {\n    startworker(buildlogger(), buildcadenceclient())\n}\n\nfunc buildlogger() *zap.logger {\n    config := zap.newdevelopmentconfig()\n    config.level.setlevel(zapcore.infolevel)\n\n    var err error\n    logger, err := config.build()\n    if err != nil {\n        panic("failed to setup logger")\n    }\n\n    return logger\n}\n\nfunc buildcadenceclient() workflowserviceclient.interface {\n    ch, err := tchannel.newchanneltransport(tchannel.servicename(clientname))\n    if err != nil {\n        panic("failed to setup tchannel")\n    }\n    dispatcher := yarpc.newdispatcher(yarpc.config{\n        name: clientname,\n        outbounds: yarpc.outbounds{\n            cadenceservice: {unary: ch.newsingleoutbound(hostport)},\n        },\n    })\n    if err := dispatcher.start(); err != nil {\n        panic("failed to start dispatcher")\n    }\n\n    return workflowserviceclient.new(dispatcher.clientconfig(cadenceservice))\n}\n\nfunc startworker(logger *zap.logger, service workflowserviceclient.interface) {\n    // tasklistname identifies set of client workflows, activities, and workers.\n    // it could be your group or client or application name.\n    workeroptions := worker.options{\n        logger:       logger,\n        metricsscope: tally.newtestscope(tasklistname, map[string]string{}),\n    }\n\n    worker := worker.new(\n        service,\n        domain,\n        tasklistname,\n        workeroptions)\n    err := worker.start()\n    if err != nil {\n        panic("failed to start worker")\n    }\n\n    logger.info("started worker.", zap.string("worker", tasklistname))\n}',charsets:{}},{title:"Creating workflows",frontmatter:{layout:"default",title:"Creating workflows",permalink:"/docs/go-client/create-workflows",readingShow:"top"},regularPath:"/docs/05-go-client/02-create-workflows.html",relativePath:"docs/05-go-client/02-create-workflows.md",key:"v-d91dcabc",path:"/docs/go-client/create-workflows/",headers:[{level:2,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:965},{level:2,title:"Declaration",slug:"declaration",normalizedTitle:"declaration",charIndex:1986},{level:2,title:"Implementation",slug:"implementation",normalizedTitle:"implementation",charIndex:932},{level:3,title:"Special Cadence client library functions and types",slug:"special-cadence-client-library-functions-and-types",normalizedTitle:"special cadence client library functions and types",charIndex:4731},{level:3,title:"Failing a workflow",slug:"failing-a-workflow",normalizedTitle:"failing a workflow",charIndex:5520},{level:2,title:"Registration",slug:"registration",normalizedTitle:"registration",charIndex:5653}],headersStr:"Overview Declaration Implementation Special Cadence client library functions and types Failing a workflow Registration",content:'# Creating workflows\nThe is the implementation of the coordination logic. The Cadence programming framework (aka client library) allows you to write the coordination logic as simple procedural code that uses standard Go data modeling. The client library takes care of the communication between the service and the Cadence service, and ensures state persistence between even in case of failures. Furthermore, any particular execution is not tied to a particular machine. Different steps of the coordination logic can end up executing on different instances, with the framework ensuring that the necessary state is recreated on the executing the step.\n\nHowever, in order to facilitate this operational model, both the Cadence programming framework and the managed service impose some requirements and restrictions on the implementation of the coordination logic. The details of these requirements and restrictions are described in theImplementation section below.\n\n# Overview\nThe sample code below shows a simple implementation of a that executes one . The also passes the sole parameter it receives as part of its initialization as a parameter to the .\n\npackage sample\n\nimport (\n    "time"\n\n    "go.uber.org/cadence/workflow"\n)\n\nfunc init() {\n    workflow.Register(SimpleWorkflow)\n}\n\nfunc SimpleWorkflow(ctx workflow.Context, value string) error {\n    ao := workflow.ActivityOptions{\n        TaskList:               "sampleTaskList",\n        ScheduleToCloseTimeout: time.Second * 60,\n        ScheduleToStartTimeout: time.Second * 60,\n        StartToCloseTimeout:    time.Second * 60,\n        HeartbeatTimeout:       time.Second * 10,\n        WaitForCancellation:    false,\n    }\n    ctx = workflow.WithActivityOptions(ctx, ao)\n\n    future := workflow.ExecuteActivity(ctx, SimpleActivity, value)\n    var result string\n    if err := future.Get(ctx, &result); err != nil {\n        return err\n    }\n    workflow.GetLogger(ctx).Info("Done", zap.String("result", result))\n    return nil\n}\n\n\n# Declaration\nIn the Cadence programing model, a is implemented with a function. The function declaration specifies the parameters the accepts as well as any values it might return.\n\nfunc SimpleWorkflow(ctx workflow.Context, value string) error\n\n\nLet’s deconstruct the declaration above:\n\n * The first parameter to the function is ctx workflow.Context. This is a required parameter for all functions and is used by the Cadence client library to pass execution context. Virtually all the client library functions that are callable from the functions require this ctx parameter. This context parameter is the same concept as the standardcontext.Context provided by Go. The only difference between workflow.Context andcontext.Context is that the Done() function in workflow.Context returnsworkflow.Channel instead the standard go chan.\n * The second parameter, string, is a custom parameter that can be used to pass data into the on start. A can have one or more such parameters. All parameters to a function must be serializable, which essentially means that params can’t be channels, functions, variadic, or unsafe pointers.\n * Since it only declares error as the return value, this means that the does not return a value. The error return value is used to indicate an error was encountered during execution and the should be terminated.\n\n# Implementation\nIn order to support the synchronous and sequential programming model for the implementation, there are certain restrictions and requirements on how the implementation must behave in order to guarantee correctness. The requirements are that:\n\n * Execution must be deterministic\n * Execution must be idempotent\n\nA straightforward way to think about these requirements is that the code is as follows:\n\n *  code can only read and manipulate local state or state received as return values from Cadence client library functions.\n *  code should not affect changes in external systems other than through invocation of .\n *  code should interact with time only through the functions provided by the Cadence client library (i.e. workflow.Now(), workflow.Sleep()).\n *  code should not create and interact with goroutines directly, it should instead use the functions provided by the Cadence client library (i.e., workflow.Go() instead of go,workflow.Channel instead of chan, workflow.Selector instead of select).\n *  code should do all logging via the logger provided by the Cadence client library (i.e., workflow.GetLogger()).\n *  code should not iterate over maps using range because the order of map iteration is randomized.\n\nNow that we have laid the ground rules, we can take a look at some of the special functions and types used for writing Cadence and how to implement some common patterns.\n\n# Special Cadence client library functions and types\nThe Cadence client library provides a number of functions and types as alternatives to some native Go functions and types. Usage of these replacement functions/types is necessary in order to ensure that the code execution is deterministic and repeatable within an execution context.\n\nCoroutine related constructs:\n\n * workflow.Go : This is a replacement for the the go statement.\n * workflow.Channel : This is a replacement for the native chan type. Cadence provides support for both buffered and unbuffered channels.\n * workflow.Selector : This is a replacement for the select statement.\n\nTime related functions:\n\n * workflow.Now() : This is a replacement for time.Now().\n * workflow.Sleep() : This is a replacement for time.Sleep().\n\n# Failing a workflow\nTo mark a as failed, all that needs to happen is for the function to return an error via the err return value.\n\n# Registration\nFor some client code to be able to invoke a type, the process needs to be aware of all the implementations it has access to. A is registered with the following call:\n\nworkflow.Register(SimpleWorkflow)\n\n\nThis call essentially creates an in-memory mapping inside the process between the fully qualified function name and the implementation. It is safe to call this registration method from an init() function. If the receives for a type it does not know, it will fail that . However, the failure of the will not cause the entire to fail.',normalizedContent:'# creating workflows\nthe is the implementation of the coordination logic. the cadence programming framework (aka client library) allows you to write the coordination logic as simple procedural code that uses standard go data modeling. the client library takes care of the communication between the service and the cadence service, and ensures state persistence between even in case of failures. furthermore, any particular execution is not tied to a particular machine. different steps of the coordination logic can end up executing on different instances, with the framework ensuring that the necessary state is recreated on the executing the step.\n\nhowever, in order to facilitate this operational model, both the cadence programming framework and the managed service impose some requirements and restrictions on the implementation of the coordination logic. the details of these requirements and restrictions are described in theimplementation section below.\n\n# overview\nthe sample code below shows a simple implementation of a that executes one . the also passes the sole parameter it receives as part of its initialization as a parameter to the .\n\npackage sample\n\nimport (\n    "time"\n\n    "go.uber.org/cadence/workflow"\n)\n\nfunc init() {\n    workflow.register(simpleworkflow)\n}\n\nfunc simpleworkflow(ctx workflow.context, value string) error {\n    ao := workflow.activityoptions{\n        tasklist:               "sampletasklist",\n        scheduletoclosetimeout: time.second * 60,\n        scheduletostarttimeout: time.second * 60,\n        starttoclosetimeout:    time.second * 60,\n        heartbeattimeout:       time.second * 10,\n        waitforcancellation:    false,\n    }\n    ctx = workflow.withactivityoptions(ctx, ao)\n\n    future := workflow.executeactivity(ctx, simpleactivity, value)\n    var result string\n    if err := future.get(ctx, &result); err != nil {\n        return err\n    }\n    workflow.getlogger(ctx).info("done", zap.string("result", result))\n    return nil\n}\n\n\n# declaration\nin the cadence programing model, a is implemented with a function. the function declaration specifies the parameters the accepts as well as any values it might return.\n\nfunc simpleworkflow(ctx workflow.context, value string) error\n\n\nlet’s deconstruct the declaration above:\n\n * the first parameter to the function is ctx workflow.context. this is a required parameter for all functions and is used by the cadence client library to pass execution context. virtually all the client library functions that are callable from the functions require this ctx parameter. this context parameter is the same concept as the standardcontext.context provided by go. the only difference between workflow.context andcontext.context is that the done() function in workflow.context returnsworkflow.channel instead the standard go chan.\n * the second parameter, string, is a custom parameter that can be used to pass data into the on start. a can have one or more such parameters. all parameters to a function must be serializable, which essentially means that params can’t be channels, functions, variadic, or unsafe pointers.\n * since it only declares error as the return value, this means that the does not return a value. the error return value is used to indicate an error was encountered during execution and the should be terminated.\n\n# implementation\nin order to support the synchronous and sequential programming model for the implementation, there are certain restrictions and requirements on how the implementation must behave in order to guarantee correctness. the requirements are that:\n\n * execution must be deterministic\n * execution must be idempotent\n\na straightforward way to think about these requirements is that the code is as follows:\n\n *  code can only read and manipulate local state or state received as return values from cadence client library functions.\n *  code should not affect changes in external systems other than through invocation of .\n *  code should interact with time only through the functions provided by the cadence client library (i.e. workflow.now(), workflow.sleep()).\n *  code should not create and interact with goroutines directly, it should instead use the functions provided by the cadence client library (i.e., workflow.go() instead of go,workflow.channel instead of chan, workflow.selector instead of select).\n *  code should do all logging via the logger provided by the cadence client library (i.e., workflow.getlogger()).\n *  code should not iterate over maps using range because the order of map iteration is randomized.\n\nnow that we have laid the ground rules, we can take a look at some of the special functions and types used for writing cadence and how to implement some common patterns.\n\n# special cadence client library functions and types\nthe cadence client library provides a number of functions and types as alternatives to some native go functions and types. usage of these replacement functions/types is necessary in order to ensure that the code execution is deterministic and repeatable within an execution context.\n\ncoroutine related constructs:\n\n * workflow.go : this is a replacement for the the go statement.\n * workflow.channel : this is a replacement for the native chan type. cadence provides support for both buffered and unbuffered channels.\n * workflow.selector : this is a replacement for the select statement.\n\ntime related functions:\n\n * workflow.now() : this is a replacement for time.now().\n * workflow.sleep() : this is a replacement for time.sleep().\n\n# failing a workflow\nto mark a as failed, all that needs to happen is for the function to return an error via the err return value.\n\n# registration\nfor some client code to be able to invoke a type, the process needs to be aware of all the implementations it has access to. a is registered with the following call:\n\nworkflow.register(simpleworkflow)\n\n\nthis call essentially creates an in-memory mapping inside the process between the fully qualified function name and the implementation. it is safe to call this registration method from an init() function. if the receives for a type it does not know, it will fail that . however, the failure of the will not cause the entire to fail.',charsets:{}},{title:"Activity overview",frontmatter:{layout:"default",title:"Activity overview",permalink:"/docs/go-client/activities",readingShow:"top"},regularPath:"/docs/05-go-client/03-activities.html",relativePath:"docs/05-go-client/03-activities.md",key:"v-241aa182",path:"/docs/go-client/activities/",headers:[{level:2,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:1159},{level:3,title:"Declaration",slug:"declaration",normalizedTitle:"declaration",charIndex:1846},{level:3,title:"Implementation",slug:"implementation",normalizedTitle:"implementation",charIndex:2970},{level:3,title:"Registration",slug:"registration",normalizedTitle:"registration",charIndex:5188},{level:2,title:"Failing an Activity",slug:"failing-an-activity",normalizedTitle:"failing an activity",charIndex:5591}],headersStr:"Overview Declaration Implementation Registration Failing an Activity",content:'# Activity overview\nAn is the implementation of a particular in the business logic.\n\n are implemented as functions. Data can be passed directly to an via function parameters. The parameters can be either basic types or structs, with the only requirement being that the parameters must be serializable. Though it is not required, we recommend that the first parameter of an function is of type context.Context, in order to allow the to interact with other framework methods. The function must return an error value, and can optionally return a result value. The result value can be either a basic type or a struct with the only requirement being that it is serializable.\n\nThe values passed to through invocation parameters or returned through the result value are recorded in the execution history. The entire execution history is transferred from the Cadence service to with every that the logic needs to process. A large execution history can thus adversely impact the performance of your . Therefore, be mindful of the amount of data you transfer via invocation parameters or return values. Otherwise, no additional limitations exist on implementations.\n\n# Overview\nThe following example demonstrates a simple that accepts a string parameter, appends a word to it, and then returns a result.\n\npackage simple\n\nimport (\n    "context"\n\n    "go.uber.org/cadence/activity"\n    "go.uber.org/zap"\n)\n\nfunc init() {\n    activity.Register(SimpleActivity)\n}\n\n// SimpleActivity is a sample Cadence activity function that takes one parameter and\n// returns a string containing the parameter value.\nfunc SimpleActivity(ctx context.Context, value string) (string, error) {\n    activity.GetLogger(ctx).Info("SimpleActivity called.", zap.String("Value", value))\n    return "Processed: " + value, nil\n}\n\n\nLet\'s take a look at each component of this activity.\n\n# Declaration\nIn the Cadence programing model, an is implemented with a function. The function declaration specifies the parameters the accepts as well as any values it might return. An function can take zero or many specific parameters and can return one or two values. It must always at least return an error value. The function can accept as parameters and return as results any serializable type.\n\nfunc SimpleActivity(ctx context.Context, value string) (string, error)\n\nThe first parameter to the function is context.Context. This is an optional parameter and can be omitted. This parameter is the standard Go context. The second string parameter is a custom specific parameter that can be used to pass data into the on start. An can have one or more such parameters. All parameters to an function must be serializable, which essentially means that params can’t be channels, functions, variadic, or unsafe pointers. The declares two return values: string and error. The string return value is used to return the result of the . The error return value is used to indicate that an error was encountered during execution.\n\n# Implementation\nYou can write implementation code in the same way that you would any other Go service code. Additionally, you can use the usual loggers and metrics controllers, and the standard Go concurrency constructs.\n\n# Heart Beating\nFor long-running , Cadence provides an API for the code to report both liveness and progress back to the Cadence managed service.\n\nprogress := 0\nfor hasWork {\n    // Send heartbeat message to the server.\n    cadence.RecordActivityHeartbeat(ctx, progress)\n    // Do some work.\n    ...\n    progress++\n}\n\n\nWhen an times out due to a missed heartbeat, the last value of the details (progress in the above sample) is returned from the cadence.ExecuteActivity function as the details field of TimeoutErrorwith TimeoutType_HEARTBEAT.\n\nNew auto heartbeat option in Cadence Go Client 0.17.0 release: In case you don\'t need to report progress, but still want to report liveness of your worker through heartbeating for your long running activities, there\'s a new auto-heartbeat option that you can enable when you register your activity. When this option is enabled Cadence library will do the heartbeat for you in the background.\n\n\tRegisterActivityOptions struct {\n\t\t...\n\t\t// Automatically send heartbeats for this activity at an interval that is less than the HeartbeatTimeout.\n\t\t// This option has no effect if the activity is executed with a HeartbeatTimeout of 0.\n\t\t// Default: false\n\t\tEnableAutoHeartbeat bool\n\t}\n\n\nYou can also heartbeat an from an external source:\n\n// Instantiate a Cadence service client.\ncadence.Client client = cadence.NewClient(...)\n\n// Record heartbeat.\nerr := client.RecordActivityHeartbeat(taskToken, details)\n\n\nThe parameters of the RecordActivityHeartbeat function are:\n\n * taskToken: The value of the binary TaskToken field of the ActivityInfo struct retrieved inside the .\n * details: The serializable payload containing progress information.\n\n# Cancellation\nWhen an is cancelled, or its has completed or failed, the context passed into its function is cancelled, which sets its channel’s closed state to Done. An can use that to perform any necessary cleanup and abort its execution. Cancellation is only delivered to that call RecordActivityHeartbeat.\n\n# Registration\nTo make the visible to the process hosting it, the must be registered via a call to activity.Register.\n\nfunc init() {\n    activity.Register(SimpleActivity)\n}\n\n\nThis call creates an in-memory mapping inside the process between the fully qualified function name and the implementation. If a receives a request to start an execution for an type it does not know, it will fail that request.\n\n# Failing an Activity\nTo mark an as failed, the function must return an error via the error return value.',normalizedContent:'# activity overview\nan is the implementation of a particular in the business logic.\n\n are implemented as functions. data can be passed directly to an via function parameters. the parameters can be either basic types or structs, with the only requirement being that the parameters must be serializable. though it is not required, we recommend that the first parameter of an function is of type context.context, in order to allow the to interact with other framework methods. the function must return an error value, and can optionally return a result value. the result value can be either a basic type or a struct with the only requirement being that it is serializable.\n\nthe values passed to through invocation parameters or returned through the result value are recorded in the execution history. the entire execution history is transferred from the cadence service to with every that the logic needs to process. a large execution history can thus adversely impact the performance of your . therefore, be mindful of the amount of data you transfer via invocation parameters or return values. otherwise, no additional limitations exist on implementations.\n\n# overview\nthe following example demonstrates a simple that accepts a string parameter, appends a word to it, and then returns a result.\n\npackage simple\n\nimport (\n    "context"\n\n    "go.uber.org/cadence/activity"\n    "go.uber.org/zap"\n)\n\nfunc init() {\n    activity.register(simpleactivity)\n}\n\n// simpleactivity is a sample cadence activity function that takes one parameter and\n// returns a string containing the parameter value.\nfunc simpleactivity(ctx context.context, value string) (string, error) {\n    activity.getlogger(ctx).info("simpleactivity called.", zap.string("value", value))\n    return "processed: " + value, nil\n}\n\n\nlet\'s take a look at each component of this activity.\n\n# declaration\nin the cadence programing model, an is implemented with a function. the function declaration specifies the parameters the accepts as well as any values it might return. an function can take zero or many specific parameters and can return one or two values. it must always at least return an error value. the function can accept as parameters and return as results any serializable type.\n\nfunc simpleactivity(ctx context.context, value string) (string, error)\n\nthe first parameter to the function is context.context. this is an optional parameter and can be omitted. this parameter is the standard go context. the second string parameter is a custom specific parameter that can be used to pass data into the on start. an can have one or more such parameters. all parameters to an function must be serializable, which essentially means that params can’t be channels, functions, variadic, or unsafe pointers. the declares two return values: string and error. the string return value is used to return the result of the . the error return value is used to indicate that an error was encountered during execution.\n\n# implementation\nyou can write implementation code in the same way that you would any other go service code. additionally, you can use the usual loggers and metrics controllers, and the standard go concurrency constructs.\n\n# heart beating\nfor long-running , cadence provides an api for the code to report both liveness and progress back to the cadence managed service.\n\nprogress := 0\nfor haswork {\n    // send heartbeat message to the server.\n    cadence.recordactivityheartbeat(ctx, progress)\n    // do some work.\n    ...\n    progress++\n}\n\n\nwhen an times out due to a missed heartbeat, the last value of the details (progress in the above sample) is returned from the cadence.executeactivity function as the details field of timeouterrorwith timeouttype_heartbeat.\n\nnew auto heartbeat option in cadence go client 0.17.0 release: in case you don\'t need to report progress, but still want to report liveness of your worker through heartbeating for your long running activities, there\'s a new auto-heartbeat option that you can enable when you register your activity. when this option is enabled cadence library will do the heartbeat for you in the background.\n\n\tregisteractivityoptions struct {\n\t\t...\n\t\t// automatically send heartbeats for this activity at an interval that is less than the heartbeattimeout.\n\t\t// this option has no effect if the activity is executed with a heartbeattimeout of 0.\n\t\t// default: false\n\t\tenableautoheartbeat bool\n\t}\n\n\nyou can also heartbeat an from an external source:\n\n// instantiate a cadence service client.\ncadence.client client = cadence.newclient(...)\n\n// record heartbeat.\nerr := client.recordactivityheartbeat(tasktoken, details)\n\n\nthe parameters of the recordactivityheartbeat function are:\n\n * tasktoken: the value of the binary tasktoken field of the activityinfo struct retrieved inside the .\n * details: the serializable payload containing progress information.\n\n# cancellation\nwhen an is cancelled, or its has completed or failed, the context passed into its function is cancelled, which sets its channel’s closed state to done. an can use that to perform any necessary cleanup and abort its execution. cancellation is only delivered to that call recordactivityheartbeat.\n\n# registration\nto make the visible to the process hosting it, the must be registered via a call to activity.register.\n\nfunc init() {\n    activity.register(simpleactivity)\n}\n\n\nthis call creates an in-memory mapping inside the process between the fully qualified function name and the implementation. if a receives a request to start an execution for an type it does not know, it will fail that request.\n\n# failing an activity\nto mark an as failed, the function must return an error via the error return value.',charsets:{}},{title:"Executing activities",frontmatter:{layout:"default",title:"Executing activities",permalink:"/docs/go-client/execute-activity",readingShow:"top"},regularPath:"/docs/05-go-client/04-execute-activity.html",relativePath:"docs/05-go-client/04-execute-activity.md",key:"v-7109c462",path:"/docs/go-client/execute-activity/",headers:[{level:2,title:"Activity options",slug:"activity-options",normalizedTitle:"activity options",charIndex:794},{level:2,title:"Activity timeouts",slug:"activity-timeouts",normalizedTitle:"activity timeouts",charIndex:1278},{level:2,title:"ExecuteActivity call",slug:"executeactivity-call",normalizedTitle:"executeactivity call",charIndex:2478}],headersStr:"Activity options Activity timeouts ExecuteActivity call",content:'# Executing activities\nThe primary responsibility of a implementation is to schedule for execution. The most straightforward way to do this is via the library method workflow.ExecuteActivity. The following sample code demonstrates making this call:\n\nao := cadence.ActivityOptions{\n    TaskList:               "sampleTaskList",\n    ScheduleToCloseTimeout: time.Second * 60,\n    ScheduleToStartTimeout: time.Second * 60,\n    StartToCloseTimeout:    time.Second * 60,\n    HeartbeatTimeout:       time.Second * 10,\n    WaitForCancellation:    false,\n}\nctx = cadence.WithActivityOptions(ctx, ao)\n\nfuture := workflow.ExecuteActivity(ctx, SimpleActivity, value)\nvar result string\nif err := future.Get(ctx, &result); err != nil {\n    return err\n}\n\n\nLet\'s take a look at each component of this call.\n\n# Activity options\nBefore calling workflow.ExecuteActivity(), you must configure ActivityOptions for the invocation. These options customize various execution timeouts, and are passed in by creating a child context from the initial context and overwriting the desired values. The child context is then passed into the workflow.ExecuteActivity() call. If multiple are sharing the same option values, then the same context instance can be used when calling workflow.ExecuteActivity().\n\n# Activity timeouts\nThere can be various kinds of timeouts associated with an . Cadence guarantees that are executed at most once, so an either succeeds or fails with one of the following timeouts:\n\nTimeout                  Description                                                                                                                                                                   \nStartToCloseTimeout      Maximum time that a worker can take to process a task after it has received the task.                                                                                         \nScheduleToStartTimeout   Time a task can wait to be picked up by an after a schedules it. If there are no workers available to process this task for the specified duration, the task will time out.   \nScheduleToCloseTimeout   Time a task can take to complete after it is scheduled by a . This is usually greater than the sum of StartToClose and ScheduleToStart timeouts.                              \nHeartbeatTimeout         If a task doesn\'t heartbeat to the Cadence service for this duration, it will be considered to have failed. This is useful for long-running tasks.                            \n\n# ExecuteActivity call\nThe first parameter in the call is the required cadence.Context object. This type is a copy ofcontext.Context with the Done() method returning cadence.Channel instead of the native Go chan.\n\nThe second parameter is the function that we registered as an function. This parameter can also be a string representing the fully qualified name of the function. The benefit of passing in the actual function object is that the framework can validate parameters.\n\nThe remaining parameters are passed to the as part of the call. In our example, we have a single parameter: value. This list of parameters must match the list of parameters declared by the function. The Cadence client library will validate this.\n\nThe method call returns immediately and returns a cadence.Future. This allows you to execute more code without having to wait for the scheduled to complete.\n\nWhen you are ready to process the results of the , call the Get() method on the future object returned. The parameters to this method are the ctx object we passed to theworkflow.ExecuteActivity() call and an output parameter that will receive the output of the. The type of the output parameter must match the type of the return value declared by the function. The Get() method will block until the completes and results are available.\n\nYou can retrieve the result value returned by workflow.ExecuteActivity() from the future and use it like any normal result from a synchronous function call. The following sample code demonstrates how you can use the result if it is a string value:\n\nvar result string\nif err := future.Get(ctx1, &result); err != nil {\n    return err\n}\n\nswitch result {\ncase "apple":\n    // Do something.\ncase "banana":\n    // Do something.\ndefault:\n    return err\n}\n\n\nIn this example, we called the Get() method on the returned future immediately after workflow.ExecuteActivity(). However, this is not necessary. If you want to execute multiple in parallel, you can repeatedly call workflow.ExecuteActivity(), store the returned futures, and then wait for all to complete by calling the Get() methods of the future at a later time.\n\nTo implement more complex wait conditions on returned future objects, use the cadence.Selector class.',normalizedContent:'# executing activities\nthe primary responsibility of a implementation is to schedule for execution. the most straightforward way to do this is via the library method workflow.executeactivity. the following sample code demonstrates making this call:\n\nao := cadence.activityoptions{\n    tasklist:               "sampletasklist",\n    scheduletoclosetimeout: time.second * 60,\n    scheduletostarttimeout: time.second * 60,\n    starttoclosetimeout:    time.second * 60,\n    heartbeattimeout:       time.second * 10,\n    waitforcancellation:    false,\n}\nctx = cadence.withactivityoptions(ctx, ao)\n\nfuture := workflow.executeactivity(ctx, simpleactivity, value)\nvar result string\nif err := future.get(ctx, &result); err != nil {\n    return err\n}\n\n\nlet\'s take a look at each component of this call.\n\n# activity options\nbefore calling workflow.executeactivity(), you must configure activityoptions for the invocation. these options customize various execution timeouts, and are passed in by creating a child context from the initial context and overwriting the desired values. the child context is then passed into the workflow.executeactivity() call. if multiple are sharing the same option values, then the same context instance can be used when calling workflow.executeactivity().\n\n# activity timeouts\nthere can be various kinds of timeouts associated with an . cadence guarantees that are executed at most once, so an either succeeds or fails with one of the following timeouts:\n\ntimeout                  description                                                                                                                                                                   \nstarttoclosetimeout      maximum time that a worker can take to process a task after it has received the task.                                                                                         \nscheduletostarttimeout   time a task can wait to be picked up by an after a schedules it. if there are no workers available to process this task for the specified duration, the task will time out.   \nscheduletoclosetimeout   time a task can take to complete after it is scheduled by a . this is usually greater than the sum of starttoclose and scheduletostart timeouts.                              \nheartbeattimeout         if a task doesn\'t heartbeat to the cadence service for this duration, it will be considered to have failed. this is useful for long-running tasks.                            \n\n# executeactivity call\nthe first parameter in the call is the required cadence.context object. this type is a copy ofcontext.context with the done() method returning cadence.channel instead of the native go chan.\n\nthe second parameter is the function that we registered as an function. this parameter can also be a string representing the fully qualified name of the function. the benefit of passing in the actual function object is that the framework can validate parameters.\n\nthe remaining parameters are passed to the as part of the call. in our example, we have a single parameter: value. this list of parameters must match the list of parameters declared by the function. the cadence client library will validate this.\n\nthe method call returns immediately and returns a cadence.future. this allows you to execute more code without having to wait for the scheduled to complete.\n\nwhen you are ready to process the results of the , call the get() method on the future object returned. the parameters to this method are the ctx object we passed to theworkflow.executeactivity() call and an output parameter that will receive the output of the. the type of the output parameter must match the type of the return value declared by the function. the get() method will block until the completes and results are available.\n\nyou can retrieve the result value returned by workflow.executeactivity() from the future and use it like any normal result from a synchronous function call. the following sample code demonstrates how you can use the result if it is a string value:\n\nvar result string\nif err := future.get(ctx1, &result); err != nil {\n    return err\n}\n\nswitch result {\ncase "apple":\n    // do something.\ncase "banana":\n    // do something.\ndefault:\n    return err\n}\n\n\nin this example, we called the get() method on the returned future immediately after workflow.executeactivity(). however, this is not necessary. if you want to execute multiple in parallel, you can repeatedly call workflow.executeactivity(), store the returned futures, and then wait for all to complete by calling the get() methods of the future at a later time.\n\nto implement more complex wait conditions on returned future objects, use the cadence.selector class.',charsets:{}},{title:"Child workflows",frontmatter:{layout:"default",title:"Child workflows",permalink:"/docs/go-client/child-workflows",readingShow:"top"},regularPath:"/docs/05-go-client/05-child-workflows.html",relativePath:"docs/05-go-client/05-child-workflows.md",key:"v-30ee6212",path:"/docs/go-client/child-workflows/",headersStr:null,content:'# Child workflows\nworkflow.ExecuteChildWorkflow enables the scheduling of other from within a \'s implementation. The parent has the ability to monitor and impact the lifecycle of the child, similar to the way it does for an that it invoked.\n\ncwo := workflow.ChildWorkflowOptions{\n    // Do not specify WorkflowID if you want Cadence to generate a unique ID for the child execution.\n    WorkflowID:                   "BID-SIMPLE-CHILD-WORKFLOW",\n    ExecutionStartToCloseTimeout: time.Minute * 30,\n}\nctx = workflow.WithChildWorkflowOptions(ctx, cwo)\n\nvar result string\nfuture := workflow.ExecuteChildWorkflow(ctx, SimpleChildWorkflow, value)\nif err := future.Get(ctx, &result); err != nil {\n    workflow.GetLogger(ctx).Error("SimpleChildWorkflow failed.", zap.Error(err))\n    return err\n}\n\n\nLet\'s take a look at each component of this call.\n\nBefore calling workflow.ExecuteChildworkflow(), you must configure ChildWorkflowOptions for the invocation. These options customize various execution timeouts, and are passed in by creating a child context from the initial context and overwriting the desired values. The child context is then passed into the workflow.ExecuteChildWorkflow() call. If multiple child are sharing the same option values, then the same context instance can be used when calling workflow.ExecuteChildworkflow().\n\nThe first parameter in the call is the required cadence.Context object. This type is a copy ofcontext.Context with the Done() method returning cadence.Channel instead of the native Go chan.\n\nThe second parameter is the function that we registered as a function. This parameter can also be a string representing the fully qualified name of the function. The benefit of this is that when you pass in the actual function object, the framework can validate parameters.\n\nThe remaining parameters are passed to the as part of the call. In our example, we have a single parameter: value. This list of parameters must match the list of parameters declared by the function.\n\nThe method call returns immediately and returns a cadence.Future. This allows you to execute more code without having to wait for the scheduled to complete.\n\nWhen you are ready to process the results of the , call the Get() method on the returned future object. The parameters to this method is the ctx object we passed to theworkflow.ExecuteChildWorkflow() call and an output parameter that will receive the output of the. The type of the output parameter must match the type of the return value declared by the function. The Get() method will block until the completes and results are available.\n\nThe workflow.ExecuteChildWorkflow() function is similar to workflow.ExecuteActivity(). All of the patterns described for using workflow.ExecuteActivity() apply to the workflow.ExecuteChildWorkflow()function as well.\n\nWhen a parent is cancelled by the user, the child can be cancelled or abandoned based on a configurable child policy.',normalizedContent:'# child workflows\nworkflow.executechildworkflow enables the scheduling of other from within a \'s implementation. the parent has the ability to monitor and impact the lifecycle of the child, similar to the way it does for an that it invoked.\n\ncwo := workflow.childworkflowoptions{\n    // do not specify workflowid if you want cadence to generate a unique id for the child execution.\n    workflowid:                   "bid-simple-child-workflow",\n    executionstarttoclosetimeout: time.minute * 30,\n}\nctx = workflow.withchildworkflowoptions(ctx, cwo)\n\nvar result string\nfuture := workflow.executechildworkflow(ctx, simplechildworkflow, value)\nif err := future.get(ctx, &result); err != nil {\n    workflow.getlogger(ctx).error("simplechildworkflow failed.", zap.error(err))\n    return err\n}\n\n\nlet\'s take a look at each component of this call.\n\nbefore calling workflow.executechildworkflow(), you must configure childworkflowoptions for the invocation. these options customize various execution timeouts, and are passed in by creating a child context from the initial context and overwriting the desired values. the child context is then passed into the workflow.executechildworkflow() call. if multiple child are sharing the same option values, then the same context instance can be used when calling workflow.executechildworkflow().\n\nthe first parameter in the call is the required cadence.context object. this type is a copy ofcontext.context with the done() method returning cadence.channel instead of the native go chan.\n\nthe second parameter is the function that we registered as a function. this parameter can also be a string representing the fully qualified name of the function. the benefit of this is that when you pass in the actual function object, the framework can validate parameters.\n\nthe remaining parameters are passed to the as part of the call. in our example, we have a single parameter: value. this list of parameters must match the list of parameters declared by the function.\n\nthe method call returns immediately and returns a cadence.future. this allows you to execute more code without having to wait for the scheduled to complete.\n\nwhen you are ready to process the results of the , call the get() method on the returned future object. the parameters to this method is the ctx object we passed to theworkflow.executechildworkflow() call and an output parameter that will receive the output of the. the type of the output parameter must match the type of the return value declared by the function. the get() method will block until the completes and results are available.\n\nthe workflow.executechildworkflow() function is similar to workflow.executeactivity(). all of the patterns described for using workflow.executeactivity() apply to the workflow.executechildworkflow()function as well.\n\nwhen a parent is cancelled by the user, the child can be cancelled or abandoned based on a configurable child policy.',charsets:{}},{title:"Activity and workflow retries",frontmatter:{layout:"default",title:"Activity and workflow retries",permalink:"/docs/go-client/retries",readingShow:"top"},regularPath:"/docs/05-go-client/06-retries.html",relativePath:"docs/05-go-client/06-retries.md",key:"v-63bf2e6c",path:"/docs/go-client/retries/",headersStr:null,content:"# Activity and workflow retries\n and can fail due to various intermediate conditions. In those cases, we want to retry the failed or child or even the parent . This can be achieved by supplying an optional retry policy. A retry policy looks like the following:\n\n// RetryPolicy defines the retry policy.\nRetryPolicy struct {\n    // Backoff interval for the first retry. If coefficient is 1.0 then it is used for all retries.\n    // Required, no default value.\n    InitialInterval time.Duration\n\n    // Coefficient used to calculate the next retry backoff interval.\n    // The next retry interval is previous interval multiplied by this coefficient.\n    // Must be 1 or larger. Default is 2.0.\n    BackoffCoefficient float64\n\n    // Maximum backoff interval between retries. Exponential backoff leads to interval increase.\n    // This value is the cap of the interval. Default is 100x of initial interval.\n    MaximumInterval time.Duration\n\n    // Maximum time to retry. Either ExpirationInterval or MaximumAttempts is required.\n    // When exceeded the retries stop even if maximum retries is not reached yet.\n    ExpirationInterval time.Duration\n\n    // Maximum number of attempts. When exceeded the retries stop even if not expired yet.\n    // If not set or set to 0, it means unlimited, and relies on ExpirationInterval to stop.\n    // Either MaximumAttempts or ExpirationInterval is required.\n    MaximumAttempts int32\n\n    // Non-Retriable errors. This is optional. Cadence server will stop retry if error reason matches this list.\n    // Error reason for custom error is specified when your activity/workflow returns cadence.NewCustomError(reason).\n    // Error reason for panic error is \"cadenceInternal:Panic\".\n    // Error reason for any other error is \"cadenceInternal:Generic\".\n    // Error reason for timeouts is: \"cadenceInternal:Timeout TIMEOUT_TYPE\". TIMEOUT_TYPE could be START_TO_CLOSE or HEARTBEAT.\n    // Note that cancellation is not a failure, so it won't be retried.\n    NonRetriableErrorReasons []string\n}\n\n\nTo enable retry, supply a custom retry policy to ActivityOptions or ChildWorkflowOptionswhen you execute them.\n\nexpiration := time.Minute * 10\nretryPolicy := &cadence.RetryPolicy{\n    InitialInterval:    time.Second,\n    BackoffCoefficient: 2,\n    MaximumInterval:    expiration,\n    ExpirationInterval: time.Minute * 10,\n    MaximumAttempts:    5,\n}\nao := workflow.ActivityOptions{\n    ScheduleToStartTimeout: expiration,\n    StartToCloseTimeout:    expiration,\n    HeartbeatTimeout:       time.Second * 30,\n    RetryPolicy:            retryPolicy, // Enable retry.\n}\nctx = workflow.WithActivityOptions(ctx, ao)\nactivityFuture := workflow.ExecuteActivity(ctx, SampleActivity, params)\n\n\nIf heartbeat its progress before it failed, the retry attempt will contain the progress so implementation could resume from failed progress like:\n\nfunc SampleActivity(ctx context.Context, inputArg InputParams) error {\n    startIdx := inputArg.StartIndex\n    if activity.HasHeartbeatDetails(ctx) {\n        // Recover from finished progress.\n        var finishedIndex int\n        if err := activity.GetHeartbeatDetails(ctx, &finishedIndex); err == nil {\n            startIdx = finishedIndex + 1 // Start from next one.\n        }\n    }\n\n    // Normal activity logic...\n    for i:=startIdx; i<inputArg.EndIdx; i++ {\n        // Code for processing item i goes here...\n        activity.RecordHeartbeat(ctx, i) // Report progress.\n    }\n}\n\n\nLike retry for an , you need to supply a retry policy for ChildWorkflowOptions to enable retry for a child . To enable retry for a parent , supply a retry policy when you start a via StartWorkflowOptions.\n\nThere are some subtle changes to 's history when RetryPolicy is used. For an with RetryPolicy:\n\n * The ActivityTaskScheduledEvent will have extended ScheduleToStartTimeout and ScheduleToCloseTimeout. These two timeouts will be overwritten by the server to be as long as the retry policy's ExpirationInterval. If the ExpirationIntervalis not specified, it will be overwritten to the 's timeout.\n * The ActivityTaskStartedEvent will not show up in history until the is completed or failed with no more retry. This is to avoid recording the ActivityTaskStarted but later it failed and retried. Using the DescribeWorkflowExecutionAPI will return the PendingActivityInfo and it will contain attemptCount if it is retrying.\n\nFor a with RetryPolicy:\n\n * If a failed and needs to retry, the will be closed with a ContinueAsNew . The will have the ContinueAsNewInitiator set to RetryPolicy and the new RunID for the next retry attempt.\n * The new attempt will be created immediately. But the first won't be scheduled until the backoff duration which is also recorded in the new run's WorkflowExecutionStartedEventAttributes as firstDecisionTaskBackoffSeconds.",normalizedContent:"# activity and workflow retries\n and can fail due to various intermediate conditions. in those cases, we want to retry the failed or child or even the parent . this can be achieved by supplying an optional retry policy. a retry policy looks like the following:\n\n// retrypolicy defines the retry policy.\nretrypolicy struct {\n    // backoff interval for the first retry. if coefficient is 1.0 then it is used for all retries.\n    // required, no default value.\n    initialinterval time.duration\n\n    // coefficient used to calculate the next retry backoff interval.\n    // the next retry interval is previous interval multiplied by this coefficient.\n    // must be 1 or larger. default is 2.0.\n    backoffcoefficient float64\n\n    // maximum backoff interval between retries. exponential backoff leads to interval increase.\n    // this value is the cap of the interval. default is 100x of initial interval.\n    maximuminterval time.duration\n\n    // maximum time to retry. either expirationinterval or maximumattempts is required.\n    // when exceeded the retries stop even if maximum retries is not reached yet.\n    expirationinterval time.duration\n\n    // maximum number of attempts. when exceeded the retries stop even if not expired yet.\n    // if not set or set to 0, it means unlimited, and relies on expirationinterval to stop.\n    // either maximumattempts or expirationinterval is required.\n    maximumattempts int32\n\n    // non-retriable errors. this is optional. cadence server will stop retry if error reason matches this list.\n    // error reason for custom error is specified when your activity/workflow returns cadence.newcustomerror(reason).\n    // error reason for panic error is \"cadenceinternal:panic\".\n    // error reason for any other error is \"cadenceinternal:generic\".\n    // error reason for timeouts is: \"cadenceinternal:timeout timeout_type\". timeout_type could be start_to_close or heartbeat.\n    // note that cancellation is not a failure, so it won't be retried.\n    nonretriableerrorreasons []string\n}\n\n\nto enable retry, supply a custom retry policy to activityoptions or childworkflowoptionswhen you execute them.\n\nexpiration := time.minute * 10\nretrypolicy := &cadence.retrypolicy{\n    initialinterval:    time.second,\n    backoffcoefficient: 2,\n    maximuminterval:    expiration,\n    expirationinterval: time.minute * 10,\n    maximumattempts:    5,\n}\nao := workflow.activityoptions{\n    scheduletostarttimeout: expiration,\n    starttoclosetimeout:    expiration,\n    heartbeattimeout:       time.second * 30,\n    retrypolicy:            retrypolicy, // enable retry.\n}\nctx = workflow.withactivityoptions(ctx, ao)\nactivityfuture := workflow.executeactivity(ctx, sampleactivity, params)\n\n\nif heartbeat its progress before it failed, the retry attempt will contain the progress so implementation could resume from failed progress like:\n\nfunc sampleactivity(ctx context.context, inputarg inputparams) error {\n    startidx := inputarg.startindex\n    if activity.hasheartbeatdetails(ctx) {\n        // recover from finished progress.\n        var finishedindex int\n        if err := activity.getheartbeatdetails(ctx, &finishedindex); err == nil {\n            startidx = finishedindex + 1 // start from next one.\n        }\n    }\n\n    // normal activity logic...\n    for i:=startidx; i<inputarg.endidx; i++ {\n        // code for processing item i goes here...\n        activity.recordheartbeat(ctx, i) // report progress.\n    }\n}\n\n\nlike retry for an , you need to supply a retry policy for childworkflowoptions to enable retry for a child . to enable retry for a parent , supply a retry policy when you start a via startworkflowoptions.\n\nthere are some subtle changes to 's history when retrypolicy is used. for an with retrypolicy:\n\n * the activitytaskscheduledevent will have extended scheduletostarttimeout and scheduletoclosetimeout. these two timeouts will be overwritten by the server to be as long as the retry policy's expirationinterval. if the expirationintervalis not specified, it will be overwritten to the 's timeout.\n * the activitytaskstartedevent will not show up in history until the is completed or failed with no more retry. this is to avoid recording the activitytaskstarted but later it failed and retried. using the describeworkflowexecutionapi will return the pendingactivityinfo and it will contain attemptcount if it is retrying.\n\nfor a with retrypolicy:\n\n * if a failed and needs to retry, the will be closed with a continueasnew . the will have the continueasnewinitiator set to retrypolicy and the new runid for the next retry attempt.\n * the new attempt will be created immediately. but the first won't be scheduled until the backoff duration which is also recorded in the new run's workflowexecutionstartedeventattributes as firstdecisiontaskbackoffseconds.",charsets:{}},{title:"Error handling",frontmatter:{layout:"default",title:"Error handling",permalink:"/docs/go-client/error-handling",readingShow:"top"},regularPath:"/docs/05-go-client/07-error-handling.html",relativePath:"docs/05-go-client/07-error-handling.md",key:"v-103bbcbc",path:"/docs/go-client/error-handling/",headersStr:null,content:'# Error handling\nAn , or child , might fail and you could handle errors differently based on different error cases. If the returns an error as errors.New() or fmt.Errorf(), those errors will be converted to workflow.GenericError. If the returns an error ascadence.NewCustomError(“err-reason”, details), that error will be converted to *cadence.CustomError. There are other types of errors such as workflow.TimeoutError, workflow.CanceledError andworkflow.PanicError. Following is an example of what your error code might look like:\n\nerr := workflow.ExecuteActivity(ctx, YourActivityFunc).Get(ctx, nil)\nswitch err := err.(type) {\n    case *cadence.CustomError:\n        switch err.Reason() {\n            case "err-reason-a":\n                // Handle error-reason-a.\n                var details YourErrorDetailsType\n                err.Details(&details)\n                // Deal with details.\n            case "err-reason-b":\n                // Handle error-reason-b.\n            default:\n                // Handle all other error reasons.\n        }\n    case *workflow.GenericError:\n        switch err.Error() {\n            case "err-msg-1":\n                // Handle error with message "err-msg-1".\n            case "err-msg-2":\n                // Handle error with message "err-msg-2".\n            default:\n                // Handle all other generic errors.\n        }\n    case *workflow.TimeoutError:\n        switch err.TimeoutType() {\n            case shared.TimeoutTypeScheduleToStart:\n                // Handle ScheduleToStart timeout.\n            case shared.TimeoutTypeStartToClose:\n                // Handle StartToClose timeout.\n            case shared.TimeoutTypeHeartbeat:\n                // Handle heartbeat timeout.\n            default:\n        }\n    case *workflow.PanicError:\n        // Handle panic error.\n    case *cadence.CanceledError:\n        // Handle canceled error.\n    default:\n        // All other cases (ideally, this should not happen).\n}',normalizedContent:'# error handling\nan , or child , might fail and you could handle errors differently based on different error cases. if the returns an error as errors.new() or fmt.errorf(), those errors will be converted to workflow.genericerror. if the returns an error ascadence.newcustomerror(“err-reason”, details), that error will be converted to *cadence.customerror. there are other types of errors such as workflow.timeouterror, workflow.cancelederror andworkflow.panicerror. following is an example of what your error code might look like:\n\nerr := workflow.executeactivity(ctx, youractivityfunc).get(ctx, nil)\nswitch err := err.(type) {\n    case *cadence.customerror:\n        switch err.reason() {\n            case "err-reason-a":\n                // handle error-reason-a.\n                var details yourerrordetailstype\n                err.details(&details)\n                // deal with details.\n            case "err-reason-b":\n                // handle error-reason-b.\n            default:\n                // handle all other error reasons.\n        }\n    case *workflow.genericerror:\n        switch err.error() {\n            case "err-msg-1":\n                // handle error with message "err-msg-1".\n            case "err-msg-2":\n                // handle error with message "err-msg-2".\n            default:\n                // handle all other generic errors.\n        }\n    case *workflow.timeouterror:\n        switch err.timeouttype() {\n            case shared.timeouttypescheduletostart:\n                // handle scheduletostart timeout.\n            case shared.timeouttypestarttoclose:\n                // handle starttoclose timeout.\n            case shared.timeouttypeheartbeat:\n                // handle heartbeat timeout.\n            default:\n        }\n    case *workflow.panicerror:\n        // handle panic error.\n    case *cadence.cancelederror:\n        // handle canceled error.\n    default:\n        // all other cases (ideally, this should not happen).\n}',charsets:{}},{title:"Signals",frontmatter:{layout:"default",title:"Signals",permalink:"/docs/go-client/signals",readingShow:"top"},regularPath:"/docs/05-go-client/08-signals.html",relativePath:"docs/05-go-client/08-signals.md",key:"v-65ea467c",path:"/docs/go-client/signals/",headers:[{level:2,title:"SignalWithStart",slug:"signalwithstart",normalizedTitle:"signalwithstart",charIndex:1650}],headersStr:"SignalWithStart",content:'# Signals\n provide a mechanism to send data directly to a running . Previously, you had two options for passing data to the implementation:\n\n * Via start parameters\n * As return values from \n\nWith start parameters, we could only pass in values before began.\n\nReturn values from allowed us to pass information to a running , but this approach comes with its own complications. One major drawback is reliance on polling. This means that the data needs to be stored in a third-party location until it\'s ready to be picked up by the . Further, the lifecycle of this requires management, and the requires manual restart if it fails before acquiring the data.\n\n, on the other hand, provide a fully asynchronous and durable mechanism for providing data to a running . When a is received for a running , Cadence persists the and the payload in the history. The can then process the at any time afterwards without the risk of losing the information. The also has the option to stop execution by blocking on a channel.\n\nvar signalVal string\nsignalChan := workflow.GetSignalChannel(ctx, signalName)\n\ns := workflow.NewSelector(ctx)\ns.AddReceive(signalChan, func(c workflow.Channel, more bool) {\n    c.Receive(ctx, &signalVal)\n    workflow.GetLogger(ctx).Info("Received signal!", zap.String("signal", signalName), zap.String("value", signalVal))\n})\ns.Select(ctx)\n\nif len(signalVal) > 0 && signalVal != "SOME_VALUE" {\n    return errors.New("signalVal")\n}\n\n\nIn the example above, the code uses workflow.GetSignalChannel to open aworkflow.Channel for the named . We then use a workflow.Selector to wait on this channel and process the payload received with the .\n\n# SignalWithStart\nYou may not know if a is running and can accept a . Theclient.SignalWithStartWorkflow API allows you to send a to the current instance if one exists or to create a new run and then send the . SignalWithStartWorkflow therefore doesn\'t take a as a parameter.',normalizedContent:'# signals\n provide a mechanism to send data directly to a running . previously, you had two options for passing data to the implementation:\n\n * via start parameters\n * as return values from \n\nwith start parameters, we could only pass in values before began.\n\nreturn values from allowed us to pass information to a running , but this approach comes with its own complications. one major drawback is reliance on polling. this means that the data needs to be stored in a third-party location until it\'s ready to be picked up by the . further, the lifecycle of this requires management, and the requires manual restart if it fails before acquiring the data.\n\n, on the other hand, provide a fully asynchronous and durable mechanism for providing data to a running . when a is received for a running , cadence persists the and the payload in the history. the can then process the at any time afterwards without the risk of losing the information. the also has the option to stop execution by blocking on a channel.\n\nvar signalval string\nsignalchan := workflow.getsignalchannel(ctx, signalname)\n\ns := workflow.newselector(ctx)\ns.addreceive(signalchan, func(c workflow.channel, more bool) {\n    c.receive(ctx, &signalval)\n    workflow.getlogger(ctx).info("received signal!", zap.string("signal", signalname), zap.string("value", signalval))\n})\ns.select(ctx)\n\nif len(signalval) > 0 && signalval != "some_value" {\n    return errors.new("signalval")\n}\n\n\nin the example above, the code uses workflow.getsignalchannel to open aworkflow.channel for the named . we then use a workflow.selector to wait on this channel and process the payload received with the .\n\n# signalwithstart\nyou may not know if a is running and can accept a . theclient.signalwithstartworkflow api allows you to send a to the current instance if one exists or to create a new run and then send the . signalwithstartworkflow therefore doesn\'t take a as a parameter.',charsets:{}},{title:"Side effect",frontmatter:{layout:"default",title:"Side effect",permalink:"/docs/go-client/side-effect",readingShow:"top"},regularPath:"/docs/05-go-client/10-side-effect.html",relativePath:"docs/05-go-client/10-side-effect.md",key:"v-3c7b0dd4",path:"/docs/go-client/side-effect/",headersStr:null,content:'# Side effect\nworkflow.SideEffect is useful for short, nondeterministic code snippets, such as getting a random value or generating a UUID. It executes the provided function once and records its result into the history. workflow.SideEffect does not re-execute upon replay, but instead returns the recorded result. It can be seen as an "inline" . Something to note about workflow.SideEffectis that, unlike the Cadence guarantee of at-most-once execution for , there is no such guarantee with workflow.SideEffect. Under certain failure conditions, workflow.SideEffect can end up executing a function more than once.\n\nThe only way to fail SideEffect is to panic, which causes a failure. After the timeout, Cadence reschedules and then re-executes the , giving SideEffect another chance to succeed. Do not return any data from SideEffect other than through its recorded return value.\n\nThe following sample demonstrates how to use SideEffect:\n\nencodedRandom := SideEffect(func(ctx cadence.Context) interface{} {\n    return rand.Intn(100)\n})\n\nvar random int\nencodedRandom.Get(&random)\nif random < 50 {\n    ...\n} else {\n    ...\n}',normalizedContent:'# side effect\nworkflow.sideeffect is useful for short, nondeterministic code snippets, such as getting a random value or generating a uuid. it executes the provided function once and records its result into the history. workflow.sideeffect does not re-execute upon replay, but instead returns the recorded result. it can be seen as an "inline" . something to note about workflow.sideeffectis that, unlike the cadence guarantee of at-most-once execution for , there is no such guarantee with workflow.sideeffect. under certain failure conditions, workflow.sideeffect can end up executing a function more than once.\n\nthe only way to fail sideeffect is to panic, which causes a failure. after the timeout, cadence reschedules and then re-executes the , giving sideeffect another chance to succeed. do not return any data from sideeffect other than through its recorded return value.\n\nthe following sample demonstrates how to use sideeffect:\n\nencodedrandom := sideeffect(func(ctx cadence.context) interface{} {\n    return rand.intn(100)\n})\n\nvar random int\nencodedrandom.get(&random)\nif random < 50 {\n    ...\n} else {\n    ...\n}',charsets:{}},{title:"Continue as new",frontmatter:{layout:"default",title:"Continue as new",permalink:"/docs/go-client/continue-as-new",readingShow:"top"},regularPath:"/docs/05-go-client/09-continue-as-new.html",relativePath:"docs/05-go-client/09-continue-as-new.md",key:"v-b60e670c",path:"/docs/go-client/continue-as-new/",headersStr:null,content:"# Continue as new\n that need to rerun periodically could naively be implemented as a big for loop with a sleep where the entire logic of the is inside the body of the for loop. The problem with this approach is that the history for that will keep growing to a point where it reaches the maximum size enforced by the service.\n\nContinueAsNew is the low level construct that enables implementing such without the risk of failures down the road. The operation atomically completes the current execution and starts a new execution of the with the same . The new execution will not carry over any history from the old execution. To trigger this behavior, the function should terminate by returning the special ContinueAsNewError error:\n\nfunc SimpleWorkflow(workflow.Context ctx, value string) error {\n    ...\n    return workflow.NewContinueAsNewError(ctx, SimpleWorkflow, value)\n}",normalizedContent:"# continue as new\n that need to rerun periodically could naively be implemented as a big for loop with a sleep where the entire logic of the is inside the body of the for loop. the problem with this approach is that the history for that will keep growing to a point where it reaches the maximum size enforced by the service.\n\ncontinueasnew is the low level construct that enables implementing such without the risk of failures down the road. the operation atomically completes the current execution and starts a new execution of the with the same . the new execution will not carry over any history from the old execution. to trigger this behavior, the function should terminate by returning the special continueasnewerror error:\n\nfunc simpleworkflow(workflow.context ctx, value string) error {\n    ...\n    return workflow.newcontinueasnewerror(ctx, simpleworkflow, value)\n}",charsets:{}},{title:"Queries",frontmatter:{layout:"default",title:"Queries",permalink:"/docs/go-client/queries",readingShow:"top"},regularPath:"/docs/05-go-client/11-queries.html",relativePath:"docs/05-go-client/11-queries.md",key:"v-a558de54",path:"/docs/go-client/queries/",headers:[{level:2,title:"Consistent Query",slug:"consistent-query",normalizedTitle:"consistent query",charIndex:2006}],headersStr:"Consistent Query",content:'# Queries\nIf a has been stuck at a state for longer than an expected period of time, you might want to the current call stack. You can use the Cadence to perform this . For example:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt __stack_trace\n\nThis command uses __stack_trace, which is a built-in type supported by the Cadence client library. You can add custom types to handle such as the current state of a, or how many the has completed. To do this, you need to set up a handler using workflow.SetQueryHandler.\n\nThe handler must be a function that returns two values:\n\n 1. A serializable result\n 2. An error\n\nThe handler function can receive any number of input parameters, but all input parameters must be serializable. The following sample code sets up a handler that handles the type ofcurrent_state:\n\nfunc MyWorkflow(ctx workflow.Context, input string) error {\n    currentState := "started" // This could be any serializable struct.\n    err := workflow.SetQueryHandler(ctx, "current_state", func() (string, error) {\n        return currentState, nil\n    })\n    if err != nil {\n        currentState = "failed to register query handler"\n        return err\n    }\n    // Your normal workflow code begins here, and you update the currentState as the code makes progress.\n    currentState = "waiting timer"\n    err = NewTimer(ctx, time.Hour).Get(ctx, nil)\n    if err != nil {\n        currentState = "timer failed"\n        return err\n    }\n\n    currentState = "waiting activity"\n    ctx = WithActivityOptions(ctx, myActivityOptions)\n    err = ExecuteActivity(ctx, MyActivity, "my_input").Get(ctx, nil)\n    if err != nil {\n        currentState = "activity failed"\n        return err\n    }\n    currentState = "done"\n    return nil\n}\n\n\nYou can now current_state by using the \n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nYou can also issue a from code using the QueryWorkflow() API on a Cadence client object.\n\n# Consistent Query\n has two consistency levels, eventual and strong. Consider if you were to a and then immediately the \n\ncadence-cli --domain samples-domain workflow signal -w my_workflow_id -r my_run_id -n signal_name -if ./input.json\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nIn this example if were to change state, may or may not see that state update reflected in the result. This is what it means for to be eventually consistent.\n\n has another consistency level called strong consistency. A strongly consistent is guaranteed to be based on state which includes all that came before the was issued. An is considered to have come before a if the call creating the external returned success before the was issued. External which are created while the is outstanding may or may not be reflected in the state the result is based on.\n\nIn order to run consistent through the do the following:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state --qcl strong\n\nIn order to run a using the go client do the following:\n\nresp, err := cadenceClient.QueryWorkflowWithOptions(ctx, &client.QueryWorkflowWithOptionsRequest{\n    WorkflowID:            workflowID,\n    RunID:                 runID,\n    QueryType:             queryType,\n    QueryConsistencyLevel: shared.QueryConsistencyLevelStrong.Ptr(),\n})\n\n\nWhen using strongly consistent you should expect higher latency than eventually consistent .',normalizedContent:'# queries\nif a has been stuck at a state for longer than an expected period of time, you might want to the current call stack. you can use the cadence to perform this . for example:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt __stack_trace\n\nthis command uses __stack_trace, which is a built-in type supported by the cadence client library. you can add custom types to handle such as the current state of a, or how many the has completed. to do this, you need to set up a handler using workflow.setqueryhandler.\n\nthe handler must be a function that returns two values:\n\n 1. a serializable result\n 2. an error\n\nthe handler function can receive any number of input parameters, but all input parameters must be serializable. the following sample code sets up a handler that handles the type ofcurrent_state:\n\nfunc myworkflow(ctx workflow.context, input string) error {\n    currentstate := "started" // this could be any serializable struct.\n    err := workflow.setqueryhandler(ctx, "current_state", func() (string, error) {\n        return currentstate, nil\n    })\n    if err != nil {\n        currentstate = "failed to register query handler"\n        return err\n    }\n    // your normal workflow code begins here, and you update the currentstate as the code makes progress.\n    currentstate = "waiting timer"\n    err = newtimer(ctx, time.hour).get(ctx, nil)\n    if err != nil {\n        currentstate = "timer failed"\n        return err\n    }\n\n    currentstate = "waiting activity"\n    ctx = withactivityoptions(ctx, myactivityoptions)\n    err = executeactivity(ctx, myactivity, "my_input").get(ctx, nil)\n    if err != nil {\n        currentstate = "activity failed"\n        return err\n    }\n    currentstate = "done"\n    return nil\n}\n\n\nyou can now current_state by using the \n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nyou can also issue a from code using the queryworkflow() api on a cadence client object.\n\n# consistent query\n has two consistency levels, eventual and strong. consider if you were to a and then immediately the \n\ncadence-cli --domain samples-domain workflow signal -w my_workflow_id -r my_run_id -n signal_name -if ./input.json\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state\n\nin this example if were to change state, may or may not see that state update reflected in the result. this is what it means for to be eventually consistent.\n\n has another consistency level called strong consistency. a strongly consistent is guaranteed to be based on state which includes all that came before the was issued. an is considered to have come before a if the call creating the external returned success before the was issued. external which are created while the is outstanding may or may not be reflected in the state the result is based on.\n\nin order to run consistent through the do the following:\n\ncadence-cli --domain samples-domain workflow query -w my_workflow_id -r my_run_id -qt current_state --qcl strong\n\nin order to run a using the go client do the following:\n\nresp, err := cadenceclient.queryworkflowwithoptions(ctx, &client.queryworkflowwithoptionsrequest{\n    workflowid:            workflowid,\n    runid:                 runid,\n    querytype:             querytype,\n    queryconsistencylevel: shared.queryconsistencylevelstrong.ptr(),\n})\n\n\nwhen using strongly consistent you should expect higher latency than eventually consistent .',charsets:{}},{title:"Async activity completion",frontmatter:{layout:"default",title:"Async activity completion",permalink:"/docs/go-client/activity-async-completion",readingShow:"top"},regularPath:"/docs/05-go-client/12-activity-async-completion.html",relativePath:"docs/05-go-client/12-activity-async-completion.md",key:"v-5b7bae8a",path:"/docs/go-client/activity-async-completion/",headersStr:null,content:'# Asynchronous activity completion\nThere are certain scenarios when completing an upon completion of its function is not possible or desirable. For example, you might have an application that requires user input in order to complete the . You could implement the with a polling mechanism, but a simpler and less resource-intensive implementation is to asynchronously complete a Cadence .\n\nThere two parts to implementing an asynchronously completed activity:\n\n 1. The provides the information necessary for completion from an external system and notifies the Cadence service that it is waiting for that outside callback.\n 2. The external service calls the Cadence service to complete the .\n\nThe following example demonstrates the first part:\n\n// Retrieve the activity information needed to asynchronously complete the activity.\nactivityInfo := cadence.GetActivityInfo(ctx)\ntaskToken := activityInfo.TaskToken\n\n// Send the taskToken to the external service that will complete the activity.\n...\n\n// Return from the activity a function indicating that Cadence should wait for an async completion\n// message.\nreturn "", activity.ErrResultPending\n\n\nThe following code demonstrates how to complete the successfully:\n\n// Instantiate a Cadence service client.\n// The same client can be used to complete or fail any number of activities.\ncadence.Client client = cadence.NewClient(...)\n\n// Complete the activity.\nclient.CompleteActivity(taskToken, result, nil)\n\n\nTo fail the , you would do the following:\n\n// Fail the activity.\nclient.CompleteActivity(taskToken, nil, err)\n\n\nFollowing are the parameters of the CompleteActivity function:\n\n * taskToken: The value of the binary TaskToken field of the ActivityInfo struct retrieved inside the .\n * result: The return value to record for the . The type of this value must match the type of the return value declared by the function.\n * err: The error code to return if the terminates with an error.\n\nIf error is not null, the value of the result field is ignored.',normalizedContent:'# asynchronous activity completion\nthere are certain scenarios when completing an upon completion of its function is not possible or desirable. for example, you might have an application that requires user input in order to complete the . you could implement the with a polling mechanism, but a simpler and less resource-intensive implementation is to asynchronously complete a cadence .\n\nthere two parts to implementing an asynchronously completed activity:\n\n 1. the provides the information necessary for completion from an external system and notifies the cadence service that it is waiting for that outside callback.\n 2. the external service calls the cadence service to complete the .\n\nthe following example demonstrates the first part:\n\n// retrieve the activity information needed to asynchronously complete the activity.\nactivityinfo := cadence.getactivityinfo(ctx)\ntasktoken := activityinfo.tasktoken\n\n// send the tasktoken to the external service that will complete the activity.\n...\n\n// return from the activity a function indicating that cadence should wait for an async completion\n// message.\nreturn "", activity.errresultpending\n\n\nthe following code demonstrates how to complete the successfully:\n\n// instantiate a cadence service client.\n// the same client can be used to complete or fail any number of activities.\ncadence.client client = cadence.newclient(...)\n\n// complete the activity.\nclient.completeactivity(tasktoken, result, nil)\n\n\nto fail the , you would do the following:\n\n// fail the activity.\nclient.completeactivity(tasktoken, nil, err)\n\n\nfollowing are the parameters of the completeactivity function:\n\n * tasktoken: the value of the binary tasktoken field of the activityinfo struct retrieved inside the .\n * result: the return value to record for the . the type of this value must match the type of the return value declared by the function.\n * err: the error code to return if the terminates with an error.\n\nif error is not null, the value of the result field is ignored.',charsets:{}},{title:"Testing",frontmatter:{layout:"default",title:"Testing",permalink:"/docs/go-client/workflow-testing",readingShow:"top"},regularPath:"/docs/05-go-client/13-workflow-testing.html",relativePath:"docs/05-go-client/13-workflow-testing.md",key:"v-722c1fc2",path:"/docs/go-client/workflow-testing/",headers:[{level:2,title:"Setup",slug:"setup",normalizedTitle:"setup",charIndex:618},{level:2,title:"A Simple Test",slug:"a-simple-test",normalizedTitle:"a simple test",charIndex:2852},{level:2,title:"Activity mocking and overriding",slug:"activity-mocking-and-overriding",normalizedTitle:"activity mocking and overriding",charIndex:3527}],headersStr:"Setup A Simple Test Activity mocking and overriding",content:'# Testing\nThe Cadence Go client library provides a test framework to facilitate testing implementations. The framework is suited for implementing unit tests as well as functional tests of the logic.\n\nThe following code implements unit tests for the SimpleWorkflow sample:\n\npackage sample\n\nimport (\n    "errors"\n    "testing"\n\n    "github.com/stretchr/testify/mock"\n    "github.com/stretchr/testify/suite"\n\n    "go.uber.org/cadence"\n    "go.uber.org/cadence/testsuite"\n)\n\ntype UnitTestSuite struct {\n    suite.Suite\n    testsuite.WorkflowTestSuite\n\n    env *testsuite.TestWorkflowEnvironment\n}\n\nfunc (s *UnitTestSuite) SetupTest() {\n    s.env = s.NewTestWorkflowEnvironment()\n}\n\nfunc (s *UnitTestSuite) AfterTest(suiteName, testName string) {\n    s.env.AssertExpectations(s.T())\n}\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_Success() {\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_success")\n\n    s.True(s.env.IsWorkflowCompleted())\n    s.NoError(s.env.GetWorkflowError())\n}\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_ActivityParamCorrect() {\n    s.env.OnActivity(SimpleActivity, mock.Anything, mock.Anything).Return(\n        func(ctx context.Context, value string) (string, error) {\n            s.Equal("test_success", value)\n            return value, nil\n        }\n    )\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_success")\n\n    s.True(s.env.IsWorkflowCompleted())\n    s.NoError(s.env.GetWorkflowError())\n}\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_ActivityFails() {\n    s.env.OnActivity(SimpleActivity, mock.Anything, mock.Anything).Return(\n        "", errors.New("SimpleActivityFailure"))\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_failure")\n\n    s.True(s.env.IsWorkflowCompleted())\n\n    s.NotNil(s.env.GetWorkflowError())\n    s.True(cadence.IsGenericError(s.env.GetWorkflowError()))\n    s.Equal("SimpleActivityFailure", s.env.GetWorkflowError().Error())\n}\n\nfunc TestUnitTestSuite(t *testing.T) {\n    suite.Run(t, new(UnitTestSuite))\n}\n\n\n# Setup\nTo run unit tests, we first define a "test suite" struct that absorbs both the basic suite functionality from testifyvia suite.Suite and the suite functionality from the Cadence test framework viacadence.WorkflowTestSuite. Because every test in this test suite will test our , we add a property to our struct to hold an instance of the test environment. This allows us to initialize the test environment in a setup method. For testing , we use a cadence.TestWorkflowEnvironment.\n\nNext, we implement a SetupTest method to setup a new test environment before each test. Doing so ensures that each test runs in its own isolated sandbox. We also implement an AfterTest function where we assert that all mocks we set up were indeed called by invoking s.env.AssertExpectations(s.T()).\n\nFinally, we create a regular test function recognized by "go test" and pass the struct to suite.Run.\n\n# A Simple Test\nThe most simple test case we can write is to have the test environment execute the and then evaluate the results.\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_Success() {\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_success")\n\n    s.True(s.env.IsWorkflowCompleted())\n    s.NoError(s.env.GetWorkflowError())\n}\n\n\nCalling s.env.ExecuteWorkflow(...) executes the logic and any invoked inside the test process. The first parameter of s.env.ExecuteWorkflow(...) contains the functions, and any subsequent parameters contain values for custom input parameters declared by the function.\n\n> Note that unless the invocations are mocked or implementation replaced (see Activity mocking and overriding), the test environment will execute the actual code including any calls to outside services.\n\n\nAfter executing the in the above example, we assert that the ran through completion via the call to s.env.IsWorkflowComplete(). We also assert that no errors were returned by asserting on the return value of s.env.GetWorkflowError(). If our returned a value, we could have retrieved that value via a call to s.env.GetWorkflowResult(&value) and had additional asserts on that value.\n\n# Activity mocking and overriding\nWhen running unit tests on , we want to test the logic in isolation. Additionally, we want to inject errors during our test runs. The test framework provides two mechanisms that support these scenarios: mocking and overriding. Both of these mechanisms allow you to change the behavior of invoked by your without the need to modify the actual code.\n\nLet\'s take a look at a test that simulates a test that fails via the "activity mocking" mechanism.\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_ActivityFails() {\n    s.env.OnActivity(SimpleActivity, mock.Anything, mock.Anything).Return(\n        "", errors.New("SimpleActivityFailure"))\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_failure")\n\n    s.True(s.env.IsWorkflowCompleted())\n\n    s.NotNil(s.env.GetWorkflowError())\n    _, ok := s.env.GetWorkflowError().(*cadence.GenericError)\n    s.True(ok)\n    s.Equal("SimpleActivityFailure", s.env.GetWorkflowError().Error())\n}\n\n\nThis test simulates the execution of the SimpleActivity that is invoked by our SimpleWorkflow returning an error. We accomplish this by setting up a mock on the test environment for the SimpleActivity that returns an error.\n\ns.env.OnActivity(SimpleActivity, mock.Anything, mock.Anything).Return(\n    "", errors.New("SimpleActivityFailure"))\n\n\nWith the mock set up we can now execute the via the s.env.ExecuteWorkflow(...) method and assert that the completed successfully and returned the expected error.\n\nSimply mocking the execution to return a desired value or error is a pretty powerful mechanism to isolate logic. However, sometimes we want to replace the with an alternate implementation to support a more complex test scenario. Let\'s assume we want to validate that the gets called with the correct parameters.\n\nfunc (s *UnitTestSuite) Test_SimpleWorkflow_ActivityParamCorrect() {\n    s.env.OnActivity(SimpleActivity, mock.Anything, mock.Anything).Return(\n        func(ctx context.Context, value string) (string, error) {\n            s.Equal("test_success", value)\n            return value, nil\n        }\n    )\n    s.env.ExecuteWorkflow(SimpleWorkflow, "test_success")\n\n    s.True(s.env.IsWorkflowCompleted())\n    s.NoError(s.env.GetWorkflowError())\n}\n\n\nIn this example, we provide a function implementation as the parameter to Return. This allows us to provide an alternate implementation for the SimpleActivity. The framework will execute this function whenever the is invoked and pass on the return value from the function as the result of the invocation. Additionally, the framework will validate that the signature of the “mock” function matches the signature of the original function.\n\nSince this can be an entire function, there is no limitation as to what we can do here. In this example, we assert that the “value” param has the same content as the value param we passed to the .',normalizedContent:'# testing\nthe cadence go client library provides a test framework to facilitate testing implementations. the framework is suited for implementing unit tests as well as functional tests of the logic.\n\nthe following code implements unit tests for the simpleworkflow sample:\n\npackage sample\n\nimport (\n    "errors"\n    "testing"\n\n    "github.com/stretchr/testify/mock"\n    "github.com/stretchr/testify/suite"\n\n    "go.uber.org/cadence"\n    "go.uber.org/cadence/testsuite"\n)\n\ntype unittestsuite struct {\n    suite.suite\n    testsuite.workflowtestsuite\n\n    env *testsuite.testworkflowenvironment\n}\n\nfunc (s *unittestsuite) setuptest() {\n    s.env = s.newtestworkflowenvironment()\n}\n\nfunc (s *unittestsuite) aftertest(suitename, testname string) {\n    s.env.assertexpectations(s.t())\n}\n\nfunc (s *unittestsuite) test_simpleworkflow_success() {\n    s.env.executeworkflow(simpleworkflow, "test_success")\n\n    s.true(s.env.isworkflowcompleted())\n    s.noerror(s.env.getworkflowerror())\n}\n\nfunc (s *unittestsuite) test_simpleworkflow_activityparamcorrect() {\n    s.env.onactivity(simpleactivity, mock.anything, mock.anything).return(\n        func(ctx context.context, value string) (string, error) {\n            s.equal("test_success", value)\n            return value, nil\n        }\n    )\n    s.env.executeworkflow(simpleworkflow, "test_success")\n\n    s.true(s.env.isworkflowcompleted())\n    s.noerror(s.env.getworkflowerror())\n}\n\nfunc (s *unittestsuite) test_simpleworkflow_activityfails() {\n    s.env.onactivity(simpleactivity, mock.anything, mock.anything).return(\n        "", errors.new("simpleactivityfailure"))\n    s.env.executeworkflow(simpleworkflow, "test_failure")\n\n    s.true(s.env.isworkflowcompleted())\n\n    s.notnil(s.env.getworkflowerror())\n    s.true(cadence.isgenericerror(s.env.getworkflowerror()))\n    s.equal("simpleactivityfailure", s.env.getworkflowerror().error())\n}\n\nfunc testunittestsuite(t *testing.t) {\n    suite.run(t, new(unittestsuite))\n}\n\n\n# setup\nto run unit tests, we first define a "test suite" struct that absorbs both the basic suite functionality from testifyvia suite.suite and the suite functionality from the cadence test framework viacadence.workflowtestsuite. because every test in this test suite will test our , we add a property to our struct to hold an instance of the test environment. this allows us to initialize the test environment in a setup method. for testing , we use a cadence.testworkflowenvironment.\n\nnext, we implement a setuptest method to setup a new test environment before each test. doing so ensures that each test runs in its own isolated sandbox. we also implement an aftertest function where we assert that all mocks we set up were indeed called by invoking s.env.assertexpectations(s.t()).\n\nfinally, we create a regular test function recognized by "go test" and pass the struct to suite.run.\n\n# a simple test\nthe most simple test case we can write is to have the test environment execute the and then evaluate the results.\n\nfunc (s *unittestsuite) test_simpleworkflow_success() {\n    s.env.executeworkflow(simpleworkflow, "test_success")\n\n    s.true(s.env.isworkflowcompleted())\n    s.noerror(s.env.getworkflowerror())\n}\n\n\ncalling s.env.executeworkflow(...) executes the logic and any invoked inside the test process. the first parameter of s.env.executeworkflow(...) contains the functions, and any subsequent parameters contain values for custom input parameters declared by the function.\n\n> note that unless the invocations are mocked or implementation replaced (see activity mocking and overriding), the test environment will execute the actual code including any calls to outside services.\n\n\nafter executing the in the above example, we assert that the ran through completion via the call to s.env.isworkflowcomplete(). we also assert that no errors were returned by asserting on the return value of s.env.getworkflowerror(). if our returned a value, we could have retrieved that value via a call to s.env.getworkflowresult(&value) and had additional asserts on that value.\n\n# activity mocking and overriding\nwhen running unit tests on , we want to test the logic in isolation. additionally, we want to inject errors during our test runs. the test framework provides two mechanisms that support these scenarios: mocking and overriding. both of these mechanisms allow you to change the behavior of invoked by your without the need to modify the actual code.\n\nlet\'s take a look at a test that simulates a test that fails via the "activity mocking" mechanism.\n\nfunc (s *unittestsuite) test_simpleworkflow_activityfails() {\n    s.env.onactivity(simpleactivity, mock.anything, mock.anything).return(\n        "", errors.new("simpleactivityfailure"))\n    s.env.executeworkflow(simpleworkflow, "test_failure")\n\n    s.true(s.env.isworkflowcompleted())\n\n    s.notnil(s.env.getworkflowerror())\n    _, ok := s.env.getworkflowerror().(*cadence.genericerror)\n    s.true(ok)\n    s.equal("simpleactivityfailure", s.env.getworkflowerror().error())\n}\n\n\nthis test simulates the execution of the simpleactivity that is invoked by our simpleworkflow returning an error. we accomplish this by setting up a mock on the test environment for the simpleactivity that returns an error.\n\ns.env.onactivity(simpleactivity, mock.anything, mock.anything).return(\n    "", errors.new("simpleactivityfailure"))\n\n\nwith the mock set up we can now execute the via the s.env.executeworkflow(...) method and assert that the completed successfully and returned the expected error.\n\nsimply mocking the execution to return a desired value or error is a pretty powerful mechanism to isolate logic. however, sometimes we want to replace the with an alternate implementation to support a more complex test scenario. let\'s assume we want to validate that the gets called with the correct parameters.\n\nfunc (s *unittestsuite) test_simpleworkflow_activityparamcorrect() {\n    s.env.onactivity(simpleactivity, mock.anything, mock.anything).return(\n        func(ctx context.context, value string) (string, error) {\n            s.equal("test_success", value)\n            return value, nil\n        }\n    )\n    s.env.executeworkflow(simpleworkflow, "test_success")\n\n    s.true(s.env.isworkflowcompleted())\n    s.noerror(s.env.getworkflowerror())\n}\n\n\nin this example, we provide a function implementation as the parameter to return. this allows us to provide an alternate implementation for the simpleactivity. the framework will execute this function whenever the is invoked and pass on the return value from the function as the result of the invocation. additionally, the framework will validate that the signature of the “mock” function matches the signature of the original function.\n\nsince this can be an entire function, there is no limitation as to what we can do here. in this example, we assert that the “value” param has the same content as the value param we passed to the .',charsets:{}},{title:"Versioning",frontmatter:{layout:"default",title:"Versioning",permalink:"/docs/go-client/workflow-versioning",readingShow:"top"},regularPath:"/docs/05-go-client/14-workflow-versioning.html",relativePath:"docs/05-go-client/14-workflow-versioning.md",key:"v-957246a8",path:"/docs/go-client/workflow-versioning/",headers:[{level:2,title:"workflow.GetVersion()",slug:"workflow-getversion",normalizedTitle:"workflow.getversion()",charIndex:313},{level:2,title:"Sanity checking",slug:"sanity-checking",normalizedTitle:"sanity checking",charIndex:5611}],headersStr:"workflow.GetVersion() Sanity checking",content:'# Versioning\nThe definition code of a Cadence must be deterministic because Cadence uses sourcing to reconstruct the state by replaying the saved history data on the definition code. This means that any incompatible update to the definition code could cause a non-deterministic issue if not handled correctly.\n\n# workflow.GetVersion()\nConsider the following definition:\n\nfunc MyWorkflow(ctx workflow.Context, data string) (string, error) {\n    ao := workflow.ActivityOptions{\n        ScheduleToStartTimeout: time.Minute,\n        StartToCloseTimeout:    time.Minute,\n    }\n    ctx = workflow.WithActivityOptions(ctx, ao)\n    var result1 string\n    err := workflow.ExecuteActivity(ctx, ActivityA, data).Get(ctx, &result1)\n    if err != nil {\n        return "", err\n    }\n    var result2 string\n    err = workflow.ExecuteActivity(ctx, ActivityB, result1).Get(ctx, &result2)\n    return result2, err\n}\n\n\nNow let\'s say we have replaced ActivityA with ActivityC, and deployed the updated code. If there is an existing that was started by the original version of the code, where ActivityA had already completed and the result was recorded to history, the new version of the code will pick up that and try to resume from there. However, the will fail because the new code expects a result for ActivityC from the history data, but instead it gets the result for ActivityA. This causes the to fail on the non-deterministic error.\n\nThus we use workflow.GetVersion().\n\nvar err error\nv := workflow.GetVersion(ctx, "Step1", workflow.DefaultVersion, 1)\nif v == workflow.DefaultVersion {\n    err = workflow.ExecuteActivity(ctx, ActivityA, data).Get(ctx, &result1)\n} else {\n    err = workflow.ExecuteActivity(ctx, ActivityC, data).Get(ctx, &result1)\n}\nif err != nil {\n    return "", err\n}\n\nvar result2 string\nerr = workflow.ExecuteActivity(ctx, ActivityB, result1).Get(ctx, &result2)\nreturn result2, err\n\n\nWhen workflow.GetVersion() is run for the new , it records a marker in the history so that all future calls to GetVersion for this change ID--Step 1 in the example--on this will always return the given version number, which is 1 in the example.\n\nIf you make an additional change, such as replacing ActivityC with ActivityD, you need to add some additional code:\n\nv := workflow.GetVersion(ctx, "Step1", workflow.DefaultVersion, 2)\nif v == workflow.DefaultVersion {\n    err = workflow.ExecuteActivity(ctx, ActivityA, data).Get(ctx, &result1)\n} else if v == 1 {\n    err = workflow.ExecuteActivity(ctx, ActivityC, data).Get(ctx, &result1)\n} else {\n    err = workflow.ExecuteActivity(ctx, ActivityD, data).Get(ctx, &result1)\n}\n\n\nNote that we have changed maxSupported from 1 to 2. A that had already passed thisGetVersion() call before it was introduced will return DefaultVersion. A that was run with maxSupported set to 1, will return 1. New will return 2.\n\nAfter you are sure that all of the prior to version 1 have completed, you can remove the code for that version. It should now look like the following:\n\nv := workflow.GetVersion(ctx, "Step1", 1, 2)\nif v == 1 {\n    err = workflow.ExecuteActivity(ctx, ActivityC, data).Get(ctx, &result1)\n} else {\n    err = workflow.ExecuteActivity(ctx, ActivityD, data).Get(ctx, &result1)\n}\n\n\nYou\'ll note that minSupported has changed from DefaultVersion to 1. If an older version of the history is replayed on this code, it will fail because the minimum expected version is 1. After you are sure that all of the for version 1 have completed, then you can remove 1 so that your code would look like the following:\n\n_ := workflow.GetVersion(ctx, "Step1", 2, 2)\nerr = workflow.ExecuteActivity(ctx, ActivityD, data).Get(ctx, &result1)\n\n\nNote that we have preserved the call to GetVersion(). There are two reasons to preserve this call:\n\n 1. This ensures that if there is a still running for an older version, it will fail here and not proceed.\n 2. If you need to make additional changes for Step1, such as changing ActivityD to ActivityE, you only need to update maxVersion from 2 to 3 and branch from there.\n\nYou only need to preserve the first call to GetVersion() for each changeID. All subsequent calls toGetVersion() with the same change ID are safe to remove. If necessary, you can remove the firstGetVersion() call, but you need to ensure the following:\n\n * All executions with an older version are completed.\n * You can no longer use Step1 for the changeID. If you need to make changes to that same part in the future, such as change from ActivityD to ActivityE, you would need to use a different changeID like Step1-fix2, and start minVersion from DefaultVersion again. The code would look like the following:\n\nv := workflow.GetVersion(ctx, "Step1-fix2", workflow.DefaultVersion, 1)\nif v == workflow.DefaultVersion {\n    err = workflow.ExecuteActivity(ctx, ActivityD, data).Get(ctx, &result1)\n} else {\n    err = workflow.ExecuteActivity(ctx, ActivityE, data).Get(ctx, &result1)\n}\n\n\nUpgrading a is straightforward if you don\'t need to preserve your currently running. You can simply terminate all of the currently running and suspend new ones from being created while you deploy the new version of your code, which does not use GetVersion(), and then resume creation. However, that is often not the case, and you need to take care of the currently running , so using GetVersion() to update your code is the method to use.\n\nHowever, if you want your currently running to proceed based on the current logic, but you want to ensure new are running on new logic, you can define your as a new WorkflowType, and change your start path (calls to StartWorkflow()) to start the new type.\n\n# Sanity checking\nThe Cadence client SDK performs a sanity check to help prevent obvious incompatible changes. The sanity check verifies whether a made in replay matches the recorded in history, in the same order. The is generated by calling any of the following methods:\n\n * workflow.ExecuteActivity()\n * workflow.ExecuteChildWorkflow()\n * workflow.NewTimer()\n * workflow.Sleep()\n * workflow.SideEffect()\n * workflow.RequestCancelWorkflow()\n * workflow.SignalExternalWorkflow()\n * workflow.UpsertSearchAttributes()\n\nAdding, removing, or reordering any of the above methods triggers the sanity check and results in a non-deterministic error.\n\nThe sanity check does not perform a thorough check. For example, it does not check on the \'s input arguments or the timer duration. If the check is enforced on every property, then it becomes too restricted and harder to maintain the code. For example, if you move your code from one package to another package, that changes the ActivityType, which technically becomes a different. But, we don\'t want to fail on that change, so we only check the function name part of theActivityType.',normalizedContent:'# versioning\nthe definition code of a cadence must be deterministic because cadence uses sourcing to reconstruct the state by replaying the saved history data on the definition code. this means that any incompatible update to the definition code could cause a non-deterministic issue if not handled correctly.\n\n# workflow.getversion()\nconsider the following definition:\n\nfunc myworkflow(ctx workflow.context, data string) (string, error) {\n    ao := workflow.activityoptions{\n        scheduletostarttimeout: time.minute,\n        starttoclosetimeout:    time.minute,\n    }\n    ctx = workflow.withactivityoptions(ctx, ao)\n    var result1 string\n    err := workflow.executeactivity(ctx, activitya, data).get(ctx, &result1)\n    if err != nil {\n        return "", err\n    }\n    var result2 string\n    err = workflow.executeactivity(ctx, activityb, result1).get(ctx, &result2)\n    return result2, err\n}\n\n\nnow let\'s say we have replaced activitya with activityc, and deployed the updated code. if there is an existing that was started by the original version of the code, where activitya had already completed and the result was recorded to history, the new version of the code will pick up that and try to resume from there. however, the will fail because the new code expects a result for activityc from the history data, but instead it gets the result for activitya. this causes the to fail on the non-deterministic error.\n\nthus we use workflow.getversion().\n\nvar err error\nv := workflow.getversion(ctx, "step1", workflow.defaultversion, 1)\nif v == workflow.defaultversion {\n    err = workflow.executeactivity(ctx, activitya, data).get(ctx, &result1)\n} else {\n    err = workflow.executeactivity(ctx, activityc, data).get(ctx, &result1)\n}\nif err != nil {\n    return "", err\n}\n\nvar result2 string\nerr = workflow.executeactivity(ctx, activityb, result1).get(ctx, &result2)\nreturn result2, err\n\n\nwhen workflow.getversion() is run for the new , it records a marker in the history so that all future calls to getversion for this change id--step 1 in the example--on this will always return the given version number, which is 1 in the example.\n\nif you make an additional change, such as replacing activityc with activityd, you need to add some additional code:\n\nv := workflow.getversion(ctx, "step1", workflow.defaultversion, 2)\nif v == workflow.defaultversion {\n    err = workflow.executeactivity(ctx, activitya, data).get(ctx, &result1)\n} else if v == 1 {\n    err = workflow.executeactivity(ctx, activityc, data).get(ctx, &result1)\n} else {\n    err = workflow.executeactivity(ctx, activityd, data).get(ctx, &result1)\n}\n\n\nnote that we have changed maxsupported from 1 to 2. a that had already passed thisgetversion() call before it was introduced will return defaultversion. a that was run with maxsupported set to 1, will return 1. new will return 2.\n\nafter you are sure that all of the prior to version 1 have completed, you can remove the code for that version. it should now look like the following:\n\nv := workflow.getversion(ctx, "step1", 1, 2)\nif v == 1 {\n    err = workflow.executeactivity(ctx, activityc, data).get(ctx, &result1)\n} else {\n    err = workflow.executeactivity(ctx, activityd, data).get(ctx, &result1)\n}\n\n\nyou\'ll note that minsupported has changed from defaultversion to 1. if an older version of the history is replayed on this code, it will fail because the minimum expected version is 1. after you are sure that all of the for version 1 have completed, then you can remove 1 so that your code would look like the following:\n\n_ := workflow.getversion(ctx, "step1", 2, 2)\nerr = workflow.executeactivity(ctx, activityd, data).get(ctx, &result1)\n\n\nnote that we have preserved the call to getversion(). there are two reasons to preserve this call:\n\n 1. this ensures that if there is a still running for an older version, it will fail here and not proceed.\n 2. if you need to make additional changes for step1, such as changing activityd to activitye, you only need to update maxversion from 2 to 3 and branch from there.\n\nyou only need to preserve the first call to getversion() for each changeid. all subsequent calls togetversion() with the same change id are safe to remove. if necessary, you can remove the firstgetversion() call, but you need to ensure the following:\n\n * all executions with an older version are completed.\n * you can no longer use step1 for the changeid. if you need to make changes to that same part in the future, such as change from activityd to activitye, you would need to use a different changeid like step1-fix2, and start minversion from defaultversion again. the code would look like the following:\n\nv := workflow.getversion(ctx, "step1-fix2", workflow.defaultversion, 1)\nif v == workflow.defaultversion {\n    err = workflow.executeactivity(ctx, activityd, data).get(ctx, &result1)\n} else {\n    err = workflow.executeactivity(ctx, activitye, data).get(ctx, &result1)\n}\n\n\nupgrading a is straightforward if you don\'t need to preserve your currently running. you can simply terminate all of the currently running and suspend new ones from being created while you deploy the new version of your code, which does not use getversion(), and then resume creation. however, that is often not the case, and you need to take care of the currently running , so using getversion() to update your code is the method to use.\n\nhowever, if you want your currently running to proceed based on the current logic, but you want to ensure new are running on new logic, you can define your as a new workflowtype, and change your start path (calls to startworkflow()) to start the new type.\n\n# sanity checking\nthe cadence client sdk performs a sanity check to help prevent obvious incompatible changes. the sanity check verifies whether a made in replay matches the recorded in history, in the same order. the is generated by calling any of the following methods:\n\n * workflow.executeactivity()\n * workflow.executechildworkflow()\n * workflow.newtimer()\n * workflow.sleep()\n * workflow.sideeffect()\n * workflow.requestcancelworkflow()\n * workflow.signalexternalworkflow()\n * workflow.upsertsearchattributes()\n\nadding, removing, or reordering any of the above methods triggers the sanity check and results in a non-deterministic error.\n\nthe sanity check does not perform a thorough check. for example, it does not check on the \'s input arguments or the timer duration. if the check is enforced on every property, then it becomes too restricted and harder to maintain the code. for example, if you move your code from one package to another package, that changes the activitytype, which technically becomes a different. but, we don\'t want to fail on that change, so we only check the function name part of theactivitytype.',charsets:{}},{title:"Sessions",frontmatter:{layout:"default",title:"Sessions",permalink:"/docs/go-client/sessions",readingShow:"top"},regularPath:"/docs/05-go-client/15-sessions.html",relativePath:"docs/05-go-client/15-sessions.md",key:"v-389752bc",path:"/docs/go-client/sessions/",headers:[{level:2,title:"Use Cases",slug:"use-cases",normalizedTitle:"use cases",charIndex:252},{level:2,title:"Basic Usage",slug:"basic-usage",normalizedTitle:"basic usage",charIndex:833},{level:3,title:"Sample Code",slug:"sample-code",normalizedTitle:"sample code",charIndex:3528},{level:2,title:"Session Metadata",slug:"session-metadata",normalizedTitle:"session metadata",charIndex:4555},{level:2,title:"Concurrent Session Limitation",slug:"concurrent-session-limitation",normalizedTitle:"concurrent session limitation",charIndex:3054},{level:2,title:"Recreate Session",slug:"recreate-session",normalizedTitle:"recreate session",charIndex:5568},{level:2,title:"Q & A",slug:"q-a",normalizedTitle:"q &amp; a",charIndex:null},{level:3,title:"Is there a complete example?",slug:"is-there-a-complete-example",normalizedTitle:"is there a complete example?",charIndex:6227},{level:3,title:"What happens to my activity if the worker dies?",slug:"what-happens-to-my-activity-if-the-worker-dies",normalizedTitle:"what happens to my activity if the worker dies?",charIndex:6366},{level:3,title:"Is the concurrent session limitation per process or per host?",slug:"is-the-concurrent-session-limitation-per-process-or-per-host",normalizedTitle:"is the concurrent session limitation per process or per host?",charIndex:6572},{level:2,title:"Future Work",slug:"future-work",normalizedTitle:"future work",charIndex:6746}],headersStr:"Use Cases Basic Usage Sample Code Session Metadata Concurrent Session Limitation Recreate Session Q & A Is there a complete example? What happens to my activity if the worker dies? Is the concurrent session limitation per process or per host? Future Work",content:"# Sessions\nThe session framework provides a straightforward interface for scheduling multiple on a single without requiring you to manually specify the name. It also includes features like concurrent session limitation and worker failure detection.\n\n# Use Cases\n * File Processing: You may want to implement a that can download a file, process it, and then upload the modified version. If these three steps are implemented as three different , all of them should be executed by the same .\n   \n   \n * Machine Learning Model Training: Training a machine learning model typically involves three stages: download the data set, optimize the model, and upload the trained parameter. Since the models may consume a large amount of resources (GPU memory for example), the number of models processed on a host needs to be limited.\n   \n   \n\n# Basic Usage\nBefore using the session framework to write your code, you need to configure your to process sessions. To do that, set the EnableSessionWorker field of worker.Options to true when starting your .\n\nThe most important APIs provided by the session framework are workflow.CreateSession() and workflow.CompleteSession(). The basic idea is that all the executed within a session will be processed by the same and these two APIs allow you to create new sessions and close them after all finish executing.\n\nHere's a more detailed description of these two APIs:\n\ntype SessionOptions struct {\n    // ExecutionTimeout: required, no default.\n    //     Specifies the maximum amount of time the session can run.\n    ExecutionTimeout time.Duration\n\n    // CreationTimeout: required, no default.\n    //     Specifies how long session creation can take before returning an error.\n    CreationTimeout  time.Duration\n}\n\nfunc CreateSession(ctx Context, sessionOptions *SessionOptions) (Context, error)\n\n\nCreateSession() takes in workflow.Context, sessionOptions and returns a new context which contains metadata information of the created session (referred to as the session context below). When it's called, it will check the name specified in the ActivityOptions (or in the StartWorkflowOptions if the name is not specified in ActivityOptions), and create the session on one of the which is polling that .\n\nThe returned session context should be used to execute all belonging to the session. The context will be cancelled if the executing this session dies or CompleteSession() is called. When using the returned session context to execute , a workflow.ErrSessionFailed error may be returned if the session framework detects that the executing this session has died. The failure of your won't affect the state of the session, so you still need to handle the errors returned from your and call CompleteSession() if necessary.\n\nCreateSession() will return an error if the context passed in already contains an open session. If all the are currently busy and unable to handle new sessions, the framework will keep retrying until the CreationTimeout you specified in SessionOptions has passed before returning an error (check the Concurrent Session Limitation section for more details).\n\nfunc CompleteSession(ctx Context)\n\n\nCompleteSession() releases the resources reserved on the , so it's important to call it as soon as you no longer need the session. It will cancel the session context and therefore all the using that session context. Note that it's safe to call CompleteSession() on a failed session, meaning that you can call it from a defer function after the session is successfully created.\n\n# Sample Code\nfunc FileProcessingWorkflow(ctx workflow.Context, fileID string) (err error) {\n    ao := workflow.ActivityOptions{\n        ScheduleToStartTimeout: time.Second * 5,\n        StartToCloseTimeout:    time.Minute,\n    }\n    ctx = workflow.WithActivityOptions(ctx, ao)\n\n    so := &workflow.SessionOptions{\n        CreationTimeout:  time.Minute,\n        ExecutionTimeout: time.Minute,\n    }\n    sessionCtx, err := workflow.CreateSession(ctx, so)\n    if err != nil {\n        return err\n    }\n    defer workflow.CompleteSession(sessionCtx)\n\n    var fInfo *fileInfo\n    err = workflow.ExecuteActivity(sessionCtx, downloadFileActivityName, fileID).Get(sessionCtx, &fInfo)\n    if err != nil {\n        return err\n    }\n\n    var fInfoProcessed *fileInfo\n    err = workflow.ExecuteActivity(sessionCtx, processFileActivityName, *fInfo).Get(sessionCtx, &fInfoProcessed)\n    if err != nil {\n        return err\n    }\n\n    return workflow.ExecuteActivity(sessionCtx, uploadFileActivityName, *fInfoProcessed).Get(sessionCtx, nil)\n}\n\n\n# Session Metadata\ntype SessionInfo struct {\n    // A unique ID for the session\n    SessionID         string\n\n    // The hostname of the worker that is executing the session\n    HostName          string\n\n    // ... other unexported fields\n}\n\nfunc GetSessionInfo(ctx Context) *SessionInfo\n\n\nThe session context also stores some session metadata, which can be retrieved by the GetSessionInfo() API. If the context passed in doesn't contain any session metadata, this API will return a nil pointer.\n\n# Concurrent Session Limitation\nTo limit the number of concurrent sessions running on a , set the MaxConcurrentSessionExecutionSize field of worker.Options to the desired value. By default this field is set to a very large value, so there's no need to manually set it if no limitation is needed.\n\nIf a hits this limitation, it won't accept any new CreateSession() requests until one of the existing sessions is completed. CreateSession() will return an error if the session can't be created within CreationTimeout.\n\n# Recreate Session\nFor long-running sessions, you may want to use the ContinueAsNew feature to split the into multiple runs when all need to be executed by the same . The RecreateSession() API is designed for such a use case.\n\nfunc RecreateSession(ctx Context, recreateToken []byte, sessionOptions *SessionOptions) (Context, error)\n\n\nIts usage is the same as CreateSession() except that it also takes in a recreateToken, which is needed to create a new session on the same as the previous one. You can get the token by calling the GetRecreateToken() method of the SessionInfo object.\n\ntoken := workflow.GetSessionInfo(sessionCtx).GetRecreateToken()\n\n\n# Q & A\n# Is there a complete example?\nYes, the file processing example in the cadence-sample repo has been updated to use the session framework.\n\n# What happens to my activity if the worker dies?\nIf your has already been scheduled, it will be cancelled. If not, you will get a workflow.ErrSessionFailed error when you call workflow.ExecuteActivity().\n\n# Is the concurrent session limitation per process or per host?\nIt's per process, so make sure there's only one process running on the host if you plan to use that feature.\n\n# Future Work\n * Support automatic session re-establishingRight now a session is considered failed if the process dies. However, for some use cases, you may only care whether host is alive or not. For these uses cases, the session should be automatically re-established if the process is restarted.\n   \n   \n * Support fine-grained concurrent session limitationThe current implementation assumes that all sessions are consuming the same type of resource and there's only one global limitation. Our plan is to allow you to specify what type of resource your session will consume and enforce different limitations on different types of resources.",normalizedContent:"# sessions\nthe session framework provides a straightforward interface for scheduling multiple on a single without requiring you to manually specify the name. it also includes features like concurrent session limitation and worker failure detection.\n\n# use cases\n * file processing: you may want to implement a that can download a file, process it, and then upload the modified version. if these three steps are implemented as three different , all of them should be executed by the same .\n   \n   \n * machine learning model training: training a machine learning model typically involves three stages: download the data set, optimize the model, and upload the trained parameter. since the models may consume a large amount of resources (gpu memory for example), the number of models processed on a host needs to be limited.\n   \n   \n\n# basic usage\nbefore using the session framework to write your code, you need to configure your to process sessions. to do that, set the enablesessionworker field of worker.options to true when starting your .\n\nthe most important apis provided by the session framework are workflow.createsession() and workflow.completesession(). the basic idea is that all the executed within a session will be processed by the same and these two apis allow you to create new sessions and close them after all finish executing.\n\nhere's a more detailed description of these two apis:\n\ntype sessionoptions struct {\n    // executiontimeout: required, no default.\n    //     specifies the maximum amount of time the session can run.\n    executiontimeout time.duration\n\n    // creationtimeout: required, no default.\n    //     specifies how long session creation can take before returning an error.\n    creationtimeout  time.duration\n}\n\nfunc createsession(ctx context, sessionoptions *sessionoptions) (context, error)\n\n\ncreatesession() takes in workflow.context, sessionoptions and returns a new context which contains metadata information of the created session (referred to as the session context below). when it's called, it will check the name specified in the activityoptions (or in the startworkflowoptions if the name is not specified in activityoptions), and create the session on one of the which is polling that .\n\nthe returned session context should be used to execute all belonging to the session. the context will be cancelled if the executing this session dies or completesession() is called. when using the returned session context to execute , a workflow.errsessionfailed error may be returned if the session framework detects that the executing this session has died. the failure of your won't affect the state of the session, so you still need to handle the errors returned from your and call completesession() if necessary.\n\ncreatesession() will return an error if the context passed in already contains an open session. if all the are currently busy and unable to handle new sessions, the framework will keep retrying until the creationtimeout you specified in sessionoptions has passed before returning an error (check the concurrent session limitation section for more details).\n\nfunc completesession(ctx context)\n\n\ncompletesession() releases the resources reserved on the , so it's important to call it as soon as you no longer need the session. it will cancel the session context and therefore all the using that session context. note that it's safe to call completesession() on a failed session, meaning that you can call it from a defer function after the session is successfully created.\n\n# sample code\nfunc fileprocessingworkflow(ctx workflow.context, fileid string) (err error) {\n    ao := workflow.activityoptions{\n        scheduletostarttimeout: time.second * 5,\n        starttoclosetimeout:    time.minute,\n    }\n    ctx = workflow.withactivityoptions(ctx, ao)\n\n    so := &workflow.sessionoptions{\n        creationtimeout:  time.minute,\n        executiontimeout: time.minute,\n    }\n    sessionctx, err := workflow.createsession(ctx, so)\n    if err != nil {\n        return err\n    }\n    defer workflow.completesession(sessionctx)\n\n    var finfo *fileinfo\n    err = workflow.executeactivity(sessionctx, downloadfileactivityname, fileid).get(sessionctx, &finfo)\n    if err != nil {\n        return err\n    }\n\n    var finfoprocessed *fileinfo\n    err = workflow.executeactivity(sessionctx, processfileactivityname, *finfo).get(sessionctx, &finfoprocessed)\n    if err != nil {\n        return err\n    }\n\n    return workflow.executeactivity(sessionctx, uploadfileactivityname, *finfoprocessed).get(sessionctx, nil)\n}\n\n\n# session metadata\ntype sessioninfo struct {\n    // a unique id for the session\n    sessionid         string\n\n    // the hostname of the worker that is executing the session\n    hostname          string\n\n    // ... other unexported fields\n}\n\nfunc getsessioninfo(ctx context) *sessioninfo\n\n\nthe session context also stores some session metadata, which can be retrieved by the getsessioninfo() api. if the context passed in doesn't contain any session metadata, this api will return a nil pointer.\n\n# concurrent session limitation\nto limit the number of concurrent sessions running on a , set the maxconcurrentsessionexecutionsize field of worker.options to the desired value. by default this field is set to a very large value, so there's no need to manually set it if no limitation is needed.\n\nif a hits this limitation, it won't accept any new createsession() requests until one of the existing sessions is completed. createsession() will return an error if the session can't be created within creationtimeout.\n\n# recreate session\nfor long-running sessions, you may want to use the continueasnew feature to split the into multiple runs when all need to be executed by the same . the recreatesession() api is designed for such a use case.\n\nfunc recreatesession(ctx context, recreatetoken []byte, sessionoptions *sessionoptions) (context, error)\n\n\nits usage is the same as createsession() except that it also takes in a recreatetoken, which is needed to create a new session on the same as the previous one. you can get the token by calling the getrecreatetoken() method of the sessioninfo object.\n\ntoken := workflow.getsessioninfo(sessionctx).getrecreatetoken()\n\n\n# q & a\n# is there a complete example?\nyes, the file processing example in the cadence-sample repo has been updated to use the session framework.\n\n# what happens to my activity if the worker dies?\nif your has already been scheduled, it will be cancelled. if not, you will get a workflow.errsessionfailed error when you call workflow.executeactivity().\n\n# is the concurrent session limitation per process or per host?\nit's per process, so make sure there's only one process running on the host if you plan to use that feature.\n\n# future work\n * support automatic session re-establishingright now a session is considered failed if the process dies. however, for some use cases, you may only care whether host is alive or not. for these uses cases, the session should be automatically re-established if the process is restarted.\n   \n   \n * support fine-grained concurrent session limitationthe current implementation assumes that all sessions are consuming the same type of resource and there's only one global limitation. our plan is to allow you to specify what type of resource your session will consume and enforce different limitations on different types of resources.",charsets:{}},{title:"Distributed CRON",frontmatter:{layout:"default",title:"Distributed CRON",permalink:"/docs/go-client/distributed-cron",readingShow:"top"},regularPath:"/docs/05-go-client/16-distributed-cron.html",relativePath:"docs/05-go-client/16-distributed-cron.md",key:"v-0c11d262",path:"/docs/go-client/distributed-cron/",headers:[{level:2,title:"Convert existing cron workflow",slug:"convert-existing-cron-workflow",normalizedTitle:"convert existing cron workflow",charIndex:1992},{level:2,title:"Retrieve last successful result",slug:"retrieve-last-successful-result",normalizedTitle:"retrieve last successful result",charIndex:2451}],headersStr:"Convert existing cron workflow Retrieve last successful result",content:"# Distributed CRON\nIt is relatively straightforward to turn any Cadence into a Cron . All you need is to supply a cron schedule when starting the using the CronSchedule parameter ofStartWorkflowOptions.\n\nYou can also start a using the Cadence with an optional cron schedule using the --cron argument.\n\nFor with CronSchedule:\n\n * Cron schedule is based on UTC time. For example cron schedule \"15 8 * * *\" will run daily at 8:15am UTC.\n * If a failed and a RetryPolicy is supplied to the StartWorkflowOptions as well, the will retry based on the RetryPolicy. While the is retrying, the server will not schedule the next cron run.\n * Cadence server only schedules the next cron run after the current run is completed. If the next schedule is due while a is running (or retrying), then it will skip that schedule.\n * Cron will not stop until they are terminated or cancelled.\n\nCadence supports the standard cron spec:\n\n// CronSchedule - Optional cron schedule for workflow. If a cron schedule is specified, the workflow will run\n// as a cron based on the schedule. The scheduling will be based on UTC time. The schedule for next run only happen\n// after the current run is completed/failed/timeout. If a RetryPolicy is also supplied, and the workflow failed\n// or timed out, the workflow will be retried based on the retry policy. While the workflow is retrying, it won't\n// schedule its next run. If next schedule is due while the workflow is running (or retrying), then it will skip that\n// schedule. Cron workflow will not stop until it is terminated or cancelled (by returning cadence.CanceledError).\n// The cron spec is as following:\n// ┌───────────── minute (0 - 59)\n// │ ┌───────────── hour (0 - 23)\n// │ │ ┌───────────── day of the month (1 - 31)\n// │ │ │ ┌───────────── month (1 - 12)\n// │ │ │ │ ┌───────────── day of the week (0 - 6) (Sunday to Saturday)\n// │ │ │ │ │\n// │ │ │ │ │\n// * * * * *\nCronSchedule string\n\n\nThe crontab guru site is useful for testing your cron expressions.\n\n# Convert existing cron workflow\nBefore CronSchedule was available, the previous approach to implementing cron was to use a delay timer as the last step and then returnContinueAsNew. One problem with that implementation is that if the fails or times out, the cron would stop.\n\nTo convert those to make use of Cadence CronSchedule, all you need is to remove the delay timer and return without usingContinueAsNew. Then start the with the desired CronSchedule.\n\n# Retrieve last successful result\nSometimes it is useful to obtain the progress of previous successful runs. This is supported by two new APIs in the client library:HasLastCompletionResult and GetLastCompletionResult. Below is an example of how to use this in Go:\n\nfunc CronWorkflow(ctx workflow.Context) (CronResult, error) {\n    startTimestamp := time.Time{} // By default start from 0 time.\n    if workflow.HasLastCompletionResult(ctx) {\n        var progress CronResult\n        if err := workflow.GetLastCompletionResult(ctx, &progress); err == nil {\n            startTimestamp = progress.LastSyncTimestamp\n        }\n    }\n    endTimestamp := workflow.Now(ctx)\n\n    // Process work between startTimestamp (exclusive), endTimestamp (inclusive).\n    // Business logic implementation goes here.\n\n    result := CronResult{LastSyncTimestamp: endTimestamp}\n    return result, nil\n}\n\n\nNote that this works even if one of the cron schedule runs failed. The next schedule will still get the last successful result if it ever successfully completed at least once. For example, for a daily cron , if the first day run succeeds and the second day fails, then the third day run will still get the result from first day's run using these APIs.",normalizedContent:"# distributed cron\nit is relatively straightforward to turn any cadence into a cron . all you need is to supply a cron schedule when starting the using the cronschedule parameter ofstartworkflowoptions.\n\nyou can also start a using the cadence with an optional cron schedule using the --cron argument.\n\nfor with cronschedule:\n\n * cron schedule is based on utc time. for example cron schedule \"15 8 * * *\" will run daily at 8:15am utc.\n * if a failed and a retrypolicy is supplied to the startworkflowoptions as well, the will retry based on the retrypolicy. while the is retrying, the server will not schedule the next cron run.\n * cadence server only schedules the next cron run after the current run is completed. if the next schedule is due while a is running (or retrying), then it will skip that schedule.\n * cron will not stop until they are terminated or cancelled.\n\ncadence supports the standard cron spec:\n\n// cronschedule - optional cron schedule for workflow. if a cron schedule is specified, the workflow will run\n// as a cron based on the schedule. the scheduling will be based on utc time. the schedule for next run only happen\n// after the current run is completed/failed/timeout. if a retrypolicy is also supplied, and the workflow failed\n// or timed out, the workflow will be retried based on the retry policy. while the workflow is retrying, it won't\n// schedule its next run. if next schedule is due while the workflow is running (or retrying), then it will skip that\n// schedule. cron workflow will not stop until it is terminated or cancelled (by returning cadence.cancelederror).\n// the cron spec is as following:\n// ┌───────────── minute (0 - 59)\n// │ ┌───────────── hour (0 - 23)\n// │ │ ┌───────────── day of the month (1 - 31)\n// │ │ │ ┌───────────── month (1 - 12)\n// │ │ │ │ ┌───────────── day of the week (0 - 6) (sunday to saturday)\n// │ │ │ │ │\n// │ │ │ │ │\n// * * * * *\ncronschedule string\n\n\nthe crontab guru site is useful for testing your cron expressions.\n\n# convert existing cron workflow\nbefore cronschedule was available, the previous approach to implementing cron was to use a delay timer as the last step and then returncontinueasnew. one problem with that implementation is that if the fails or times out, the cron would stop.\n\nto convert those to make use of cadence cronschedule, all you need is to remove the delay timer and return without usingcontinueasnew. then start the with the desired cronschedule.\n\n# retrieve last successful result\nsometimes it is useful to obtain the progress of previous successful runs. this is supported by two new apis in the client library:haslastcompletionresult and getlastcompletionresult. below is an example of how to use this in go:\n\nfunc cronworkflow(ctx workflow.context) (cronresult, error) {\n    starttimestamp := time.time{} // by default start from 0 time.\n    if workflow.haslastcompletionresult(ctx) {\n        var progress cronresult\n        if err := workflow.getlastcompletionresult(ctx, &progress); err == nil {\n            starttimestamp = progress.lastsynctimestamp\n        }\n    }\n    endtimestamp := workflow.now(ctx)\n\n    // process work between starttimestamp (exclusive), endtimestamp (inclusive).\n    // business logic implementation goes here.\n\n    result := cronresult{lastsynctimestamp: endtimestamp}\n    return result, nil\n}\n\n\nnote that this works even if one of the cron schedule runs failed. the next schedule will still get the last successful result if it ever successfully completed at least once. for example, for a daily cron , if the first day run succeeds and the second day fails, then the third day run will still get the result from first day's run using these apis.",charsets:{}},{title:"Tracing and context propagation",frontmatter:{layout:"default",title:"Tracing and context propagation",permalink:"/docs/go-client/tracing",readingShow:"top"},regularPath:"/docs/05-go-client/17-tracing.html",relativePath:"docs/05-go-client/17-tracing.md",key:"v-a139e6dc",path:"/docs/go-client/tracing/",headers:[{level:2,title:"Tracing",slug:"tracing",normalizedTitle:"tracing",charIndex:2},{level:2,title:"Context Propagation",slug:"context-propagation",normalizedTitle:"context propagation",charIndex:645},{level:3,title:"Server-Side Headers Support",slug:"server-side-headers-support",normalizedTitle:"server-side headers support",charIndex:1147},{level:3,title:"Context Propagators",slug:"context-propagators",normalizedTitle:"context propagators",charIndex:2055},{level:2,title:"Q & A",slug:"q-a",normalizedTitle:"q &amp; a",charIndex:null},{level:3,title:"Is there a complete example?",slug:"is-there-a-complete-example",normalizedTitle:"is there a complete example?",charIndex:2995},{level:3,title:"Can I configure multiple context propagators?",slug:"can-i-configure-multiple-context-propagators",normalizedTitle:"can i configure multiple context propagators?",charIndex:3159}],headersStr:"Tracing Context Propagation Server-Side Headers Support Context Propagators Q & A Is there a complete example? Can I configure multiple context propagators?",content:"# Tracing and context propagation\n# Tracing\nThe Go client provides distributed tracing support through OpenTracing. Tracing can be configured by providing an opentracing.Tracerimplementation in ClientOptionsand WorkerOptions during client and instantiation, respectively. Tracing allows you to view the call graph of a along with its , child etc. For more details on how to configure and leverage tracing, see the OpenTracing documentation. The OpenTracing support has been validated using Jaeger, but other implementations mentioned here should also work. Tracing support utilizes generic context propagation support provided by the client.\n\n# Context Propagation\nWe provide a standard way to propagate custom context across a .ClientOptions and WorkerOptionsallow configuring a context propagator. The context propagator extracts and passes on information present in the context.Contextand workflow.Context objects across the . Once a context propagator is configured, you should be able to access the required values in the context objects as you would normally do in Go. For a sample, the Go client implements a tracing context propagator.\n\n# Server-Side Headers Support\nOn the server side, Cadence provides a mechanism to propagate what it calls headers across different transitions.\n\nstruct Header {\n    10: optional map<string, binary> fields\n}\n\n\nThe client leverages this to pass around selected context information. HeaderReaderand HeaderWriter are interfaces that allow reading and writing to the Cadence server headers. The client already provides implementationsfor these. HeaderWriter sets a field in the header. Headers is a map, so setting a value for the the same key multiple times will overwrite the previous values. HeaderReader iterates through the headers map and runs the provided handler function on each key/value pair, allowing you to deal with the fields you are interested in.\n\ntype HeaderWriter interface {\n    Set(string, []byte)\n}\n\ntype HeaderReader interface {\n    ForEachKey(handler func(string, []byte) error) error\n}\n\n\n# Context Propagators\nContext propagators require implementing the following four methods to propagate selected context across a workflow:\n\n * Inject is meant to pick out the context keys of interest from a Go context.Context object and write that into the headers using the HeaderWriter interface\n * InjectFromWorkflow is the same as above, but operates on a workflow.Context object\n * Extract reads the headers and places the information of interest back into the context.Context object\n * ExtractToWorkflow is the same as above, but operates on a workflow.Context object\n\nThe tracing context propagatorshows a sample implementation of context propagation.\n\ntype ContextPropagator interface {\n    Inject(context.Context, HeaderWriter) error\n\n    Extract(context.Context, HeaderReader) (context.Context, error)\n\n    InjectFromWorkflow(Context, HeaderWriter) error\n\n    ExtractToWorkflow(Context, HeaderReader) (Context, error)\n}\n\n\n# Q & A\n# Is there a complete example?\nThe context propagation sampleconfigures a custom context propagator and shows context propagation of custom keys across a and an .\n\n# Can I configure multiple context propagators?\nYes, we recommended that you configure multiple context propagators with each propagator meant to propagate a particular type of context.",normalizedContent:"# tracing and context propagation\n# tracing\nthe go client provides distributed tracing support through opentracing. tracing can be configured by providing an opentracing.tracerimplementation in clientoptionsand workeroptions during client and instantiation, respectively. tracing allows you to view the call graph of a along with its , child etc. for more details on how to configure and leverage tracing, see the opentracing documentation. the opentracing support has been validated using jaeger, but other implementations mentioned here should also work. tracing support utilizes generic context propagation support provided by the client.\n\n# context propagation\nwe provide a standard way to propagate custom context across a .clientoptions and workeroptionsallow configuring a context propagator. the context propagator extracts and passes on information present in the context.contextand workflow.context objects across the . once a context propagator is configured, you should be able to access the required values in the context objects as you would normally do in go. for a sample, the go client implements a tracing context propagator.\n\n# server-side headers support\non the server side, cadence provides a mechanism to propagate what it calls headers across different transitions.\n\nstruct header {\n    10: optional map<string, binary> fields\n}\n\n\nthe client leverages this to pass around selected context information. headerreaderand headerwriter are interfaces that allow reading and writing to the cadence server headers. the client already provides implementationsfor these. headerwriter sets a field in the header. headers is a map, so setting a value for the the same key multiple times will overwrite the previous values. headerreader iterates through the headers map and runs the provided handler function on each key/value pair, allowing you to deal with the fields you are interested in.\n\ntype headerwriter interface {\n    set(string, []byte)\n}\n\ntype headerreader interface {\n    foreachkey(handler func(string, []byte) error) error\n}\n\n\n# context propagators\ncontext propagators require implementing the following four methods to propagate selected context across a workflow:\n\n * inject is meant to pick out the context keys of interest from a go context.context object and write that into the headers using the headerwriter interface\n * injectfromworkflow is the same as above, but operates on a workflow.context object\n * extract reads the headers and places the information of interest back into the context.context object\n * extracttoworkflow is the same as above, but operates on a workflow.context object\n\nthe tracing context propagatorshows a sample implementation of context propagation.\n\ntype contextpropagator interface {\n    inject(context.context, headerwriter) error\n\n    extract(context.context, headerreader) (context.context, error)\n\n    injectfromworkflow(context, headerwriter) error\n\n    extracttoworkflow(context, headerreader) (context, error)\n}\n\n\n# q & a\n# is there a complete example?\nthe context propagation sampleconfigures a custom context propagator and shows context propagation of custom keys across a and an .\n\n# can i configure multiple context propagators?\nyes, we recommended that you configure multiple context propagators with each propagator meant to propagate a particular type of context.",charsets:{}},{title:"Workflow Replay and Shadowing",frontmatter:{layout:"default",title:"Workflow Replay and Shadowing",permalink:"/docs/go-client/workflow-replay-shadowing",readingShow:"top"},regularPath:"/docs/05-go-client/18-workflow-replay-shadowing.html",relativePath:"docs/05-go-client/18-workflow-replay-shadowing.md",key:"v-0f2e8980",path:"/docs/go-client/workflow-replay-shadowing/",headers:[{level:2,title:"Workflow Replayer",slug:"workflow-replayer",normalizedTitle:"workflow replayer",charIndex:468},{level:3,title:"Write a Replay Test",slug:"write-a-replay-test",normalizedTitle:"write a replay test",charIndex:820},{level:3,title:"Sample Replay Test",slug:"sample-replay-test",normalizedTitle:"sample replay test",charIndex:3767},{level:2,title:"Workflow Shadower",slug:"workflow-shadower",normalizedTitle:"workflow shadower",charIndex:490},{level:3,title:"Shadow Options",slug:"shadow-options",normalizedTitle:"shadow options",charIndex:4908},{level:3,title:"Local Shadowing Test",slug:"local-shadowing-test",normalizedTitle:"local shadowing test",charIndex:6585},{level:3,title:"Shadowing Worker",slug:"shadowing-worker",normalizedTitle:"shadowing worker",charIndex:7650}],headersStr:"Workflow Replayer Write a Replay Test Sample Replay Test Workflow Shadower Shadow Options Local Shadowing Test Shadowing Worker",content:"# Workflow Replay and Shadowing\nIn the Versioning section, we mentioned that incompatible changes to workflow definition code could cause non-deterministic issues when processing workflow tasks if versioning is not done correctly. However, it may be hard for you to tell if a particular change is incompatible or not and whether versioning logic is needed. To help you identify incompatible changes and catch them before production traffic is impacted, we implemented Workflow Replayer and Workflow Shadower.\n\n# Workflow Replayer\nWorkflow Replayer is a testing component for replaying existing workflow histories against a workflow definition. The replaying logic is the same as the one used for processing workflow tasks, so if there's any incompatible changes in the workflow definition, the replay test will fail.\n\n# Write a Replay Test\n# Step 1: Create workflow replayer\nCreate a workflow Replayer by:\n\nreplayer := worker.NewWorkflowReplayer()\n\n\nor if custom data converter, context propagator, interceptor, etc. is used in your workflow:\n\noptions := worker.ReplayOptions{\n  DataConverter: myDataConverter,\n  ContextPropagators: []workflow.ContextPropagator{\n    myContextPropagator,\n  },\n  WorkflowInterceptorChainFactories: []interceptors.WorkflowInterceptorFactory{\n    myInterceptorFactory,\n  },\n  Tracer: myTracer,\n}\nreplayer := worker.NewWorkflowReplayWithOptions(options)\n\n\n# Step 2: Register workflow definition\nNext, register your workflow definitions as you normally do. Make sure workflows are registered the same way as they were when running and generating histories; otherwise the replay will not be able to find the corresponding definition.\n\nreplayer.RegisterWorkflow(myWorkflowFunc1)\nreplayer.RegisterWorkflow(myWorkflowFunc2, workflow.RegisterOptions{\n\tName: workflowName,\n})\n\n\n# Step 3: Prepare workflow histories\nReplayer can read workflow history from a local json file or fetch it directly from the Cadence server. If you would like to use the first method, you can use the following CLI command, otherwise you can skip to the next step.\n\ncadence --do <domain> workflow show --wid <workflowID> --rid <runID> --of <output file name>\n\n\nThe dumped workflow history will be stored in the file at the path you specified in json format.\n\n# Step 4: Call the replay method\nOnce you have the workflow history or have the connection to Cadence server for fetching history, call one of the four replay methods to start the replay test.\n\n// if workflow history has been loaded into memory\nerr := replayer.ReplayWorkflowHistory(logger, history)\n\n// if workflow history is stored in a json file\nerr = replayer.ReplayWorkflowHistoryFromJSONFile(logger, jsonFileName)\n\n// if workflow history is stored in a json file and you only want to replay part of it\n// NOTE: lastEventID can't be set arbitrarily. It must be the end of of a history events batch\n// when in doubt, set to the eventID of decisionTaskStarted events.\nerr = replayer.ReplayPartialWorkflowHistoryFromJSONFile(logger, jsonFileName, lastEventID)\n\n// if you want to fetch workflow history directly from cadence server\n// please check the Worker Service page for how to create a cadence service client\nerr = replayer.ReplayWorkflowExecution(ctx, cadenceServiceClient, logger, domain, execution)\n\n\n# Step 5: Check returned error\nIf an error is returned from the replay method, it means there's a incompatible change in the workflow definition and the error message will contain more information regarding where the non-deterministic error happens.\n\nNote: currently an error will be returned if there are less than 3 events in the history. It is because the first 3 events in the history has nothing to do with the workflow code, so Replayer can't tell if there's a incompatible change or not.\n\n# Sample Replay Test\nThis sample is also available in our samples repo at here.\n\nfunc TestReplayWorkflowHistoryFromFile(t *testing.T) {\n\treplayer := worker.NewWorkflowReplayer()\n\treplayer.RegisterWorkflow(helloWorldWorkflow)\n\terr := replayer.ReplayWorkflowHistoryFromJSONFile(zaptest.NewLogger(t), \"helloworld.json\")\n\trequire.NoError(t, err)\n}\n\n\n# Workflow Shadower\nWorkflow Replayer works well when verifying the compatibility against a small number of workflow histories. If there are lots of workflows in production need to be verified, dumping all histories manually clearly won't work. Directly fetching histories from cadence server might be a solution, but the time to replay all workflow histories might be too long for a test.\n\nWorkflow Shadower is built on top of Workflow Replayer to address this problem. The basic idea of shadowing is: scan workflows based on the filters you defined, fetch history for each of workflow in the scan result from Cadence server and run the replay test. It can be run either as a test to serve local development purpose or as a workflow in your worker to continuously replay production workflows.\n\n# Shadow Options\nComplete documentation on shadow options which includes default values, accepted values, etc. can be found here. The following sections are just a brief description of each option.\n\n# Scan Filters\n * WorkflowQuery: If you are familiar with our advanced visibility query syntax, you can specify a query directly. If specified, all other scan filters must be left empty.\n * WorkflowTypes: A list of workflow Type names.\n * WorkflowStatus: A list of workflow status.\n * WorkflowStartTimeFilter: Min and max timestamp for workflow start time.\n * SamplingRate: Sampling workflows from the scan result before executing the replay test.\n\n# Shadow Exit Condition\n * ExpirationInterval: Shadowing will exit when the specified interval has passed.\n * ShadowCount: Shadowing will exit after this number of workflow has been replayed. Note: replay maybe skipped due to errors like can't fetch history, history too short, etc. Skipped workflows won't be taken account into ShadowCount.\n\n# Shadow Mode\n * Normal: Shadowing will complete after all workflows matches WorkflowQuery (after sampling) have been replayed or when exit condition is met.\n * Continuous: A new round of shadowing will be started after all workflows matches WorkflowQuery have been replayed. There will be a 5 min wait period between each round, and currently this wait period is not configurable. Shadowing will complete only when ExitCondition is met. ExitCondition must be specified when using this mode.\n\n# Shadow Concurrency\n * Concurrency: workflow replay concurrency. If not specified, will be default to 1. For local shadowing, an error will be returned if a value higher than 1 is specified.\n\n# Local Shadowing Test\nLocal shadowing test is similar to the replay test. First create a workflow shadower with optional shadow and replay options, then register the workflow that need to be shadowed. Finally, call the Run method to start the shadowing. The method will return if shadowing has finished or any non-deterministic error is found.\n\nHere's a simple example. The example is also available here.\n\nfunc TestShadowWorkflow(t *testing.T) {\n\toptions := worker.ShadowOptions{\n\t\tWorkflowStartTimeFilter: worker.TimeFilter{\n\t\t\tMinTimestamp: time.Now().Add(-time.Hour),\n\t\t},\n\t\tExitCondition: worker.ShadowExitCondition{\n\t\t\tShadowCount: 10,\n\t\t},\n\t}\n\n  // please check the Worker Service page for how to create a cadence service client\n\tservice := buildCadenceClient()\n\tshadower, err := worker.NewWorkflowShadower(service, \"samples-domain\", options, worker.ReplayOptions{}, zaptest.NewLogger(t))\n\tassert.NoError(t, err)\n\n\tshadower.RegisterWorkflowWithOptions(helloWorldWorkflow, workflow.RegisterOptions{Name: \"helloWorld\"})\n\tassert.NoError(t, shadower.Run())\n}\n\n\n# Shadowing Worker\nNOTE:\n\n * All shadow workflows are running in one Cadence system domain, and right now, every user domain can only have one shadow workflow at a time.\n * The Cadence server used for scanning and getting workflow history will also be the Cadence server for running your shadow workflow. Currently, there's no way to specify different Cadence servers for hosting the shadowing workflow and scanning/fetching workflow.\n\nYour worker can also be configured to run in shadow mode to run shadow tests as a workflow. This is useful if there's a number of workflows need to be replayed. Using a workflow can make sure the shadowing won't accidentally fail in the middle and the replay load can be distributed by deploying more shadow mode workers. It can also be incorporated into your deployment process to make sure there's no failed replay checks before deploying your change to production workers.\n\nWhen running in shadow mode, the normal decision, activity and session worker will be disabled so that it won't update any production workflows. A special shadow activity worker will be started to execute activities for scanning and replaying workflows. The actual shadow workflow logic is controlled by Cadence server and your worker is only responsible for scanning and replaying workflows.\n\nReplay succeed, skipped and failed metrics will be emitted by your worker when executing the shadow workflow and you can monitor those metrics to see if there's any incompatible changes.\n\nTo enable the shadow mode, the only change needed is setting the EnableShadowWorker field in worker.Options to true, and then specify the ShadowOptions.\n\nRegistered workflows will be forwarded to the underlying WorkflowReplayer. DataConverter, WorkflowInterceptorChainFactories, ContextPropagators, and Tracer specified in the worker.Options will also be used as ReplayOptions. Since all shadow workflows are running in one system domain, to avoid conflict, the actual task list name used will be domain-tasklist.\n\nA sample setup can be found here.",normalizedContent:"# workflow replay and shadowing\nin the versioning section, we mentioned that incompatible changes to workflow definition code could cause non-deterministic issues when processing workflow tasks if versioning is not done correctly. however, it may be hard for you to tell if a particular change is incompatible or not and whether versioning logic is needed. to help you identify incompatible changes and catch them before production traffic is impacted, we implemented workflow replayer and workflow shadower.\n\n# workflow replayer\nworkflow replayer is a testing component for replaying existing workflow histories against a workflow definition. the replaying logic is the same as the one used for processing workflow tasks, so if there's any incompatible changes in the workflow definition, the replay test will fail.\n\n# write a replay test\n# step 1: create workflow replayer\ncreate a workflow replayer by:\n\nreplayer := worker.newworkflowreplayer()\n\n\nor if custom data converter, context propagator, interceptor, etc. is used in your workflow:\n\noptions := worker.replayoptions{\n  dataconverter: mydataconverter,\n  contextpropagators: []workflow.contextpropagator{\n    mycontextpropagator,\n  },\n  workflowinterceptorchainfactories: []interceptors.workflowinterceptorfactory{\n    myinterceptorfactory,\n  },\n  tracer: mytracer,\n}\nreplayer := worker.newworkflowreplaywithoptions(options)\n\n\n# step 2: register workflow definition\nnext, register your workflow definitions as you normally do. make sure workflows are registered the same way as they were when running and generating histories; otherwise the replay will not be able to find the corresponding definition.\n\nreplayer.registerworkflow(myworkflowfunc1)\nreplayer.registerworkflow(myworkflowfunc2, workflow.registeroptions{\n\tname: workflowname,\n})\n\n\n# step 3: prepare workflow histories\nreplayer can read workflow history from a local json file or fetch it directly from the cadence server. if you would like to use the first method, you can use the following cli command, otherwise you can skip to the next step.\n\ncadence --do <domain> workflow show --wid <workflowid> --rid <runid> --of <output file name>\n\n\nthe dumped workflow history will be stored in the file at the path you specified in json format.\n\n# step 4: call the replay method\nonce you have the workflow history or have the connection to cadence server for fetching history, call one of the four replay methods to start the replay test.\n\n// if workflow history has been loaded into memory\nerr := replayer.replayworkflowhistory(logger, history)\n\n// if workflow history is stored in a json file\nerr = replayer.replayworkflowhistoryfromjsonfile(logger, jsonfilename)\n\n// if workflow history is stored in a json file and you only want to replay part of it\n// note: lasteventid can't be set arbitrarily. it must be the end of of a history events batch\n// when in doubt, set to the eventid of decisiontaskstarted events.\nerr = replayer.replaypartialworkflowhistoryfromjsonfile(logger, jsonfilename, lasteventid)\n\n// if you want to fetch workflow history directly from cadence server\n// please check the worker service page for how to create a cadence service client\nerr = replayer.replayworkflowexecution(ctx, cadenceserviceclient, logger, domain, execution)\n\n\n# step 5: check returned error\nif an error is returned from the replay method, it means there's a incompatible change in the workflow definition and the error message will contain more information regarding where the non-deterministic error happens.\n\nnote: currently an error will be returned if there are less than 3 events in the history. it is because the first 3 events in the history has nothing to do with the workflow code, so replayer can't tell if there's a incompatible change or not.\n\n# sample replay test\nthis sample is also available in our samples repo at here.\n\nfunc testreplayworkflowhistoryfromfile(t *testing.t) {\n\treplayer := worker.newworkflowreplayer()\n\treplayer.registerworkflow(helloworldworkflow)\n\terr := replayer.replayworkflowhistoryfromjsonfile(zaptest.newlogger(t), \"helloworld.json\")\n\trequire.noerror(t, err)\n}\n\n\n# workflow shadower\nworkflow replayer works well when verifying the compatibility against a small number of workflow histories. if there are lots of workflows in production need to be verified, dumping all histories manually clearly won't work. directly fetching histories from cadence server might be a solution, but the time to replay all workflow histories might be too long for a test.\n\nworkflow shadower is built on top of workflow replayer to address this problem. the basic idea of shadowing is: scan workflows based on the filters you defined, fetch history for each of workflow in the scan result from cadence server and run the replay test. it can be run either as a test to serve local development purpose or as a workflow in your worker to continuously replay production workflows.\n\n# shadow options\ncomplete documentation on shadow options which includes default values, accepted values, etc. can be found here. the following sections are just a brief description of each option.\n\n# scan filters\n * workflowquery: if you are familiar with our advanced visibility query syntax, you can specify a query directly. if specified, all other scan filters must be left empty.\n * workflowtypes: a list of workflow type names.\n * workflowstatus: a list of workflow status.\n * workflowstarttimefilter: min and max timestamp for workflow start time.\n * samplingrate: sampling workflows from the scan result before executing the replay test.\n\n# shadow exit condition\n * expirationinterval: shadowing will exit when the specified interval has passed.\n * shadowcount: shadowing will exit after this number of workflow has been replayed. note: replay maybe skipped due to errors like can't fetch history, history too short, etc. skipped workflows won't be taken account into shadowcount.\n\n# shadow mode\n * normal: shadowing will complete after all workflows matches workflowquery (after sampling) have been replayed or when exit condition is met.\n * continuous: a new round of shadowing will be started after all workflows matches workflowquery have been replayed. there will be a 5 min wait period between each round, and currently this wait period is not configurable. shadowing will complete only when exitcondition is met. exitcondition must be specified when using this mode.\n\n# shadow concurrency\n * concurrency: workflow replay concurrency. if not specified, will be default to 1. for local shadowing, an error will be returned if a value higher than 1 is specified.\n\n# local shadowing test\nlocal shadowing test is similar to the replay test. first create a workflow shadower with optional shadow and replay options, then register the workflow that need to be shadowed. finally, call the run method to start the shadowing. the method will return if shadowing has finished or any non-deterministic error is found.\n\nhere's a simple example. the example is also available here.\n\nfunc testshadowworkflow(t *testing.t) {\n\toptions := worker.shadowoptions{\n\t\tworkflowstarttimefilter: worker.timefilter{\n\t\t\tmintimestamp: time.now().add(-time.hour),\n\t\t},\n\t\texitcondition: worker.shadowexitcondition{\n\t\t\tshadowcount: 10,\n\t\t},\n\t}\n\n  // please check the worker service page for how to create a cadence service client\n\tservice := buildcadenceclient()\n\tshadower, err := worker.newworkflowshadower(service, \"samples-domain\", options, worker.replayoptions{}, zaptest.newlogger(t))\n\tassert.noerror(t, err)\n\n\tshadower.registerworkflowwithoptions(helloworldworkflow, workflow.registeroptions{name: \"helloworld\"})\n\tassert.noerror(t, shadower.run())\n}\n\n\n# shadowing worker\nnote:\n\n * all shadow workflows are running in one cadence system domain, and right now, every user domain can only have one shadow workflow at a time.\n * the cadence server used for scanning and getting workflow history will also be the cadence server for running your shadow workflow. currently, there's no way to specify different cadence servers for hosting the shadowing workflow and scanning/fetching workflow.\n\nyour worker can also be configured to run in shadow mode to run shadow tests as a workflow. this is useful if there's a number of workflows need to be replayed. using a workflow can make sure the shadowing won't accidentally fail in the middle and the replay load can be distributed by deploying more shadow mode workers. it can also be incorporated into your deployment process to make sure there's no failed replay checks before deploying your change to production workers.\n\nwhen running in shadow mode, the normal decision, activity and session worker will be disabled so that it won't update any production workflows. a special shadow activity worker will be started to execute activities for scanning and replaying workflows. the actual shadow workflow logic is controlled by cadence server and your worker is only responsible for scanning and replaying workflows.\n\nreplay succeed, skipped and failed metrics will be emitted by your worker when executing the shadow workflow and you can monitor those metrics to see if there's any incompatible changes.\n\nto enable the shadow mode, the only change needed is setting the enableshadowworker field in worker.options to true, and then specify the shadowoptions.\n\nregistered workflows will be forwarded to the underlying workflowreplayer. dataconverter, workflowinterceptorchainfactories, contextpropagators, and tracer specified in the worker.options will also be used as replayoptions. since all shadow workflows are running in one system domain, to avoid conflict, the actual task list name used will be domain-tasklist.\n\na sample setup can be found here.",charsets:{}},{title:"Introduction",frontmatter:{layout:"default",title:"Introduction",permalink:"/docs/go-client",readingShow:"top"},regularPath:"/docs/05-go-client/",relativePath:"docs/05-go-client/index.md",key:"v-2d3e7cdb",path:"/docs/go-client/",headers:[{level:2,title:"Overview",slug:"overview",normalizedTitle:"overview",charIndex:14},{level:2,title:"Links",slug:"links",normalizedTitle:"links",charIndex:708}],headersStr:"Overview Links",content:"# Go client\n# Overview\nGo client attempts to follow Go language conventions. The conversion of a Go program to the fault-oblivious function is expected to be pretty mechanical.\n\nCadence requires determinism of the code. It supports deterministic execution of the multithreaded code and constructs like select that are non-deterministic by Go design. The Cadence solution is to provide corresponding constructs in the form of interfaces that have similar capability but support deterministic execution.\n\nFor example, instead of native Go channels, code must use the workflow.Channel interface. Instead of select, the workflow.Selector interface must be used.\n\nFor more information, see Creating Workflows.\n\n# Links\n * GitHub project: https://github.com/uber-go/cadence-client\n * Samples: https://github.com/uber-common/cadence-samples\n * GoDoc documentation: https://godoc.org/go.uber.org/cadence",normalizedContent:"# go client\n# overview\ngo client attempts to follow go language conventions. the conversion of a go program to the fault-oblivious function is expected to be pretty mechanical.\n\ncadence requires determinism of the code. it supports deterministic execution of the multithreaded code and constructs like select that are non-deterministic by go design. the cadence solution is to provide corresponding constructs in the form of interfaces that have similar capability but support deterministic execution.\n\nfor example, instead of native go channels, code must use the workflow.channel interface. instead of select, the workflow.selector interface must be used.\n\nfor more information, see creating workflows.\n\n# links\n * github project: https://github.com/uber-go/cadence-client\n * samples: https://github.com/uber-common/cadence-samples\n * godoc documentation: https://godoc.org/go.uber.org/cadence",charsets:{}},{title:"Introduction",frontmatter:{layout:"default",title:"Introduction",permalink:"/docs/cli",readingShow:"top"},regularPath:"/docs/06-cli/",relativePath:"docs/06-cli/index.md",key:"v-2dfd6d7b",path:"/docs/cli/",headers:[{level:2,title:"Using the CLI",slug:"using-the-cli",normalizedTitle:"using the cli",charIndex:234},{level:3,title:"Homebrew",slug:"homebrew",normalizedTitle:"homebrew",charIndex:250},{level:3,title:"Docker",slug:"docker",normalizedTitle:"docker",charIndex:360},{level:3,title:"Build it yourself",slug:"build-it-yourself",normalizedTitle:"build it yourself",charIndex:1648},{level:2,title:"Documentation",slug:"documentation",normalizedTitle:"documentation",charIndex:2029},{level:2,title:"Environment variables",slug:"environment-variables",normalizedTitle:"environment variables",charIndex:5905},{level:2,title:"Quick Start",slug:"quick-start",normalizedTitle:"quick start",charIndex:6184},{level:3,title:"Domain operation examples",slug:"domain-operation-examples",normalizedTitle:"domain operation examples",charIndex:6542},{level:3,title:"Workflow operation examples",slug:"workflow-operation-examples",normalizedTitle:"workflow operation examples",charIndex:7067}],headersStr:"Using the CLI Homebrew Docker Build it yourself Documentation Environment variables Quick Start Domain operation examples Workflow operation examples",content:'# Command Line Interface\nThe Cadence is a command-line tool you can use to perform various on a Cadence server. It can perform operations such as register, update, and describe as well as operations like start, show history, and .\n\n# Using the CLI\n# Homebrew\nbrew install cadence-workflow\n\n\nAfter the installation is done, you can use CLI:\n\ncadence --help\n\n\n# Docker\nThe Cadence can be used directly from the Docker Hub image ubercadence/cli or by building the tool locally.\n\nExample of using the docker image to describe a \n\ndocker run --rm ubercadence/cli:master --domain samples-domain domain describe\n\n\nmaster will be the latest CLI binary from the project. But you can specify a version to best match your server version:\n\ndocker run --rm ubercadence/cli:<version> --domain samples-domain domain describe\n\n\nFor example docker run --rm ubercadence/cli:0.21.3 --domain samples-domain domain describe will be the CLI that is released as part of the v0.21.3 release. See docker hub page for all the CLI image tags.\n\nNOTE: On Docker versions 18.03 and later, you may get a "connection refused" error. You can work around this by setting the host to "host.docker.internal" (see here for more info).\n\ndocker run --rm ubercadence/cli:master --address host.docker.internal:7933 --domain samples-domain domain describe\n\n\nNOTE: Be sure to update your image when you want to try new features: docker pull ubercadence/cli:master\n\nNOTE: If you are running docker-compose Cadence server, you can also logon to the container to execute CLI:\n\ndocker exec -it docker_cadence_1 /bin/bash\n\n# cadence --address $(hostname -i):7933 --do samples domain register\n\n\n# Build it yourself\nTo build the tool locally, clone the Cadence server repo, check out the version tag (e.g. git checkout v0.21.3) and runmake tools. This produces an executable called cadence. With a local build, the same command to describe a would look like this:\n\ncadence --domain samples-domain domain describe\n\n\nAlternatively, you can build the CLI image, see instructions\n\n# Documentation\nCLI are documented by --help or -h in ANY tab of all levels:\n\n$cadence --help\nNAME:\n   cadence - A command-line tool for cadence users\n\nUSAGE:\n   cadence [global options] command [command options] [arguments...]\n\nVERSION:\n   0.18.4\n\nCOMMANDS:\n   domain, d     Operate cadence domain\n   workflow, wf  Operate cadence workflow\n   tasklist, tl  Operate cadence tasklist\n   admin, adm    Run admin operation\n   cluster, cl   Operate cadence cluster\n   help, h       Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --address value, --ad value          host:port for cadence frontend service [$CADENCE_CLI_ADDRESS]\n   --domain value, --do value           cadence workflow domain [$CADENCE_CLI_DOMAIN]\n   --context_timeout value, --ct value  optional timeout for context of RPC call in seconds (default: 5) [$CADENCE_CONTEXT_TIMEOUT]\n   --help, -h                           show help\n   --version, -v                        print the version\n\n\nAnd\n\n$cadence workflow -h\nNAME:\n   cadence workflow - Operate cadence workflow\n\nUSAGE:\n   cadence workflow command [command options] [arguments...]\n\nCOMMANDS:\n   activity, act       operate activities of workflow\n   show                show workflow history\n   showid              show workflow history with given workflow_id and run_id (a shortcut of `show -w <wid> -r <rid>`). run_id is only required for archived history\n   start               start a new workflow execution\n   run                 start a new workflow execution and get workflow progress\n   cancel, c           cancel a workflow execution\n   signal, s           signal a workflow execution\n   signalwithstart     signal the current open workflow if exists, or attempt to start a new run based on IDResuePolicy and signals it\n   terminate, term     terminate a new workflow execution\n   list, l             list open or closed workflow executions\n   listall, la         list all open or closed workflow executions\n   listarchived        list archived workflow executions\n   scan, sc, scanall   scan workflow executions (need to enable Cadence server on ElasticSearch). It will be faster than listall, but result are not sorted.\n   count, cnt          count number of workflow executions (need to enable Cadence server on ElasticSearch)\n   query               query workflow execution\n   stack               query workflow execution with __stack_trace as query type\n   describe, desc      show information of workflow execution\n   describeid, descid  show information of workflow execution with given workflow_id and optional run_id (a shortcut of `describe -w <wid> -r <rid>`)\n   observe, ob         show the progress of workflow history\n   observeid, obid     show the progress of workflow history with given workflow_id and optional run_id (a shortcut of `observe -w <wid> -r <rid>`)\n   reset, rs           reset the workflow, by either eventID or resetType.\n   reset-batch         reset workflow in batch by resetType: LastDecisionCompleted,LastContinuedAsNew,BadBinary,DecisionCompletedTime,FirstDecisionScheduled,LastDecisionScheduled,FirstDecisionCompletedTo get base workflowIDs/runIDs to reset, source is from input file or visibility query.\n   batch               batch operation on a list of workflows from query.\n\nOPTIONS:\n   --help, -h  show help\n\n\n$cadence wf signal -h\nNAME:\n   cadence workflow signal - signal a workflow execution\n\nUSAGE:\n   cadence workflow signal [command options] [arguments...]\n\nOPTIONS:\n   --workflow_id value, --wid value, -w value  WorkflowID\n   --run_id value, --rid value, -r value       RunID\n   --name value, -n value                      SignalName\n   --input value, -i value                     Input for the signal, in JSON format.\n   --input_file value, --if value              Input for the signal from JSON file.\n\n\n\nAnd etc.\n\nThe example commands below will use cadence for brevity.\n\n# Environment variables\nSetting environment variables for repeated parameters can shorten the commands.\n\n * CADENCE_CLI_ADDRESS - host:port for Cadence frontend service, the default is for the local server\n * CADENCE_CLI_DOMAIN - default , so you don\'t need to specify --domain\n\n# Quick Start\nRun cadence for help on top level commands and global options Run cadence domain for help on operations Run cadence workflow for help on operations Run cadence tasklist for help on tasklist operations (cadence help, cadence help [domain|workflow] will also print help messages)\n\nNote: make sure you have a Cadence server running before using \n\n# Domain operation examples\n * Register a new named "samples-domain":\n\ncadence --domain samples-domain domain register\n# OR using short alias\ncadence --do samples-domain d re \n\n\nIf your Cadence cluster has enable global domain(XDC replication), then you have to specify the replicaiton settings when registering a domain:\n\ncadence --domains amples-domain domain register --active_cluster clusterNameA --clusters clusterNameA clusterNameB\n\n\n * View "samples-domain" details:\n\ncadence --domain samples-domain domain describe\n\n\n# Workflow operation examples\nThe following examples assume the CADENCE_CLI_DOMAIN environment variable is set.\n\n# Run workflow\nStart a and see its progress. This command doesn\'t finish until completes.\n\ncadence workflow run --tl helloWorldGroup --wt main.Workflow --et 60 -i \'"cadence"\'\n\n# view help messages for workflow run\ncadence workflow run -h\n\n\nBrief explanation: To run a , the user must specify the following:\n\n 1. Tasklist name (--tl)\n 2. Workflow type (--wt)\n 3. Execution start to close timeout in seconds (--et)\n 4. Input in JSON format (--i) (optional)\n\ns example uses this cadence-samples workflowand takes a string as input with the -i \'"cadence"\' parameter. Single quotes (\'\') are used to wrap input as JSON.\n\nNote: You need to start the so that the can make progress. (Run make && ./bin/helloworld -m worker in cadence-samples to start the )\n\n# Show running workers of a tasklist\ncadence tasklist desc --tl helloWorldGroup\n\n\n# Start workflow\ncadence workflow start --tl helloWorldGroup --wt main.Workflow --et 60 -i \'"cadence"\'\n\n# view help messages for workflow start\ncadence workflow start -h\n\n# for a workflow with multiple inputs, separate each json with space/newline like\ncadence workflow start --tl helloWorldGroup --wt main.WorkflowWith3Args --et 60 -i \'"your_input_string" 123 {"Name":"my-string", "Age":12345}\'\n\n\nThe start command is similar to the run command, but immediately returns the workflow_id and run_id after starting the . Use the show command to view the \'s history/progress.\n\n# Reuse the same workflow id when starting/running a workflow\nUse option --workflowidreusepolicy or --wrp to configure the reuse policy.Option 0 AllowDuplicateFailedOnly: Allow starting a using the same when a with the same is not already running and the last execution close state is one of [terminated, cancelled, timedout, failed].Option 1 AllowDuplicate: Allow starting a using the same when a with the same is not already running.Option 2 RejectDuplicate: Do not allow starting a using the same as a previous .\n\n# use AllowDuplicateFailedOnly option to start a workflow\ncadence workflow start --tl helloWorldGroup --wt main.Workflow --et 60 -i \'"cadence"\' --wid "<duplicated workflow id>" --wrp 0\n\n# use AllowDuplicate option to run a workflow\ncadence workflow run --tl helloWorldGroup --wt main.Workflow --et 60 -i \'"cadence"\' --wid "<duplicated workflow id>" --wrp 1\n\n\n# Start a workflow with a memo\nMemos are immutable key/value pairs that can be attached to a run when starting the . These are visible when listing . More information on memos can be foundhere.\n\ncadence wf start -tl helloWorldGroup -wt main.Workflow -et 60 -i \'"cadence"\' -memo_key ‘“Service” “Env” “Instance”’ -memo ‘“serverName1” “test” 5’\n\n\n# Show workflow history\ncadence workflow show -w 3ea6b242-b23c-4279-bb13-f215661b4717 -r 866ae14c-88cf-4f1e-980f-571e031d71b0\n# a shortcut of this is (without -w -r flag)\ncadence workflow showid 3ea6b242-b23c-4279-bb13-f215661b4717 866ae14c-88cf-4f1e-980f-571e031d71b0\n\n# if run_id is not provided, it will show the latest run history of that workflow_id\ncadence workflow show -w 3ea6b242-b23c-4279-bb13-f215661b4717\n# a shortcut of this is\ncadence workflow showid 3ea6b242-b23c-4279-bb13-f215661b4717\n\n\n# Show workflow execution information\ncadence workflow describe -w 3ea6b242-b23c-4279-bb13-f215661b4717 -r 866ae14c-88cf-4f1e-980f-571e031d71b0\n# a shortcut of this is (without -w -r flag)\ncadence workflow describeid 3ea6b242-b23c-4279-bb13-f215661b4717 866ae14c-88cf-4f1e-980f-571e031d71b0\n\n# if run_id is not provided, it will show the latest workflow execution of that workflow_id\ncadence workflow describe -w 3ea6b242-b23c-4279-bb13-f215661b4717\n# a shortcut of this is\ncadence workflow describeid 3ea6b242-b23c-4279-bb13-f215661b4717\n\n\n# List closed or open workflow executions\ncadence workflow list\n\n# default will only show one page, to view more items, use --more flag\ncadence workflow list -m\n\n\nUse --query to list with SQL like \n\ncadence workflow list --query "WorkflowType=\'main.SampleParentWorkflow\' AND CloseTime = missing "\n\n\nThis will return all open with workflowType as "main.SampleParentWorkflow".\n\n# Query workflow execution\n# use custom query type\ncadence workflow query -w <wid> -r <rid> --qt <query-type>\n\n# use build-in query type "__stack_trace" which is supported by Cadence client library\ncadence workflow query -w <wid> -r <rid> --qt __stack_trace\n# a shortcut to query using __stack_trace is (without --qt flag)\ncadence workflow stack -w <wid> -r <rid>\n\n\n# Signal, cancel, terminate workflow\n# signal\ncadence workflow signal -w <wid> -r <rid> -n <signal-name> -i \'"signal-value"\'\n\n# cancel\ncadence workflow cancel -w <wid> -r <rid>\n\n# terminate\ncadence workflow terminate -w <wid> -r <rid> --reason\n\n\nTerminating a running will record a WorkflowExecutionTerminated as the closing in the history. No more will be scheduled for a terminated . Canceling a running will record a WorkflowExecutionCancelRequested in the history, and a new will be scheduled. The has a chance to do some clean up work after cancellation.\n\n# Signal, cancel, terminate workflows as a batch job\nBatch job is based on List Workflow Query(--query). It supports , cancel and terminate as batch job type. For terminating as batch job, it will terminte the children recursively.\n\nStart a batch job(using as batch type):\n\ncadence --do samples-domain wf batch start --query "WorkflowType=\'main.SampleParentWorkflow\' AND CloseTime=missing" --reason "test" --bt signal --sig testname\nThis batch job will be operating on 5 workflows.\nPlease confirm[Yes/No]:yes\n{\n    "jobID": "<batch-job-id>",\n    "msg": "batch job is started"\n}\n\n\n\nYou need to remember the JobID or use List command to get all your batch jobs:\n\ncadence --do samples-domain wf batch list\n\n\nDescribe the progress of a batch job:\n\ncadence --do samples-domain wf batch desc -jid <batch-job-id>\n\n\nTerminate a batch job:\n\ncadence --do samples-domain wf batch terminate -jid <batch-job-id>\n\n\nNote that the operation performed by a batch will not be rolled back by terminating the batch. However, you can use reset to rollback your .\n\n# Restart, reset workflow\nThe Reset command allows resetting a to a particular point and continue running from there. There are a lot of use cases:\n\n * Rerun a failed from the beginning with the same start parameters.\n * Rerun a failed from the failing point without losing the achieved progress(history).\n * After deploying new code, reset an open to let the run to different flows.\n\nYou can reset to some predefined types:\n\ncadence workflow reset -w <wid> -r <rid> --reset_type <reset_type> --reason "some_reason"\n\n\n * FirstDecisionCompleted: reset to the beginning of the history.\n * LastDecisionCompleted: reset to the end of the history.\n * LastContinuedAsNew: reset to the end of the history for the previous run.\n\nIf you are familiar with the Cadence history , You can also reset to any finish by using:\n\ncadence workflow reset -w <wid> -r <rid> --event_id <decision_finish_event_id> --reason "some_reason"\n\n\nSome things to note:\n\n * When reset, a new run will be kicked off with the same workflowID. But if there is a running execution for the workflow(workflowID), the current run will be terminated.\n * decision_finish_event_id is the ID of of the type: DecisionTaskComplete/DecisionTaskFailed/DecisionTaskTimeout.\n * To restart a from the beginning, reset to the first finish .\n\nTo reset multiple , you can use batch reset command:\n\ncadence workflow reset-batch --input_file <file_of_workflows_to_reset> --reset_type <reset_type> --reason "some_reason"\n\n\n# Recovery from bad deployment -- auto-reset workflow\nIf a bad deployment lets a run into a wrong state, you might want to reset the to the point that the bad deployment started to run. But usually it is not easy to find out all the impacted, and every reset point for each . In this case, auto-reset will automatically reset all the given a bad deployment identifier.\n\nLet\'s get familiar with some concepts. Each deployment will have an identifier, we call it "Binary Checksum" as it is usually generated by the md5sum of a binary file. For a , each binary checksum will be associated with an auto-reset point, which contains a runID, an eventID, and the created_time that binary/deployment made the first for the .\n\nTo find out which binary checksum of the bad deployment to reset, you should be aware of at least one running into a bad state. Use the describe command with --reset_points_only option to show all the reset points:\n\ncadence wf desc -w <WorkflowID>  --reset_points_only\n+----------------------------------+--------------------------------+--------------------------------------+---------+\n|         BINARY CHECKSUM          |          CREATE TIME           |                RUNID                 | EVENTID |\n+----------------------------------+--------------------------------+--------------------------------------+---------+\n| c84c5afa552613a83294793f4e664a7f | 2019-05-24 10:01:00.398455019  | 2dd29ab7-2dd8-4668-83e0-89cae261cfb1 |       4 |\n| aae748fdc557a3f873adbe1dd066713f | 2019-05-24 11:01:00.067691445  | d42d21b8-2adb-4313-b069-3837d44d6ce6 |       4 |\n...\n...\n\n\nThen use this command to tell Cadence to auto-reset all impacted by the bad deployment. The command will store the bad binary checksum into info and trigger a process to reset all your .\n\ncadence --do <YourDomainName> domain update --add_bad_binary aae748fdc557a3f873adbe1dd066713f  --reason "rollback bad deployment"\n\n\nAs you add the bad binary checksum to your , Cadence will not dispatch any to the bad binary. So make sure that you have rolled back to a good deployment(or roll out new bits with bug fixes). Otherwise your can\'t make any progress after auto-reset.',normalizedContent:'# command line interface\nthe cadence is a command-line tool you can use to perform various on a cadence server. it can perform operations such as register, update, and describe as well as operations like start, show history, and .\n\n# using the cli\n# homebrew\nbrew install cadence-workflow\n\n\nafter the installation is done, you can use cli:\n\ncadence --help\n\n\n# docker\nthe cadence can be used directly from the docker hub image ubercadence/cli or by building the tool locally.\n\nexample of using the docker image to describe a \n\ndocker run --rm ubercadence/cli:master --domain samples-domain domain describe\n\n\nmaster will be the latest cli binary from the project. but you can specify a version to best match your server version:\n\ndocker run --rm ubercadence/cli:<version> --domain samples-domain domain describe\n\n\nfor example docker run --rm ubercadence/cli:0.21.3 --domain samples-domain domain describe will be the cli that is released as part of the v0.21.3 release. see docker hub page for all the cli image tags.\n\nnote: on docker versions 18.03 and later, you may get a "connection refused" error. you can work around this by setting the host to "host.docker.internal" (see here for more info).\n\ndocker run --rm ubercadence/cli:master --address host.docker.internal:7933 --domain samples-domain domain describe\n\n\nnote: be sure to update your image when you want to try new features: docker pull ubercadence/cli:master\n\nnote: if you are running docker-compose cadence server, you can also logon to the container to execute cli:\n\ndocker exec -it docker_cadence_1 /bin/bash\n\n# cadence --address $(hostname -i):7933 --do samples domain register\n\n\n# build it yourself\nto build the tool locally, clone the cadence server repo, check out the version tag (e.g. git checkout v0.21.3) and runmake tools. this produces an executable called cadence. with a local build, the same command to describe a would look like this:\n\ncadence --domain samples-domain domain describe\n\n\nalternatively, you can build the cli image, see instructions\n\n# documentation\ncli are documented by --help or -h in any tab of all levels:\n\n$cadence --help\nname:\n   cadence - a command-line tool for cadence users\n\nusage:\n   cadence [global options] command [command options] [arguments...]\n\nversion:\n   0.18.4\n\ncommands:\n   domain, d     operate cadence domain\n   workflow, wf  operate cadence workflow\n   tasklist, tl  operate cadence tasklist\n   admin, adm    run admin operation\n   cluster, cl   operate cadence cluster\n   help, h       shows a list of commands or help for one command\n\nglobal options:\n   --address value, --ad value          host:port for cadence frontend service [$cadence_cli_address]\n   --domain value, --do value           cadence workflow domain [$cadence_cli_domain]\n   --context_timeout value, --ct value  optional timeout for context of rpc call in seconds (default: 5) [$cadence_context_timeout]\n   --help, -h                           show help\n   --version, -v                        print the version\n\n\nand\n\n$cadence workflow -h\nname:\n   cadence workflow - operate cadence workflow\n\nusage:\n   cadence workflow command [command options] [arguments...]\n\ncommands:\n   activity, act       operate activities of workflow\n   show                show workflow history\n   showid              show workflow history with given workflow_id and run_id (a shortcut of `show -w <wid> -r <rid>`). run_id is only required for archived history\n   start               start a new workflow execution\n   run                 start a new workflow execution and get workflow progress\n   cancel, c           cancel a workflow execution\n   signal, s           signal a workflow execution\n   signalwithstart     signal the current open workflow if exists, or attempt to start a new run based on idresuepolicy and signals it\n   terminate, term     terminate a new workflow execution\n   list, l             list open or closed workflow executions\n   listall, la         list all open or closed workflow executions\n   listarchived        list archived workflow executions\n   scan, sc, scanall   scan workflow executions (need to enable cadence server on elasticsearch). it will be faster than listall, but result are not sorted.\n   count, cnt          count number of workflow executions (need to enable cadence server on elasticsearch)\n   query               query workflow execution\n   stack               query workflow execution with __stack_trace as query type\n   describe, desc      show information of workflow execution\n   describeid, descid  show information of workflow execution with given workflow_id and optional run_id (a shortcut of `describe -w <wid> -r <rid>`)\n   observe, ob         show the progress of workflow history\n   observeid, obid     show the progress of workflow history with given workflow_id and optional run_id (a shortcut of `observe -w <wid> -r <rid>`)\n   reset, rs           reset the workflow, by either eventid or resettype.\n   reset-batch         reset workflow in batch by resettype: lastdecisioncompleted,lastcontinuedasnew,badbinary,decisioncompletedtime,firstdecisionscheduled,lastdecisionscheduled,firstdecisioncompletedto get base workflowids/runids to reset, source is from input file or visibility query.\n   batch               batch operation on a list of workflows from query.\n\noptions:\n   --help, -h  show help\n\n\n$cadence wf signal -h\nname:\n   cadence workflow signal - signal a workflow execution\n\nusage:\n   cadence workflow signal [command options] [arguments...]\n\noptions:\n   --workflow_id value, --wid value, -w value  workflowid\n   --run_id value, --rid value, -r value       runid\n   --name value, -n value                      signalname\n   --input value, -i value                     input for the signal, in json format.\n   --input_file value, --if value              input for the signal from json file.\n\n\n\nand etc.\n\nthe example commands below will use cadence for brevity.\n\n# environment variables\nsetting environment variables for repeated parameters can shorten the commands.\n\n * cadence_cli_address - host:port for cadence frontend service, the default is for the local server\n * cadence_cli_domain - default , so you don\'t need to specify --domain\n\n# quick start\nrun cadence for help on top level commands and global options run cadence domain for help on operations run cadence workflow for help on operations run cadence tasklist for help on tasklist operations (cadence help, cadence help [domain|workflow] will also print help messages)\n\nnote: make sure you have a cadence server running before using \n\n# domain operation examples\n * register a new named "samples-domain":\n\ncadence --domain samples-domain domain register\n# or using short alias\ncadence --do samples-domain d re \n\n\nif your cadence cluster has enable global domain(xdc replication), then you have to specify the replicaiton settings when registering a domain:\n\ncadence --domains amples-domain domain register --active_cluster clusternamea --clusters clusternamea clusternameb\n\n\n * view "samples-domain" details:\n\ncadence --domain samples-domain domain describe\n\n\n# workflow operation examples\nthe following examples assume the cadence_cli_domain environment variable is set.\n\n# run workflow\nstart a and see its progress. this command doesn\'t finish until completes.\n\ncadence workflow run --tl helloworldgroup --wt main.workflow --et 60 -i \'"cadence"\'\n\n# view help messages for workflow run\ncadence workflow run -h\n\n\nbrief explanation: to run a , the user must specify the following:\n\n 1. tasklist name (--tl)\n 2. workflow type (--wt)\n 3. execution start to close timeout in seconds (--et)\n 4. input in json format (--i) (optional)\n\ns example uses this cadence-samples workflowand takes a string as input with the -i \'"cadence"\' parameter. single quotes (\'\') are used to wrap input as json.\n\nnote: you need to start the so that the can make progress. (run make && ./bin/helloworld -m worker in cadence-samples to start the )\n\n# show running workers of a tasklist\ncadence tasklist desc --tl helloworldgroup\n\n\n# start workflow\ncadence workflow start --tl helloworldgroup --wt main.workflow --et 60 -i \'"cadence"\'\n\n# view help messages for workflow start\ncadence workflow start -h\n\n# for a workflow with multiple inputs, separate each json with space/newline like\ncadence workflow start --tl helloworldgroup --wt main.workflowwith3args --et 60 -i \'"your_input_string" 123 {"name":"my-string", "age":12345}\'\n\n\nthe start command is similar to the run command, but immediately returns the workflow_id and run_id after starting the . use the show command to view the \'s history/progress.\n\n# reuse the same workflow id when starting/running a workflow\nuse option --workflowidreusepolicy or --wrp to configure the reuse policy.option 0 allowduplicatefailedonly: allow starting a using the same when a with the same is not already running and the last execution close state is one of [terminated, cancelled, timedout, failed].option 1 allowduplicate: allow starting a using the same when a with the same is not already running.option 2 rejectduplicate: do not allow starting a using the same as a previous .\n\n# use allowduplicatefailedonly option to start a workflow\ncadence workflow start --tl helloworldgroup --wt main.workflow --et 60 -i \'"cadence"\' --wid "<duplicated workflow id>" --wrp 0\n\n# use allowduplicate option to run a workflow\ncadence workflow run --tl helloworldgroup --wt main.workflow --et 60 -i \'"cadence"\' --wid "<duplicated workflow id>" --wrp 1\n\n\n# start a workflow with a memo\nmemos are immutable key/value pairs that can be attached to a run when starting the . these are visible when listing . more information on memos can be foundhere.\n\ncadence wf start -tl helloworldgroup -wt main.workflow -et 60 -i \'"cadence"\' -memo_key ‘“service” “env” “instance”’ -memo ‘“servername1” “test” 5’\n\n\n# show workflow history\ncadence workflow show -w 3ea6b242-b23c-4279-bb13-f215661b4717 -r 866ae14c-88cf-4f1e-980f-571e031d71b0\n# a shortcut of this is (without -w -r flag)\ncadence workflow showid 3ea6b242-b23c-4279-bb13-f215661b4717 866ae14c-88cf-4f1e-980f-571e031d71b0\n\n# if run_id is not provided, it will show the latest run history of that workflow_id\ncadence workflow show -w 3ea6b242-b23c-4279-bb13-f215661b4717\n# a shortcut of this is\ncadence workflow showid 3ea6b242-b23c-4279-bb13-f215661b4717\n\n\n# show workflow execution information\ncadence workflow describe -w 3ea6b242-b23c-4279-bb13-f215661b4717 -r 866ae14c-88cf-4f1e-980f-571e031d71b0\n# a shortcut of this is (without -w -r flag)\ncadence workflow describeid 3ea6b242-b23c-4279-bb13-f215661b4717 866ae14c-88cf-4f1e-980f-571e031d71b0\n\n# if run_id is not provided, it will show the latest workflow execution of that workflow_id\ncadence workflow describe -w 3ea6b242-b23c-4279-bb13-f215661b4717\n# a shortcut of this is\ncadence workflow describeid 3ea6b242-b23c-4279-bb13-f215661b4717\n\n\n# list closed or open workflow executions\ncadence workflow list\n\n# default will only show one page, to view more items, use --more flag\ncadence workflow list -m\n\n\nuse --query to list with sql like \n\ncadence workflow list --query "workflowtype=\'main.sampleparentworkflow\' and closetime = missing "\n\n\nthis will return all open with workflowtype as "main.sampleparentworkflow".\n\n# query workflow execution\n# use custom query type\ncadence workflow query -w <wid> -r <rid> --qt <query-type>\n\n# use build-in query type "__stack_trace" which is supported by cadence client library\ncadence workflow query -w <wid> -r <rid> --qt __stack_trace\n# a shortcut to query using __stack_trace is (without --qt flag)\ncadence workflow stack -w <wid> -r <rid>\n\n\n# signal, cancel, terminate workflow\n# signal\ncadence workflow signal -w <wid> -r <rid> -n <signal-name> -i \'"signal-value"\'\n\n# cancel\ncadence workflow cancel -w <wid> -r <rid>\n\n# terminate\ncadence workflow terminate -w <wid> -r <rid> --reason\n\n\nterminating a running will record a workflowexecutionterminated as the closing in the history. no more will be scheduled for a terminated . canceling a running will record a workflowexecutioncancelrequested in the history, and a new will be scheduled. the has a chance to do some clean up work after cancellation.\n\n# signal, cancel, terminate workflows as a batch job\nbatch job is based on list workflow query(--query). it supports , cancel and terminate as batch job type. for terminating as batch job, it will terminte the children recursively.\n\nstart a batch job(using as batch type):\n\ncadence --do samples-domain wf batch start --query "workflowtype=\'main.sampleparentworkflow\' and closetime=missing" --reason "test" --bt signal --sig testname\nthis batch job will be operating on 5 workflows.\nplease confirm[yes/no]:yes\n{\n    "jobid": "<batch-job-id>",\n    "msg": "batch job is started"\n}\n\n\n\nyou need to remember the jobid or use list command to get all your batch jobs:\n\ncadence --do samples-domain wf batch list\n\n\ndescribe the progress of a batch job:\n\ncadence --do samples-domain wf batch desc -jid <batch-job-id>\n\n\nterminate a batch job:\n\ncadence --do samples-domain wf batch terminate -jid <batch-job-id>\n\n\nnote that the operation performed by a batch will not be rolled back by terminating the batch. however, you can use reset to rollback your .\n\n# restart, reset workflow\nthe reset command allows resetting a to a particular point and continue running from there. there are a lot of use cases:\n\n * rerun a failed from the beginning with the same start parameters.\n * rerun a failed from the failing point without losing the achieved progress(history).\n * after deploying new code, reset an open to let the run to different flows.\n\nyou can reset to some predefined types:\n\ncadence workflow reset -w <wid> -r <rid> --reset_type <reset_type> --reason "some_reason"\n\n\n * firstdecisioncompleted: reset to the beginning of the history.\n * lastdecisioncompleted: reset to the end of the history.\n * lastcontinuedasnew: reset to the end of the history for the previous run.\n\nif you are familiar with the cadence history , you can also reset to any finish by using:\n\ncadence workflow reset -w <wid> -r <rid> --event_id <decision_finish_event_id> --reason "some_reason"\n\n\nsome things to note:\n\n * when reset, a new run will be kicked off with the same workflowid. but if there is a running execution for the workflow(workflowid), the current run will be terminated.\n * decision_finish_event_id is the id of of the type: decisiontaskcomplete/decisiontaskfailed/decisiontasktimeout.\n * to restart a from the beginning, reset to the first finish .\n\nto reset multiple , you can use batch reset command:\n\ncadence workflow reset-batch --input_file <file_of_workflows_to_reset> --reset_type <reset_type> --reason "some_reason"\n\n\n# recovery from bad deployment -- auto-reset workflow\nif a bad deployment lets a run into a wrong state, you might want to reset the to the point that the bad deployment started to run. but usually it is not easy to find out all the impacted, and every reset point for each . in this case, auto-reset will automatically reset all the given a bad deployment identifier.\n\nlet\'s get familiar with some concepts. each deployment will have an identifier, we call it "binary checksum" as it is usually generated by the md5sum of a binary file. for a , each binary checksum will be associated with an auto-reset point, which contains a runid, an eventid, and the created_time that binary/deployment made the first for the .\n\nto find out which binary checksum of the bad deployment to reset, you should be aware of at least one running into a bad state. use the describe command with --reset_points_only option to show all the reset points:\n\ncadence wf desc -w <workflowid>  --reset_points_only\n+----------------------------------+--------------------------------+--------------------------------------+---------+\n|         binary checksum          |          create time           |                runid                 | eventid |\n+----------------------------------+--------------------------------+--------------------------------------+---------+\n| c84c5afa552613a83294793f4e664a7f | 2019-05-24 10:01:00.398455019  | 2dd29ab7-2dd8-4668-83e0-89cae261cfb1 |       4 |\n| aae748fdc557a3f873adbe1dd066713f | 2019-05-24 11:01:00.067691445  | d42d21b8-2adb-4313-b069-3837d44d6ce6 |       4 |\n...\n...\n\n\nthen use this command to tell cadence to auto-reset all impacted by the bad deployment. the command will store the bad binary checksum into info and trigger a process to reset all your .\n\ncadence --do <yourdomainname> domain update --add_bad_binary aae748fdc557a3f873adbe1dd066713f  --reason "rollback bad deployment"\n\n\nas you add the bad binary checksum to your , cadence will not dispatch any to the bad binary. so make sure that you have rolled back to a good deployment(or roll out new bits with bug fixes). otherwise your can\'t make any progress after auto-reset.',charsets:{cjk:!0}},{title:"Cluster Configuration",frontmatter:{layout:"default",title:"Cluster Configuration",permalink:"/docs/operation-guide/setup",readingShow:"top"},regularPath:"/docs/07-operation-guide/01-setup.html",relativePath:"docs/07-operation-guide/01-setup.md",key:"v-94771a14",path:"/docs/operation-guide/setup/",headers:[{level:2,title:"Static configuration",slug:"static-configuration",normalizedTitle:"static configuration",charIndex:804},{level:3,title:"Understand the basic static configuration",slug:"understand-the-basic-static-configuration",normalizedTitle:"understand the basic static configuration",charIndex:827},{level:3,title:"The full list of static configuration",slug:"the-full-list-of-static-configuration",normalizedTitle:"the full list of static configuration",charIndex:22140},{level:2,title:"Dynamic Configuration",slug:"dynamic-configuration",normalizedTitle:"dynamic configuration",charIndex:233},{level:3,title:"How to update Dynamic Configuration",slug:"how-to-update-dynamic-configuration",normalizedTitle:"how to update dynamic configuration",charIndex:27019},{level:2,title:"Other Advanced Features",slug:"other-advanced-features",normalizedTitle:"other advanced features",charIndex:28850},{level:2,title:"Deployment & Release",slug:"deployment-release",normalizedTitle:"deployment &amp; release",charIndex:null}],headersStr:"Static configuration Understand the basic static configuration The full list of static configuration Dynamic Configuration How to update Dynamic Configuration Other Advanced Features Deployment & Release",content:'# Cluster Configuration\nThis section will help to understand what you need for setting up a Cadence cluster.\n\nYou should understand some basic static configuration of Cadence cluster.\n\nThere are also many other configuration called "Dynamic Configuration" for fine tuning the cluster. The default values are good to go for small clusters.\n\nCadence’s minimum dependency is a database(Cassandra or SQL based like MySQL/Postgres). Cadence uses it for persistence. All instances of Cadence clusters are stateless.\n\nFor production you also need a metric server(Prometheus/Statsd/M3/etc).\n\nFor advanced features Cadence depends on others like ElastiCache+Kafka if you need Advanced visibility feature to search workflows. Cadence will depends on a blob store like S3 if you need to enable archival feature.\n\n# Static configuration\n# Understand the basic static configuration\nThere are quite many configs in Cadence. Here are the most basic configuration that you should understand.\n\nConfig name             Explanation                                                                                                                                                                                                                                                                                                                                          Recommended value                                                                                                                                                  \nnumHistoryShards        This is the most important one in Cadence config.It will be a fixed number in the cluster forever. The only way to change it is to migrate to another cluster. Refer to Migrate cluster section.                                                                                                                                                     1K~16K depending on the size ranges of the cluster you expect to run, and the instance size. Typically 2K for SQL based persistence, and 8K for Cassandra based.   \n                        Some facts about it:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                        1. Each workflow will be mapped to a single shard. Within a shard, all the workflow creation/updates are serialized.                                                                                                                                                                                                                                                                                                                                                                                                    \n                        2. Each shard will be assigned to only one History node to own the shard, using a Consistent Hashing Ring. Each shard will consume a small amount of memory/CPU to do background processing. Therefore, a single History node cannot own too many shards. You may need to figure out a good number range based on your instance size(memory/CPU).                                                                                                                                                                       \n                        3. Also, you can’t add an infinite number of nodes to a cluster because this config is fixed. When the number of History nodes is closed or equal to numHistoryShards, there will be some History nodes that have no shards assigned to it. This will be wasting resources.                                                                                                                                                                                                                                             \n                        Based on above, you don’t want to have a small number of shards which will limit the maximum size of your cluster. You also don’t want to have a too big number, which will require you to have a quite big initial size of the cluster.                                                                                                                                                                                                                                                                                \n                        Also, typically a production cluster will start with a smaller number and then we add more nodes/hosts to it. But to keep high availability, it’s recommended to use at least 4 nodes for each service(Frontend/History/Matching) at the beginning.                                                                                                                                                                                                                                                                     \nringpop                 This is the config to let all nodes of all services connected to each other. ALL the bootstrap nodes MUST be reachable by ringpop when a service is starting up, within a MaxJoinDuration. defaultMaxJoinDuration is 2 minutes.                                                                                                                      For dns mode: Recommended to put the DNS of Frontend service                                                                                                       \n                        It’s not required that bootstrap nodes need to be Frontend/History or Matching. In fact, it can be running none of them as long as it runs Ringpop protocol.                                                                                                                                                                                         For hosts or hostfile mode: A list of Frontend service node addresses if using hosts mode. Make sure all the bootstrap nodes are reachable at startup.             \npublicClient            The Cadence Frontend service addresses that internal Cadence system(like system workflows) need to talk to.                                                                                                                                                                                                                                          Recommended be DNS of Frontend service, so that requests will be distributed to all Frontend nodes.                                                                \n                        After connected, all nodes in Ringpop will form a ring with identifiers of what service they serve. Ideally Cadence should be able to get Frontend address from there. But Ringpop doesn’t expose this API yet.                                                                                                                                      Using localhost+Port or local container IP address+Port will not work if the IP/container is not running frontend                                                  \nservices.NAME.rpc       Configuration of how to listen to network ports and serve traffic.                                                                                                                                                                                                                                                                                   Name: Use as recommended in development.yaml. bindOnIP : an IP address that the container will serve the traffic with                                              \n                        bindOnLocalHost:true will bind on 127.0.0.1. It’s mostly for local development. In production usually you have to specify the IP that containers will use by using bindOnIP                                                                                                                                                                                                                                                                                                                                             \n                        NAME is the matter for the “--services” option in the server startup command.                                                                                                                                                                                                                                                                                                                                                                                                                                           \nservices.NAME.pprof     Golang profiling service , will bind on the same IP as RPC                                                                                                                                                                                                                                                                                           a port that you want to serve pprof request                                                                                                                        \nservices.Name.metrics   See Metrics&Logging section                                                                                                                                                                                                                                                                                                                          cc                                                                                                                                                                 \nclusterMetadata         Cadence cluster configuration.                                                                                                                                                                                                                                                                                                                       As explanation.                                                                                                                                                    \n                        enableGlobalDomain：true will enable Cadence Cross datacenter replication(aka XDC) feature.                                                                                                                                                                                                                                                                                                                                                                                                                              \n                        failoverVersionIncrement: This decides the maximum clusters that you will have replicated to each other at the same time. For example 10 is sufficient for most cases.                                                                                                                                                                                                                                                                                                                                                  \n                        masterClusterName: a master cluster must be one of the enabled clusters, usually the very first cluster to start. It is only meaningful for internal purposes.                                                                                                                                                                                                                                                                                                                                                          \n                        currentClusterName: current cluster name using this config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n                        clusterInformation is a map from clusterName to the cluster configure                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n                        initialFailoverVersion: each cluster must use a different value from 0 to failoverVersionIncrement-1.                                                                                                                                                                                                                                                                                                                                                                                                                   \n                        rpcName: must be “cadence-frontend”. Can be improved in this issue.                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n                        rpcAddress: the address to talk to the Frontend of the cluster for inter-cluster replication.                                                                                                                                                                                                                                                                                                                                                                                                                           \n                        Note that even if you don’t need XDC replication right now, if you want to migrate data stores in the future, you should enable xdc from every beginning. You just need to use the same name of cluster for both masterClusterName and currentClusterName.                                                                                                                                                                                                                                                              \n                        Go to cross dc replication for how to configure replication in production                                                                                                                                                                                                                                                                                                                                                                                                                                               \ndcRedirectionPolicy     For allowing forwarding frontend requests from passive cluster to active clusters.                                                                                                                                                                                                                                                                   “selected-apis-forwarding”                                                                                                                                         \narchival                This is for archival history feature, skip if you don’t need it. Go to workflow archival for how to configure archival in production                                                                                                                                                                                                                 N/A                                                                                                                                                                \nblobstore               This is also for archival history feature Default cadence server is using file based blob store implementation.                                                                                                                                                                                                                                      N/A                                                                                                                                                                \ndomainDefaults          default config for each domain. Right now only being used for Archival feature.                                                                                                                                                                                                                                                                      N/A                                                                                                                                                                \ndynamicConfigClient     Dynamic config is a config manager that you can change config without restarting servers. It’s a good way for Cadence to keep high availability and make things easy to configure.                                                                                                                                                                   Same as the sample development config                                                                                                                              \n                        Default cadence server is using FileBasedClientConfig. But you can implement the dynamic config interface if you have a better way to manage.                                                                                                                                                                                                                                                                                                                                                                           \npersistence             Configuration for data store / persistence layer.                                                                                                                                                                                                                                                                                                    As explanation                                                                                                                                                     \n                        Values of DefaultStore VisibilityStore AdvancedVisibilityStore should be keys of map DataStores.                                                                                                                                                                                                                                                                                                                                                                                                                        \n                        DefaultStore is for core Cadence functionality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                        VisibilityStore is for basic visibility feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                        AdvancedVisibilityStore is for advanced visibility                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n                        Go to advanced visibility for detailed configuration of advanced visibility. See persistence documentation about using different database for Cadence                                                                                                                                                                                                                                                                                                                                                                   \n\n# The full list of static configuration\nStarting from v0.21.0, all the static configuration are defined by GoDocs in details.\n\nVersion                    GoDocs Link                                    Github Link                                    \nv0.21.0                    Configuration Docs                             Configuration                                  \n...other higher versions   ...Replace the version in the URL of v0.21.0   ...Replace the version in the URL of v0.21.0   \n\nFor earlier versions, you can find all the configurations similarly:\n\nVersion                   GoDocs Link                                    Github Link                                    \nv0.20.0                   Configuration Docs                             Configuration                                  \nv0.19.2                   Configuration Docs                             Configuration                                  \nv0.18.2                   Configuration Docs                             Configuration                                  \nv0.17.0                   Configuration Docs                             Configuration                                  \n...other lower versions   ...Replace the version in the URL of v0.20.0   ...Replace the version in the URL of v0.20.0   \n\n# Dynamic Configuration\nDynamic configuration is for fine tuning a Cadence cluster.\n\nThere are a lot more dynamic configurations than static configurations. Most of the default values are good for small clusters. As a cluster is scaled up, you may look for tuning it for the optimal performance.\n\nStarting from v0.21.0 with this change, all the dynamic configuration are well defined by GoDocs.\n\nVersion                    GoDocs Link                                    Github Link                                    \nv0.21.0                    Dynamic Configuration Docs                     Dynamic Configuration                          \n...other higher versions   ...Replace the version in the URL of v0.21.0   ...Replace the version in the URL of v0.21.0   \n\nFor earlier versions, you can find all the configurations similarly:\n\nVersion                   GoDocs Link                                    Github Link                                    \nv0.20.0                   Dynamic Configuration Docs                     Dynamic Configuration                          \nv0.19.2                   Dynamic Configuration Docs                     Dynamic Configuration                          \nv0.18.2                   Dynamic Configuration Docs                     Dynamic Configuration                          \nv0.17.0                   Dynamic Configuration Docs                     Dynamic Configuration                          \n...other lower versions   ...Replace the version in the URL of v0.20.0   ...Replace the version in the URL of v0.20.0   \n\nHowever, the GoDocs in earlier versions don\'t contain detailed information. You need to look it up the newer version of GoDocs.\nFor example, search for "EnableGlobalDomain" in Dynamic Configuration Comments in v0.21.0 or Docs of v0.21.0, as the usage of DynamicConfiguration never changes.\n\n * KeyName is the key that you will use in the dynamicconfig yaml content\n * Default value is the default value\n * Value type indicates the type that you should change the yaml value of: * Int should be integer like 123\n    * Float should be number like 123.4\n    * Duration should be Golang duration like: 10s, 2m, 5h for 10 seconds, 2 minutes and 5 hours.\n    * Bool should be true or false\n    * Map should be map of yaml\n   \n   \n * Allowed filters indicates what kinds of filters you can set as constraints with the dynamic configuration. * DomainName can be used with domainName\n    * N/A means no filters can be set. The config will be global.\n   \n   \n\nFor example, if you want to change the ratelimiting for List API, below is the config:\n\n// FrontendVisibilityListMaxQPS is max qps frontend can list open/close workflows\n// KeyName: frontend.visibilityListMaxQPS\n// Value type: Int\n// Default value: 10\n// Allowed filters: DomainName\nFrontendVisibilityListMaxQPS\n\n\nThen you can add the config like:\n\nfrontend.visibilityListMaxQPS:\n  - value: 1000\n  constraints:\n    domainName: "domainA"\n    frontend.visibilityListMaxQPS:\n  - value: 2000\n  constraints:\n    domainName: "domainB"      \n\n\nYou will expect to see domainA will be able to perform 1K List operation per second, while domainB can perform 2K per second.\n\nNOTE 1: the size related configuration numbers are based on byte.\n\nNOTE 2: for <frontend,history,matching>.persistenceMaxQPS versus <frontend,history,matching>.persistenceGlobalMaxQPS --- persistenceMaxQPS is local for single node while persistenceGlobalMaxQPS is global for all node. persistenceGlobalMaxQPS is preferred if set as greater than zero. But by default it is zero so persistenceMaxQPS is being used.\n\n# How to update Dynamic Configuration\n * Local docker-compose by mounting volume: update the cadence section in the docker compose file, and mount config folder to host machine like the following:\n\ncadence:\n  image: ubercadence/server:master-auto-setup\n  ports:\n    ...(don\'t change anything here)\n  environment:\n    ...(don\'t change anything here)\n    - "DYNAMIC_CONFIG_FILE_PATH=/etc/custom-dynamicconfig/development.yaml"\n  volumes:\n    - "/Users/<?>/cadence/config/dynamicconfig:/etc/custom-dynamicconfig"\n\n\n * Local docker-compose by logging into the container: run docker exec -it docker_cadence_1 /bin/bash to login your container. Then vi config/dynamicconfig/development.yaml to make any change. After you changed the config, use docker restart docker_cadence_1 to restart the cadence instance. Note that you can also use this approach to change static config, but it must be changed through config/config_template.yaml instead of config/docker.yaml because config/docker.yaml is generated on startup.\n   \n   \n * In production cluster: Follow this example of Helm Chart to deploy Cadence, update dynamic config here and restart the cluster.\n   \n   \n * DEBUG: How to make sure your updates on dynamicconfig is loaded? for example, if you added the following to development.yaml\n   \n   \n\nfrontend.visibilityListMaxQPS:\n  - value: 10000\n\n\nAfter restarting Cadence instances, execute a command like this to let Cadence load the config(it\'s lazy loading when using it).cadence --domain <> workflow list\n\nThen you should see the logs like below\n\ncadence_1        | {"level":"info","ts":"2021-05-07T18:43:07.869Z","msg":"First loading dynamic config","service":"cadence-frontend","key":"frontend.visibilityListMaxQPS,domainName:sample,clusterName:primary","value":"10000","default-value":"10","logging-call-at":"config.go:93"}\n\n\n# Other Advanced Features\n * Go to advanced visibility for how to configure advanced visibility in production.\n   \n   \n * Go to workflow archival for how to configure archival in production.\n   \n   \n * Go to cross dc replication for how to configure replication in production.\n   \n   \n\n# Deployment & Release\nKubernetes is the most popular way to deploy Cadence cluster. And easiest way is to use Cadence Helm Charts that maintained by a community project.\n\nIf you are looking for deploying Cadence using other technologies, then it\'s reccomended to use Cadence docker images. You can use offical ones, or you may customize it based on what you need. See Cadence docker package for how to run the images.\n\nIt\'s always recommended to use the latest release. See Cadence release pages.\n\nPlease subscribe the release of project by :\n\nGo to https://github.com/uber/cadence -> Click the right top "Watch" button -> Custom -> "Release".\n\nAnd see how to upgrade a Cadence cluster',normalizedContent:'# cluster configuration\nthis section will help to understand what you need for setting up a cadence cluster.\n\nyou should understand some basic static configuration of cadence cluster.\n\nthere are also many other configuration called "dynamic configuration" for fine tuning the cluster. the default values are good to go for small clusters.\n\ncadence’s minimum dependency is a database(cassandra or sql based like mysql/postgres). cadence uses it for persistence. all instances of cadence clusters are stateless.\n\nfor production you also need a metric server(prometheus/statsd/m3/etc).\n\nfor advanced features cadence depends on others like elasticache+kafka if you need advanced visibility feature to search workflows. cadence will depends on a blob store like s3 if you need to enable archival feature.\n\n# static configuration\n# understand the basic static configuration\nthere are quite many configs in cadence. here are the most basic configuration that you should understand.\n\nconfig name             explanation                                                                                                                                                                                                                                                                                                                                          recommended value                                                                                                                                                  \nnumhistoryshards        this is the most important one in cadence config.it will be a fixed number in the cluster forever. the only way to change it is to migrate to another cluster. refer to migrate cluster section.                                                                                                                                                     1k~16k depending on the size ranges of the cluster you expect to run, and the instance size. typically 2k for sql based persistence, and 8k for cassandra based.   \n                        some facts about it:                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n                        1. each workflow will be mapped to a single shard. within a shard, all the workflow creation/updates are serialized.                                                                                                                                                                                                                                                                                                                                                                                                    \n                        2. each shard will be assigned to only one history node to own the shard, using a consistent hashing ring. each shard will consume a small amount of memory/cpu to do background processing. therefore, a single history node cannot own too many shards. you may need to figure out a good number range based on your instance size(memory/cpu).                                                                                                                                                                       \n                        3. also, you can’t add an infinite number of nodes to a cluster because this config is fixed. when the number of history nodes is closed or equal to numhistoryshards, there will be some history nodes that have no shards assigned to it. this will be wasting resources.                                                                                                                                                                                                                                             \n                        based on above, you don’t want to have a small number of shards which will limit the maximum size of your cluster. you also don’t want to have a too big number, which will require you to have a quite big initial size of the cluster.                                                                                                                                                                                                                                                                                \n                        also, typically a production cluster will start with a smaller number and then we add more nodes/hosts to it. but to keep high availability, it’s recommended to use at least 4 nodes for each service(frontend/history/matching) at the beginning.                                                                                                                                                                                                                                                                     \nringpop                 this is the config to let all nodes of all services connected to each other. all the bootstrap nodes must be reachable by ringpop when a service is starting up, within a maxjoinduration. defaultmaxjoinduration is 2 minutes.                                                                                                                      for dns mode: recommended to put the dns of frontend service                                                                                                       \n                        it’s not required that bootstrap nodes need to be frontend/history or matching. in fact, it can be running none of them as long as it runs ringpop protocol.                                                                                                                                                                                         for hosts or hostfile mode: a list of frontend service node addresses if using hosts mode. make sure all the bootstrap nodes are reachable at startup.             \npublicclient            the cadence frontend service addresses that internal cadence system(like system workflows) need to talk to.                                                                                                                                                                                                                                          recommended be dns of frontend service, so that requests will be distributed to all frontend nodes.                                                                \n                        after connected, all nodes in ringpop will form a ring with identifiers of what service they serve. ideally cadence should be able to get frontend address from there. but ringpop doesn’t expose this api yet.                                                                                                                                      using localhost+port or local container ip address+port will not work if the ip/container is not running frontend                                                  \nservices.name.rpc       configuration of how to listen to network ports and serve traffic.                                                                                                                                                                                                                                                                                   name: use as recommended in development.yaml. bindonip : an ip address that the container will serve the traffic with                                              \n                        bindonlocalhost:true will bind on 127.0.0.1. it’s mostly for local development. in production usually you have to specify the ip that containers will use by using bindonip                                                                                                                                                                                                                                                                                                                                             \n                        name is the matter for the “--services” option in the server startup command.                                                                                                                                                                                                                                                                                                                                                                                                                                           \nservices.name.pprof     golang profiling service , will bind on the same ip as rpc                                                                                                                                                                                                                                                                                           a port that you want to serve pprof request                                                                                                                        \nservices.name.metrics   see metrics&logging section                                                                                                                                                                                                                                                                                                                          cc                                                                                                                                                                 \nclustermetadata         cadence cluster configuration.                                                                                                                                                                                                                                                                                                                       as explanation.                                                                                                                                                    \n                        enableglobaldomain：true will enable cadence cross datacenter replication(aka xdc) feature.                                                                                                                                                                                                                                                                                                                                                                                                                              \n                        failoverversionincrement: this decides the maximum clusters that you will have replicated to each other at the same time. for example 10 is sufficient for most cases.                                                                                                                                                                                                                                                                                                                                                  \n                        masterclustername: a master cluster must be one of the enabled clusters, usually the very first cluster to start. it is only meaningful for internal purposes.                                                                                                                                                                                                                                                                                                                                                          \n                        currentclustername: current cluster name using this config file.                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n                        clusterinformation is a map from clustername to the cluster configure                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n                        initialfailoverversion: each cluster must use a different value from 0 to failoverversionincrement-1.                                                                                                                                                                                                                                                                                                                                                                                                                   \n                        rpcname: must be “cadence-frontend”. can be improved in this issue.                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n                        rpcaddress: the address to talk to the frontend of the cluster for inter-cluster replication.                                                                                                                                                                                                                                                                                                                                                                                                                           \n                        note that even if you don’t need xdc replication right now, if you want to migrate data stores in the future, you should enable xdc from every beginning. you just need to use the same name of cluster for both masterclustername and currentclustername.                                                                                                                                                                                                                                                              \n                        go to cross dc replication for how to configure replication in production                                                                                                                                                                                                                                                                                                                                                                                                                                               \ndcredirectionpolicy     for allowing forwarding frontend requests from passive cluster to active clusters.                                                                                                                                                                                                                                                                   “selected-apis-forwarding”                                                                                                                                         \narchival                this is for archival history feature, skip if you don’t need it. go to workflow archival for how to configure archival in production                                                                                                                                                                                                                 n/a                                                                                                                                                                \nblobstore               this is also for archival history feature default cadence server is using file based blob store implementation.                                                                                                                                                                                                                                      n/a                                                                                                                                                                \ndomaindefaults          default config for each domain. right now only being used for archival feature.                                                                                                                                                                                                                                                                      n/a                                                                                                                                                                \ndynamicconfigclient     dynamic config is a config manager that you can change config without restarting servers. it’s a good way for cadence to keep high availability and make things easy to configure.                                                                                                                                                                   same as the sample development config                                                                                                                              \n                        default cadence server is using filebasedclientconfig. but you can implement the dynamic config interface if you have a better way to manage.                                                                                                                                                                                                                                                                                                                                                                           \npersistence             configuration for data store / persistence layer.                                                                                                                                                                                                                                                                                                    as explanation                                                                                                                                                     \n                        values of defaultstore visibilitystore advancedvisibilitystore should be keys of map datastores.                                                                                                                                                                                                                                                                                                                                                                                                                        \n                        defaultstore is for core cadence functionality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                        visibilitystore is for basic visibility feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n                        advancedvisibilitystore is for advanced visibility                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n                        go to advanced visibility for detailed configuration of advanced visibility. see persistence documentation about using different database for cadence                                                                                                                                                                                                                                                                                                                                                                   \n\n# the full list of static configuration\nstarting from v0.21.0, all the static configuration are defined by godocs in details.\n\nversion                    godocs link                                    github link                                    \nv0.21.0                    configuration docs                             configuration                                  \n...other higher versions   ...replace the version in the url of v0.21.0   ...replace the version in the url of v0.21.0   \n\nfor earlier versions, you can find all the configurations similarly:\n\nversion                   godocs link                                    github link                                    \nv0.20.0                   configuration docs                             configuration                                  \nv0.19.2                   configuration docs                             configuration                                  \nv0.18.2                   configuration docs                             configuration                                  \nv0.17.0                   configuration docs                             configuration                                  \n...other lower versions   ...replace the version in the url of v0.20.0   ...replace the version in the url of v0.20.0   \n\n# dynamic configuration\ndynamic configuration is for fine tuning a cadence cluster.\n\nthere are a lot more dynamic configurations than static configurations. most of the default values are good for small clusters. as a cluster is scaled up, you may look for tuning it for the optimal performance.\n\nstarting from v0.21.0 with this change, all the dynamic configuration are well defined by godocs.\n\nversion                    godocs link                                    github link                                    \nv0.21.0                    dynamic configuration docs                     dynamic configuration                          \n...other higher versions   ...replace the version in the url of v0.21.0   ...replace the version in the url of v0.21.0   \n\nfor earlier versions, you can find all the configurations similarly:\n\nversion                   godocs link                                    github link                                    \nv0.20.0                   dynamic configuration docs                     dynamic configuration                          \nv0.19.2                   dynamic configuration docs                     dynamic configuration                          \nv0.18.2                   dynamic configuration docs                     dynamic configuration                          \nv0.17.0                   dynamic configuration docs                     dynamic configuration                          \n...other lower versions   ...replace the version in the url of v0.20.0   ...replace the version in the url of v0.20.0   \n\nhowever, the godocs in earlier versions don\'t contain detailed information. you need to look it up the newer version of godocs.\nfor example, search for "enableglobaldomain" in dynamic configuration comments in v0.21.0 or docs of v0.21.0, as the usage of dynamicconfiguration never changes.\n\n * keyname is the key that you will use in the dynamicconfig yaml content\n * default value is the default value\n * value type indicates the type that you should change the yaml value of: * int should be integer like 123\n    * float should be number like 123.4\n    * duration should be golang duration like: 10s, 2m, 5h for 10 seconds, 2 minutes and 5 hours.\n    * bool should be true or false\n    * map should be map of yaml\n   \n   \n * allowed filters indicates what kinds of filters you can set as constraints with the dynamic configuration. * domainname can be used with domainname\n    * n/a means no filters can be set. the config will be global.\n   \n   \n\nfor example, if you want to change the ratelimiting for list api, below is the config:\n\n// frontendvisibilitylistmaxqps is max qps frontend can list open/close workflows\n// keyname: frontend.visibilitylistmaxqps\n// value type: int\n// default value: 10\n// allowed filters: domainname\nfrontendvisibilitylistmaxqps\n\n\nthen you can add the config like:\n\nfrontend.visibilitylistmaxqps:\n  - value: 1000\n  constraints:\n    domainname: "domaina"\n    frontend.visibilitylistmaxqps:\n  - value: 2000\n  constraints:\n    domainname: "domainb"      \n\n\nyou will expect to see domaina will be able to perform 1k list operation per second, while domainb can perform 2k per second.\n\nnote 1: the size related configuration numbers are based on byte.\n\nnote 2: for <frontend,history,matching>.persistencemaxqps versus <frontend,history,matching>.persistenceglobalmaxqps --- persistencemaxqps is local for single node while persistenceglobalmaxqps is global for all node. persistenceglobalmaxqps is preferred if set as greater than zero. but by default it is zero so persistencemaxqps is being used.\n\n# how to update dynamic configuration\n * local docker-compose by mounting volume: update the cadence section in the docker compose file, and mount config folder to host machine like the following:\n\ncadence:\n  image: ubercadence/server:master-auto-setup\n  ports:\n    ...(don\'t change anything here)\n  environment:\n    ...(don\'t change anything here)\n    - "dynamic_config_file_path=/etc/custom-dynamicconfig/development.yaml"\n  volumes:\n    - "/users/<?>/cadence/config/dynamicconfig:/etc/custom-dynamicconfig"\n\n\n * local docker-compose by logging into the container: run docker exec -it docker_cadence_1 /bin/bash to login your container. then vi config/dynamicconfig/development.yaml to make any change. after you changed the config, use docker restart docker_cadence_1 to restart the cadence instance. note that you can also use this approach to change static config, but it must be changed through config/config_template.yaml instead of config/docker.yaml because config/docker.yaml is generated on startup.\n   \n   \n * in production cluster: follow this example of helm chart to deploy cadence, update dynamic config here and restart the cluster.\n   \n   \n * debug: how to make sure your updates on dynamicconfig is loaded? for example, if you added the following to development.yaml\n   \n   \n\nfrontend.visibilitylistmaxqps:\n  - value: 10000\n\n\nafter restarting cadence instances, execute a command like this to let cadence load the config(it\'s lazy loading when using it).cadence --domain <> workflow list\n\nthen you should see the logs like below\n\ncadence_1        | {"level":"info","ts":"2021-05-07t18:43:07.869z","msg":"first loading dynamic config","service":"cadence-frontend","key":"frontend.visibilitylistmaxqps,domainname:sample,clustername:primary","value":"10000","default-value":"10","logging-call-at":"config.go:93"}\n\n\n# other advanced features\n * go to advanced visibility for how to configure advanced visibility in production.\n   \n   \n * go to workflow archival for how to configure archival in production.\n   \n   \n * go to cross dc replication for how to configure replication in production.\n   \n   \n\n# deployment & release\nkubernetes is the most popular way to deploy cadence cluster. and easiest way is to use cadence helm charts that maintained by a community project.\n\nif you are looking for deploying cadence using other technologies, then it\'s reccomended to use cadence docker images. you can use offical ones, or you may customize it based on what you need. see cadence docker package for how to run the images.\n\nit\'s always recommended to use the latest release. see cadence release pages.\n\nplease subscribe the release of project by :\n\ngo to https://github.com/uber/cadence -> click the right top "watch" button -> custom -> "release".\n\nand see how to upgrade a cadence cluster',charsets:{cjk:!0}},{title:"Cluster Maintenance",frontmatter:{layout:"default",title:"Cluster Maintenance",permalink:"/docs/operation-guide/maintain",readingShow:"top"},regularPath:"/docs/07-operation-guide/02-maintain.html",relativePath:"docs/07-operation-guide/02-maintain.md",key:"v-3cd8d962",path:"/docs/operation-guide/maintain/",headers:[{level:2,title:"Scale up & down Cluster",slug:"scale-up-down-cluster",normalizedTitle:"scale up &amp; down cluster",charIndex:null},{level:2,title:"Scale up a tasklist using Scalable tasklist feature",slug:"scale-up-a-tasklist-using-scalable-tasklist-feature",normalizedTitle:"scale up a tasklist using scalable tasklist feature",charIndex:677},{level:2,title:"Restarting Cluster",slug:"restarting-cluster",normalizedTitle:"restarting cluster",charIndex:2979},{level:2,title:"Optimize SQL Persistence",slug:"optimize-sql-persistence",normalizedTitle:"optimize sql persistence",charIndex:3054},{level:2,title:"Upgrading Server",slug:"upgrading-server",normalizedTitle:"upgrading server",charIndex:4286},{level:2,title:"Migrate Cadence cluster",slug:"migrate-cadence-cluster",normalizedTitle:"migrate cadence cluster",charIndex:6373},{level:3,title:"Migrate in a naive approach",slug:"migrate-in-a-naive-approach",normalizedTitle:"migrate in a naive approach",charIndex:6740},{level:3,title:"Migrate with XDC feature",slug:"migrate-with-xdc-feature",normalizedTitle:"migrate with xdc feature",charIndex:7240},{level:2,title:"Stress/Bench Test a cluster",slug:"stress-bench-test-a-cluster",normalizedTitle:"stress/bench test a cluster",charIndex:8802}],headersStr:"Scale up & down Cluster Scale up a tasklist using Scalable tasklist feature Restarting Cluster Optimize SQL Persistence Upgrading Server Migrate Cadence cluster Migrate in a naive approach Migrate with XDC feature Stress/Bench Test a cluster",content:'# Cluster Maintenance\nThis includes how to use and maintain a Cadence cluster for both clients and server clusters.\n\n# Scale up & down Cluster\n * When CPU/Memory is getting bottleneck on Cadence instances, you may scale up or add more instances.\n * Watch Cadence metrics * See if the external traffic to frontend is normal\n    * If the slowness is due to too many tasks on a tasklist, you may need to scale up the tasklist\n    * If persistence latency is getting too high, try scale up your DB instance\n   \n   \n * Never change the numOfShards of a cluster. If you need that because the current one is too small, follow the instructions to migrate your cluster to a new one.\n\n# Scale up a tasklist using Scalable tasklist feature\nBy default a tasklist is not scalable enough to support hundreds of tasks per second. That’s mainly because each tasklist is assigned to a Matching service node, and dispatching tasks in a tasklist is in sequence.\n\nIn the past, Cadence recommended using multiple tasklists to start workflow/activity. You need to make a list of tasklists and randomly pick one when starting workflows. And then when starting workers, let them listen to all the tasklists.\n\nNowadays, Cadence has a feature called “Scalable tasklist”. It will divide a tasklist into multiple logical partitions, which can distribute tasks to multiple Matching service nodes. By default this feature is not enabled because there is some performance penalty on the server side, plus it’s not common that a tasklist needs to support more than hundreds tasks per second.\n\nYou must make a dynamic configuration change in Cadence server to use this feature:\n\nmatching.numTasklistWritePartitions\n\nand\n\nmatching.numTasklistReadPartitions\n\nmatching.numTasklistWritePartitions is the number of partitions when a Cadence server sends a task to the tasklist. matching.numTasklistReadPartitions is the number of partitions when your worker accepts a task from the tasklist.\n\nThere are a few things to know when using this feature:\n\n * Always make sure matching.numTasklistWritePartitions <= matching.numTasklistReadPartitions . Otherwise there may be some tasks that are sent to a tasklist partition but no poller(worker) will be able to pick up.\n * Because of above, when scaling down the number of partitions, you must decrease the WritePartitions first, to wait for a certain time to ensure that tasks are drained, and then decrease ReadPartitions.\n * Both domain names and taskListName should be specified in the dynamic config. An example of using this feature. See more details about dynamic config format using file based dynamic config.\n\nmatching.numTasklistWritePartitions:\n  - value: 10\n    constraints:\n      domainName: "samples-domain"\n      taskListName: "aScalableTasklistName"\nmatching.numTasklistReadPartitions:\n  - value: 10\n    constraints:\n      domainName: "samples-domain"\n      taskListName: "aScalableTasklistName"\n\n\nNOTE: the value must be integer without double quotes.\n\n# Restarting Cluster\nMake sure rolling restart to keep high availability.\n\n# Optimize SQL Persistence\n * Connection is shared within a Cadence server host\n * For each host, The max number of connections it will consume is maxConn of defaultStore + maxConn of visibilityStore.\n * The total max number of connections your Cadence cluster will consume is the summary from all hosts(from Frontend/Matching/History/SysWorker services)\n * Frontend and history nodes need both default and visibility Stores, but matching and sys workers only need default Stores, they don\'t need to talk to visibility DBs.\n * For default Stores, history service will take the most connection, then Frontend/Matching. SysWorker will use much less than others\n * Default Stores is for Cadence’ core data model, which requires strong consistency. So it cannot use replicas. VisibilityStore is not for core data models. It’s recommended to use a separate DB for visibility store if using DB based visibility.\n * Visibility Stores usually take much less connection as the workload is much lightweight(less QPS and no explicit transactions).\n * Visibility Stores require eventual consistency for read. So it can use replicas.\n * MaxIdelConns should be less than MaxConns, so that the connections can be distributed better across hosts.\n\n# Upgrading Server\nThings need to keep in mind before upgrading a cluster:\n\n * Database schema changes need to apply first.\n * Usually schema change is backward compatible. So rolling back usually is not a problem. It also means that Cadence allows running a mixed version of schema, as long as they are all greater than or equal to the required version of the server. Other requirements for upgrading should be found in the release notes. It may contain information about config changes, or special rollback instructions if normal rollback may cause problems.\n * It\'s recommended to upgrade one minor version at a time. E.g, if you are at 0.10, you should upgrade to 0.11, stabilize it with running some normal workload to make sure that the upgraded server is happy with the schema changes. After ~1 hour, then upgrade to 0.12. then 0.13. etc.\n * The reason above is that for each minor upgrade, you should be able to follow the release notes about what you should do for upgrading. The release notes may require you to run some commands. This will also help to narrow down the cause when something goes wrong.\n * Do not use “auto-setup” images to upgrade your schema. It\'s mainly for development. At most for initial setup only.\n * Please subscribe the release of project by : Go to https://github.com/uber/cadence -> Click the right top "Watch" button -> Custom -> "Release".\n\nFor how to upgrade database schema, refer to this doc: SQL tool READMECassandra tool README\n\nThe tool makes use of a table called “schema_versions” to keep track of upgrading History. But there is no transaction guarantee for cross table operations. So in case of some error, you may need to fix or apply schema change manually. Also, the schema tool by default will upgrade schema to the latest, so no manual is required. ( you can also specify to let it upgrade to any place, like 0.14).\n\nDatabase schema changes are versioned in the folders: Versioned Schema Changes for Default Store and Versioned Schema Changes for Visibility Store if you use database for basic visibility instead of ElasticSearch.\n\n# Migrate Cadence cluster\nMigrating a Cadence cluster is rare, but could happen. There could be some reasons like:\n\n * Migrate to different storage, for example from Postgres/MySQL to Cassandra\n * Split traffic\n * Datacenter migration\n * Scale up -- move to a bigger cluster, with larger number of shards.\n\nBelow is two different approaches for migrating a cluster.\n\n# Migrate in a naive approach\nNOTE: This is the only way to migrate a local domain, because a local domain cannot be converted to a global domain, even after a cluster enables XDC feature.\n\n 1. Set up a new Cadence cluster\n 2. Connect client workers to both old and new clusters\n 3. Change workflow code to start new workflows only in the new cluster\n 4. Wait for all old workflows to finish in the old cluster\n 5. Shutdown the old Cadence cluster and stop the client workers from connecting to it.\n\n# Migrate with XDC feature\nNOTE: For now XDC feature requires to use the same numOfShards between different clusters until this PR is released to fix the bug.\n\nThe below steps require to enable the cross dc replication feature:\n\n 0. Assuming at the beginning, you have only one cluster.\n    \n    \n 1. Create your domain with the global domain feature(XDC). Since you only have one cluster, there is no replication happening. But you still need to tell the replication topology when creating your domain.\n    \n    \n\ncadence --do <domain_name> domain register --global_domain true --clusters <initialClustersName> --active_cluster <initialClusterName>\n\n 2. Later on, after you setting up a new cluster, you can add the cluster to domain replication config\n\ncadence --do <domain_name> domain update --clusters <initialClusterName> <newClusterName>\n\nIt will start replication right after for all the active workflows.\n\n 3. After you are sure the new cluster is healthy, you then switch the active cluster to the new cluster.\n\ncadence --do <domain_name> domain update --active_cluster <newClusterName>\n\n 4. After some time, you make sure the new cluster is running fine, then remove the old cluster from replication:\n\ncadence --do <domain_name> domain update --clusters <newClusterName>\n\nNOTE: It’s better to enable the XDC feature from the beginning for all domains. Because a local domain cannot be converted to a global one.\n\nIf your current domain is NOT a global domain, you cannot use the XDC feature to migrate. The only way is to migrate in a naive approach\n\n# Stress/Bench Test a cluster\nIt\'s recommended to run bench test on your cluster following this package to see the maximum throughput that it can take, whenever you change some setup.',normalizedContent:'# cluster maintenance\nthis includes how to use and maintain a cadence cluster for both clients and server clusters.\n\n# scale up & down cluster\n * when cpu/memory is getting bottleneck on cadence instances, you may scale up or add more instances.\n * watch cadence metrics * see if the external traffic to frontend is normal\n    * if the slowness is due to too many tasks on a tasklist, you may need to scale up the tasklist\n    * if persistence latency is getting too high, try scale up your db instance\n   \n   \n * never change the numofshards of a cluster. if you need that because the current one is too small, follow the instructions to migrate your cluster to a new one.\n\n# scale up a tasklist using scalable tasklist feature\nby default a tasklist is not scalable enough to support hundreds of tasks per second. that’s mainly because each tasklist is assigned to a matching service node, and dispatching tasks in a tasklist is in sequence.\n\nin the past, cadence recommended using multiple tasklists to start workflow/activity. you need to make a list of tasklists and randomly pick one when starting workflows. and then when starting workers, let them listen to all the tasklists.\n\nnowadays, cadence has a feature called “scalable tasklist”. it will divide a tasklist into multiple logical partitions, which can distribute tasks to multiple matching service nodes. by default this feature is not enabled because there is some performance penalty on the server side, plus it’s not common that a tasklist needs to support more than hundreds tasks per second.\n\nyou must make a dynamic configuration change in cadence server to use this feature:\n\nmatching.numtasklistwritepartitions\n\nand\n\nmatching.numtasklistreadpartitions\n\nmatching.numtasklistwritepartitions is the number of partitions when a cadence server sends a task to the tasklist. matching.numtasklistreadpartitions is the number of partitions when your worker accepts a task from the tasklist.\n\nthere are a few things to know when using this feature:\n\n * always make sure matching.numtasklistwritepartitions <= matching.numtasklistreadpartitions . otherwise there may be some tasks that are sent to a tasklist partition but no poller(worker) will be able to pick up.\n * because of above, when scaling down the number of partitions, you must decrease the writepartitions first, to wait for a certain time to ensure that tasks are drained, and then decrease readpartitions.\n * both domain names and tasklistname should be specified in the dynamic config. an example of using this feature. see more details about dynamic config format using file based dynamic config.\n\nmatching.numtasklistwritepartitions:\n  - value: 10\n    constraints:\n      domainname: "samples-domain"\n      tasklistname: "ascalabletasklistname"\nmatching.numtasklistreadpartitions:\n  - value: 10\n    constraints:\n      domainname: "samples-domain"\n      tasklistname: "ascalabletasklistname"\n\n\nnote: the value must be integer without double quotes.\n\n# restarting cluster\nmake sure rolling restart to keep high availability.\n\n# optimize sql persistence\n * connection is shared within a cadence server host\n * for each host, the max number of connections it will consume is maxconn of defaultstore + maxconn of visibilitystore.\n * the total max number of connections your cadence cluster will consume is the summary from all hosts(from frontend/matching/history/sysworker services)\n * frontend and history nodes need both default and visibility stores, but matching and sys workers only need default stores, they don\'t need to talk to visibility dbs.\n * for default stores, history service will take the most connection, then frontend/matching. sysworker will use much less than others\n * default stores is for cadence’ core data model, which requires strong consistency. so it cannot use replicas. visibilitystore is not for core data models. it’s recommended to use a separate db for visibility store if using db based visibility.\n * visibility stores usually take much less connection as the workload is much lightweight(less qps and no explicit transactions).\n * visibility stores require eventual consistency for read. so it can use replicas.\n * maxidelconns should be less than maxconns, so that the connections can be distributed better across hosts.\n\n# upgrading server\nthings need to keep in mind before upgrading a cluster:\n\n * database schema changes need to apply first.\n * usually schema change is backward compatible. so rolling back usually is not a problem. it also means that cadence allows running a mixed version of schema, as long as they are all greater than or equal to the required version of the server. other requirements for upgrading should be found in the release notes. it may contain information about config changes, or special rollback instructions if normal rollback may cause problems.\n * it\'s recommended to upgrade one minor version at a time. e.g, if you are at 0.10, you should upgrade to 0.11, stabilize it with running some normal workload to make sure that the upgraded server is happy with the schema changes. after ~1 hour, then upgrade to 0.12. then 0.13. etc.\n * the reason above is that for each minor upgrade, you should be able to follow the release notes about what you should do for upgrading. the release notes may require you to run some commands. this will also help to narrow down the cause when something goes wrong.\n * do not use “auto-setup” images to upgrade your schema. it\'s mainly for development. at most for initial setup only.\n * please subscribe the release of project by : go to https://github.com/uber/cadence -> click the right top "watch" button -> custom -> "release".\n\nfor how to upgrade database schema, refer to this doc: sql tool readmecassandra tool readme\n\nthe tool makes use of a table called “schema_versions” to keep track of upgrading history. but there is no transaction guarantee for cross table operations. so in case of some error, you may need to fix or apply schema change manually. also, the schema tool by default will upgrade schema to the latest, so no manual is required. ( you can also specify to let it upgrade to any place, like 0.14).\n\ndatabase schema changes are versioned in the folders: versioned schema changes for default store and versioned schema changes for visibility store if you use database for basic visibility instead of elasticsearch.\n\n# migrate cadence cluster\nmigrating a cadence cluster is rare, but could happen. there could be some reasons like:\n\n * migrate to different storage, for example from postgres/mysql to cassandra\n * split traffic\n * datacenter migration\n * scale up -- move to a bigger cluster, with larger number of shards.\n\nbelow is two different approaches for migrating a cluster.\n\n# migrate in a naive approach\nnote: this is the only way to migrate a local domain, because a local domain cannot be converted to a global domain, even after a cluster enables xdc feature.\n\n 1. set up a new cadence cluster\n 2. connect client workers to both old and new clusters\n 3. change workflow code to start new workflows only in the new cluster\n 4. wait for all old workflows to finish in the old cluster\n 5. shutdown the old cadence cluster and stop the client workers from connecting to it.\n\n# migrate with xdc feature\nnote: for now xdc feature requires to use the same numofshards between different clusters until this pr is released to fix the bug.\n\nthe below steps require to enable the cross dc replication feature:\n\n 0. assuming at the beginning, you have only one cluster.\n    \n    \n 1. create your domain with the global domain feature(xdc). since you only have one cluster, there is no replication happening. but you still need to tell the replication topology when creating your domain.\n    \n    \n\ncadence --do <domain_name> domain register --global_domain true --clusters <initialclustersname> --active_cluster <initialclustername>\n\n 2. later on, after you setting up a new cluster, you can add the cluster to domain replication config\n\ncadence --do <domain_name> domain update --clusters <initialclustername> <newclustername>\n\nit will start replication right after for all the active workflows.\n\n 3. after you are sure the new cluster is healthy, you then switch the active cluster to the new cluster.\n\ncadence --do <domain_name> domain update --active_cluster <newclustername>\n\n 4. after some time, you make sure the new cluster is running fine, then remove the old cluster from replication:\n\ncadence --do <domain_name> domain update --clusters <newclustername>\n\nnote: it’s better to enable the xdc feature from the beginning for all domains. because a local domain cannot be converted to a global one.\n\nif your current domain is not a global domain, you cannot use the xdc feature to migrate. the only way is to migrate in a naive approach\n\n# stress/bench test a cluster\nit\'s recommended to run bench test on your cluster following this package to see the maximum throughput that it can take, whenever you change some setup.',charsets:{}},{title:"Cluster Monitoring",frontmatter:{layout:"default",title:"Cluster Monitoring",permalink:"/docs/operation-guide/monitor",readingShow:"top"},regularPath:"/docs/07-operation-guide/03-monitoring.html",relativePath:"docs/07-operation-guide/03-monitoring.md",key:"v-6d823dbc",path:"/docs/operation-guide/monitor/",headers:[{level:2,title:"Instructions",slug:"instructions",normalizedTitle:"instructions",charIndex:23},{level:2,title:"Grafana dashboard templates",slug:"grafana-dashboard-templates",normalizedTitle:"grafana dashboard templates",charIndex:2115},{level:2,title:"Periodic tests(Canary) for health check",slug:"periodic-tests-canary-for-health-check",normalizedTitle:"periodic tests(canary) for health check",charIndex:2810}],headersStr:"Instructions Grafana dashboard templates Periodic tests(Canary) for health check",content:"# Cluster Monitoring\n# Instructions\nCadence is emitting metrics in both Server and client:\n\n * Follow this example to emit the client side metrics for Golang client.\n   \n   \n * Follow this example to emit the client side metrics for Java client. Make sure you at least upgrade to 3.0.0.\n   \n   \n * For production, follow this example of hemlchart to emit server side metrics. Or you can follow the example of local environment to Prometheus. All services need to expose a HTTP port to provide metircs like below\n   \n   \n\nmetrics:\n  prometheus:\n    timerType: \"histogram\"\n    listenAddress: \"0.0.0.0:8001\"\n\n\nThe rest of the instructions are using local environment as an example.\n\nFor local server emitting metrics to Promethues, easies way is to use docker-compose to start a local Cadence.\n\nMake sure to update the prometheus_config.yml to add \"host.docker.internal:9098\" to the scrape list before starting the docker-compose:\n\nglobal:\n  scrape_interval: 5s\n  external_labels:\n    monitor: 'cadence-monitor'\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: # addresses to scrape\n          - 'cadence:9090'\n          - 'cadence:8000'\n          - 'cadence:8001'\n          - 'cadence:8002'\n          - 'cadence:8003'\n          - 'host.docker.internal:9098'\n\n\nNote: host.docker.internal may not work for some docker versions\n\n * After updating the prometheus_config.yaml as above, run docker-compose up to start the local Cadence\n   \n   \n * Go the the sample repo, build the helloworld sample make helloworld and run the worker ./bin/helloworld -m worker, and then in another Shell start a workflow ./bin/helloworld\n   \n   \n * Go to local Prometheus server , you should be able to check the metrics handler from client/frontend/matching/history/sysWorker are all healthy as targets\n   \n   \n * Go to local Grafana , login as admin/admin.\n   \n   \n * Configure Prometheus as datasource: use http://host.docker.internal:9090 as URL of prometheus.\n   \n   \n * Import the Grafana dashboard tempalte as JSON files.\n   \n   \n\nClient side dashboard looks like this:\n\nAnd server basic dashboard:\n\n# Grafana dashboard templates\nThis package contains examples of Cadence dashboards with Prometheus.\n\n * Cadence-Client is the dashboard of client metrics, and a few server side metrics that belong to client side but have to be emitted by server(for example, workflow timeout).\n   \n   \n * Cadence-Server-Basic is the the basic server dashboard to monitor/navigate the health/status of a Cadence cluster.\n   \n   \n * Apart from the basic server dashboard, it's recommended to set up dashboards on different components for Cadence server: Frontend, History, Matching, Worker, Persistence, Archival, etc. Any contribution is always welcome to enrich the existing templates or new templates!\n   \n   \n\n# Periodic tests(Canary) for health check\nIt's recommended to run periodical test every hour on your cluster following this package to make sure a cluster is healthy.",normalizedContent:"# cluster monitoring\n# instructions\ncadence is emitting metrics in both server and client:\n\n * follow this example to emit the client side metrics for golang client.\n   \n   \n * follow this example to emit the client side metrics for java client. make sure you at least upgrade to 3.0.0.\n   \n   \n * for production, follow this example of hemlchart to emit server side metrics. or you can follow the example of local environment to prometheus. all services need to expose a http port to provide metircs like below\n   \n   \n\nmetrics:\n  prometheus:\n    timertype: \"histogram\"\n    listenaddress: \"0.0.0.0:8001\"\n\n\nthe rest of the instructions are using local environment as an example.\n\nfor local server emitting metrics to promethues, easies way is to use docker-compose to start a local cadence.\n\nmake sure to update the prometheus_config.yml to add \"host.docker.internal:9098\" to the scrape list before starting the docker-compose:\n\nglobal:\n  scrape_interval: 5s\n  external_labels:\n    monitor: 'cadence-monitor'\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: # addresses to scrape\n          - 'cadence:9090'\n          - 'cadence:8000'\n          - 'cadence:8001'\n          - 'cadence:8002'\n          - 'cadence:8003'\n          - 'host.docker.internal:9098'\n\n\nnote: host.docker.internal may not work for some docker versions\n\n * after updating the prometheus_config.yaml as above, run docker-compose up to start the local cadence\n   \n   \n * go the the sample repo, build the helloworld sample make helloworld and run the worker ./bin/helloworld -m worker, and then in another shell start a workflow ./bin/helloworld\n   \n   \n * go to local prometheus server , you should be able to check the metrics handler from client/frontend/matching/history/sysworker are all healthy as targets\n   \n   \n * go to local grafana , login as admin/admin.\n   \n   \n * configure prometheus as datasource: use http://host.docker.internal:9090 as url of prometheus.\n   \n   \n * import the grafana dashboard tempalte as json files.\n   \n   \n\nclient side dashboard looks like this:\n\nand server basic dashboard:\n\n# grafana dashboard templates\nthis package contains examples of cadence dashboards with prometheus.\n\n * cadence-client is the dashboard of client metrics, and a few server side metrics that belong to client side but have to be emitted by server(for example, workflow timeout).\n   \n   \n * cadence-server-basic is the the basic server dashboard to monitor/navigate the health/status of a cadence cluster.\n   \n   \n * apart from the basic server dashboard, it's recommended to set up dashboards on different components for cadence server: frontend, history, matching, worker, persistence, archival, etc. any contribution is always welcome to enrich the existing templates or new templates!\n   \n   \n\n# periodic tests(canary) for health check\nit's recommended to run periodical test every hour on your cluster following this package to make sure a cluster is healthy.",charsets:{}},{title:"Cluster Troubleshooting",frontmatter:{layout:"default",title:"Cluster Troubleshooting",permalink:"/docs/operation-guide/troubleshooting",readingShow:"top"},regularPath:"/docs/07-operation-guide/04-troubleshooting.html",relativePath:"docs/07-operation-guide/04-troubleshooting.md",key:"v-20490294",path:"/docs/operation-guide/troubleshooting/",headers:[{level:2,title:"Errors",slug:"errors",normalizedTitle:"errors",charIndex:290},{level:2,title:"Slowness",slug:"slowness",normalizedTitle:"slowness",charIndex:672},{level:3,title:"API High Latency",slug:"api-high-latency",normalizedTitle:"api high latency",charIndex:683},{level:3,title:"Task Processing Slowness",slug:"task-processing-slowness",normalizedTitle:"task processing slowness",charIndex:843}],headersStr:"Errors Slowness API High Latency Task Processing Slowness",content:"# Cluster Troubleshooting\nThis section is to cover some common operation issues as a RunBook. Feel free to add more, or raise issues in the to ask for more in cadence-docs project.Or talk to us in Slack support channel!\n\nWe will keep adding more stuff. Any contribution is very welcome.\n\n# Errors\n * Persistence Max QPS Reached for List Operations * Check metrics to see how many List operations are performed per second on the domain. Alternatively you can enable debug log level to see more details of how a List request is ratelimited, if it's a staging/QA cluster.\n    * Raise the ratelimiting for the domain if you believe the default ratelimit is too low\n   \n   \n\n# Slowness\n# API High Latency\n * Check persistence API request volume and latency * If the request volume aligned with the traffic, consider scale up the cluster\n   \n   \n\n# Task Processing Slowness\n * Check scale up cluster section",normalizedContent:"# cluster troubleshooting\nthis section is to cover some common operation issues as a runbook. feel free to add more, or raise issues in the to ask for more in cadence-docs project.or talk to us in slack support channel!\n\nwe will keep adding more stuff. any contribution is very welcome.\n\n# errors\n * persistence max qps reached for list operations * check metrics to see how many list operations are performed per second on the domain. alternatively you can enable debug log level to see more details of how a list request is ratelimited, if it's a staging/qa cluster.\n    * raise the ratelimiting for the domain if you believe the default ratelimit is too low\n   \n   \n\n# slowness\n# api high latency\n * check persistence api request volume and latency * if the request volume aligned with the traffic, consider scale up the cluster\n   \n   \n\n# task processing slowness\n * check scale up cluster section",charsets:{}},{title:"Overview",frontmatter:{layout:"default",title:"Overview",permalink:"/docs/operation-guide",readingShow:"top"},regularPath:"/docs/07-operation-guide/",relativePath:"docs/07-operation-guide/index.md",key:"v-681aeaca",path:"/docs/operation-guide/",headersStr:null,content:"# Operation Guide Overview\nThis document will cover things that you need to know to run a Cadence cluster in production. Topics including: setup, monitoring, maintenance and troubleshooting.",normalizedContent:"# operation guide overview\nthis document will cover things that you need to know to run a cadence cluster in production. topics including: setup, monitoring, maintenance and troubleshooting.",charsets:{}},{title:"MIT License",frontmatter:{layout:"default",title:"MIT License",permalink:"/docs/about/license",readingShow:"top"},regularPath:"/docs/08-about/01-license.html",relativePath:"docs/08-about/01-license.md",key:"v-4997885e",path:"/docs/about/license/",headersStr:null,content:'# MIT License\nCopyright (c) 2017 Uber Technologies, Inc.\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "Software"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.',normalizedContent:'# mit license\ncopyright (c) 2017 uber technologies, inc.\n\npermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the "software"), to deal\nin the software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the software, and to permit persons to whom the software is\nfurnished to do so, subject to the following conditions:\n\nthe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the software.\n\nthe software is provided "as is", without warranty of any kind, express or\nimplied, including but not limited to the warranties of merchantability,\nfitness for a particular purpose and noninfringement. in no event shall the\nauthors or copyright holders be liable for any claim, damages or other\nliability, whether in an action of contract, tort or otherwise, arising from,\nout of or in connection with the software or the use or other dealings in\nthe software.',charsets:{}},{title:"Contact us",frontmatter:{layout:"default",title:"Contact us",permalink:"/docs/about",readingShow:"top"},regularPath:"/docs/08-about/",relativePath:"docs/08-about/index.md",key:"v-f7dd2f4a",path:"/docs/about/",headersStr:null,content:"# Contact us\nIf you have a question, check whether it is already answered at stackoverflow under cadence-workflow tag.\n\nIf you still need help, visit .\n\nIf you have a feature request or a bug to report file an issue against one of the Cadence github repositories:\n\n * Cadence Service and CLI\n * Cadence Go Client\n * Cadence Go Client Samples\n * Cadence Java Client\n * Cadence Java Client Samples\n * Cadence Web UI",normalizedContent:"# contact us\nif you have a question, check whether it is already answered at stackoverflow under cadence-workflow tag.\n\nif you still need help, visit .\n\nif you have a feature request or a bug to report file an issue against one of the cadence github repositories:\n\n * cadence service and cli\n * cadence go client\n * cadence go client samples\n * cadence java client\n * cadence java client samples\n * cadence web ui",charsets:{}},{title:"Home",frontmatter:{home:!0,heroText:"Fault-Tolerant Stateful Code Platform",tagline:"Focus on your business logic and let Cadence take care of the complexity of distributed systems",actionText:"Get Started →",actionLink:"/docs/get-started/",footer:"© 2021 Uber Technologies, Inc.",readingShow:"top"},regularPath:"/",relativePath:"index.md",key:"v-0615a98a",path:"/",headersStr:null,content:" Easy to use\nWorkflows provide primitives to allow application developers to express complex business logic as code.\n\nThe underlying platform abstracts scalability, reliability and availability concerns from individual developers/teams.\n\nFault tolerant\nCadence enables writing stateful applications without worrying about the complexity of handling process failures.\n\nCadence preserves complete multithreaded application state including thread stacks with local variables across hardware and software failures.\n\nScalable & Reliable\nCadence is designed to scale out horizontally to handle millions of concurrent workflows.\n\nCadence provides out-of-the-box asynchronous history event replication that can help you recover from zone failures.",normalizedContent:" easy to use\nworkflows provide primitives to allow application developers to express complex business logic as code.\n\nthe underlying platform abstracts scalability, reliability and availability concerns from individual developers/teams.\n\nfault tolerant\ncadence enables writing stateful applications without worrying about the complexity of handling process failures.\n\ncadence preserves complete multithreaded application state including thread stacks with local variables across hardware and software failures.\n\nscalable & reliable\ncadence is designed to scale out horizontally to handle millions of concurrent workflows.\n\ncadence provides out-of-the-box asynchronous history event replication that can help you recover from zone failures.",charsets:{}}],themeConfig:{docsDir:"src",logo:"/img/logo-white.svg",docsRepo:"uber/cadence-docs",editLinks:!0,nav:[{text:"Docs",items:[{text:"Get Started",link:"/docs/get-started/"},{text:"Use cases",link:"/docs/use-cases/"},{text:"Concepts",link:"/docs/concepts/"},{text:"Java client",link:"/docs/java-client/"},{text:"Go client",link:"/docs/go-client/"},{text:"Command line interface",link:"/docs/cli/"},{text:"Operation Guide",link:"/docs/operation-guide/"},{text:"Glossary",link:"/GLOSSARY"},{text:"About",link:"/docs/about/"}]},{text:"Client",items:[{text:"Java Docs",link:"https://www.javadoc.io/doc/com.uber.cadence/cadence-client"},{text:"Java Client",link:"https://mvnrepository.com/artifact/com.uber.cadence/cadence-client"},{text:"Go Docs",link:"https://godoc.org/go.uber.org/cadence"},{text:"Go Client",link:"https://github.com/uber-go/cadence-client/releases/latest"}]},{text:"Community",items:[{text:"Slack",link:"http://t.uber.com/cadence-slack"},{text:"StackOverflow",link:"https://stackoverflow.com/questions/tagged/cadence-workflow"}]},{text:"GitHub",items:[{text:"Cadence Service and CLI",link:"https://github.com/uber/cadence"},{text:"Cadence Go Client",link:"https://github.com/uber-go/cadence-client"},{text:"Cadence Go Client Samples",link:"https://github.com/uber-common/cadence-samples"},{text:"Cadence Java Client",link:"https://github.com/uber-java/cadence-client"},{text:"Cadence Java Client Samples",link:"https://github.com/uber/cadence-java-samples"},{text:"Cadence Web UI",link:"https://github.com/uber/cadence-web"},{text:"Cadence Docs",link:"https://github.com/uber/cadence-docs"}]},{text:"Docker",items:[{text:"Cadence Service",link:"https://hub.docker.com/r/ubercadence/server/tags"},{text:"Cadence CLI",link:"https://hub.docker.com/r/ubercadence/cli/tags"},{text:"Cadence Web UI",link:"https://hub.docker.com/r/ubercadence/web/tags"}]}],sidebar:{"/docs/":[{title:"Get Started",path:"/docs/01-get-started",children:["01-get-started/","01-get-started/01-server-installation","01-get-started/02-java-hello-world","01-get-started/03-golang-hello-world","01-get-started/04-video-tutorials"]},{title:"Use cases",path:"/docs/02-use-cases",children:["02-use-cases/","02-use-cases/01-periodic-execution","02-use-cases/02-orchestration","02-use-cases/03-polling","02-use-cases/04-event-driven","02-use-cases/05-partitioned-scan","02-use-cases/06-batch-job","02-use-cases/07-provisioning","02-use-cases/08-deployment","02-use-cases/09-operational-management","02-use-cases/10-interactive","02-use-cases/11-dsl","02-use-cases/12-big-ml"]},{title:"Concepts",path:"/docs/03-concepts",children:["03-concepts/","03-concepts/01-workflows","03-concepts/02-activities","03-concepts/03-events","03-concepts/04-queries","03-concepts/05-topology","03-concepts/06-task-lists","03-concepts/07-archival","03-concepts/08-cross-dc-replication","03-concepts/09-search-workflows"]},{title:"Java client",path:"/docs/04-java-client",children:["04-java-client/","04-java-client/01-client-overview","04-java-client/02-workflow-interface","04-java-client/03-implementing-workflows","04-java-client/04-starting-workflow-executions","04-java-client/05-activity-interface","04-java-client/06-implementing-activities","04-java-client/07-versioning","04-java-client/08-distributed-cron","04-java-client/09-workers","04-java-client/10-signals","04-java-client/11-queries","04-java-client/12-retries","04-java-client/13-child-workflows","04-java-client/14-exception-handling","04-java-client/15-continue-as-new","04-java-client/16-side-effect","04-java-client/17-testing","04-java-client/18-workflow-replay-shadowing"]},{title:"Go client",path:"/docs/05-go-client",children:["05-go-client/","05-go-client/01-workers","05-go-client/02-create-workflows","05-go-client/03-activities","05-go-client/04-execute-activity","05-go-client/05-child-workflows","05-go-client/06-retries","05-go-client/07-error-handling","05-go-client/08-signals","05-go-client/09-continue-as-new","05-go-client/10-side-effect","05-go-client/11-queries","05-go-client/12-activity-async-completion","05-go-client/13-workflow-testing","05-go-client/14-workflow-versioning","05-go-client/15-sessions","05-go-client/16-distributed-cron","05-go-client/17-tracing","05-go-client/18-workflow-replay-shadowing"]},{title:"Command line interface",path:"/docs/06-cli/"},{title:"Production Operation",path:"/docs/07-operation-guide/",children:["07-operation-guide/","07-operation-guide/01-setup","07-operation-guide/02-maintain","07-operation-guide/03-monitoring","07-operation-guide/04-troubleshooting"]},{title:"Glossary",path:"../GLOSSARY"},{title:"About",path:"/docs/08-about",children:["08-about/","08-about/01-license"]}]}}};n(316);Pi.component("slack-link",(function(){return n.e(6).then(n.bind(null,389))})),Pi.component("Badge",(function(){return Promise.all([n.e(0),n.e(3)]).then(n.bind(null,469))})),Pi.component("CodeBlock",(function(){return Promise.all([n.e(0),n.e(4)]).then(n.bind(null,390))})),Pi.component("CodeGroup",(function(){return Promise.all([n.e(0),n.e(5)]).then(n.bind(null,391))}));n(317),n(318);var ws={name:"BackToTop",props:{threshold:{type:Number,default:300}},data:function(){return{scrollTop:null}},computed:{show:function(){return this.scrollTop>this.threshold}},mounted:function(){var e=this;this.scrollTop=this.getScrollTop(),window.addEventListener("scroll",ns()((function(){e.scrollTop=e.getScrollTop()}),100))},methods:{getScrollTop:function(){return window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0},scrollToTop:function(){window.scrollTo({top:0,behavior:"smooth"}),this.scrollTop=0}}},gs=(n(319),Object(cs.a)(ws,(function(){var e=this.$createElement,t=this._self._c||e;return t("transition",{attrs:{name:"fade"}},[this.show?t("svg",{staticClass:"go-to-top",attrs:{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 49.484 28.284"},on:{click:this.scrollToTop}},[t("g",{attrs:{transform:"translate(-229 -126.358)"}},[t("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(229 151.107) rotate(-45)"}}),this._v(" "),t("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(274.949 154.642) rotate(-135)"}})])]):this._e()])}),[],!1,null,"5fd4ef0c",null).exports);n(320);Pi.component("CodeSwitcher",(function(){return n.e(8).then(n.bind(null,392))}));n(111);var ys={name:"ReadingProgress",data:function(){return{readingTop:0,readingHeight:1,progressStyle:null,transform:void 0,running:!1}},watch:{$readingShow:function(){this.progressStyle=this.getProgressStyle(),this.$readingShow&&window.addEventListener("scroll",this.base)}},mounted:function(){this.transform=this.getTransform(),this.progressStyle=this.getProgressStyle(),this.$readingShow&&window.addEventListener("scroll",this.base)},beforeDestroy:function(){this.$readingShow&&window.removeEventListener("scroll",this.base)},methods:{base:function(){this.running||(this.running=!0,requestAnimationFrame(this.getReadingBase))},getReadingBase:function(){this.readingHeight=this.getReadingHeight()-this.getScreenHeight(),this.readingTop=this.getReadingTop(),this.progressStyle=this.getProgressStyle(),this.running=!1},getReadingHeight:function(){return Math.max(document.body.scrollHeight,document.body.offsetHeight,0)},getScreenHeight:function(){return Math.max(window.innerHeight,document.documentElement.clientHeight,0)},getReadingTop:function(){return Math.max(window.pageYOffset,document.documentElement.scrollTop,0)},getTransform:function(){var e=document.createElement("div");return["transform","-webkit-transform","-moz-transform","-o-transform","-ms-transform"].find((function(t){return t in e.style}))||void 0},getProgressStyle:function(){var e=this.readingTop/this.readingHeight;switch(this.$readingShow){case"top":case"bottom":return this.transform?"".concat(this.transform,": scaleX(").concat(e,")"):"width: ".concat(100*e,"%");case"left":case"right":return this.transform?"".concat(this.transform,": scaleY(").concat(e,")"):"height: ".concat(100*e,"%");default:return null}}}},vs=(n(321),Object(cs.a)(ys,(function(){var e=this.$createElement,t=this._self._c||e;return t("ClientOnly",[this.$readingShow?t("div",{staticClass:"reading-progress",class:this.$readingShow},[t("div",{staticClass:"progress",style:this.progressStyle})]):this._e()])}),[],!1,null,"3640397f",null).exports);n(104);function bs(e,t){var n=!0;void 0===e?(e="Term not found in the glossary",n=!1):e=ks(e);var o=n?"":" term-not-found";return t=xs(t),'<a title="'.concat(e,'" class="term').concat(o,'">').concat(t,"</a>")}function ks(e){return e.replace(/:[\w+]*:([\w+]*):/g,(function(e,t){return t})).replace(/:([\w+]*):/g,(function(e,t){return t}))}function xs(e){return e.split("_").join(" ")}function Ss(e){return e.split("_").join(" ")}var Cs={name:"Term",props:{term:{type:String,required:!0},show:{type:String,required:!1,default:""}},data:function(){return{termNotFound:!1}},computed:{terms:function(){return this.$site.pages.find((function(e){return"/GLOSSARY.html"===e.path})).frontmatter.terms},definition:function(){var e=Ss(this.term),t=this.terms[e];return t?ks(t):(this.termNotFound=!0,"Term not found in the glossary")},displayText:function(){return Ss(this.show?this.show:this.term)}}},Ts=Object(cs.a)(Cs,(function(){var e=this.$createElement;return(this._self._c||e)("a",{class:{"term-not-found":this.termNotFound,term:!0},attrs:{title:this.definition}},[this._v(this._s(this.displayText))])}),[],!1,null,null,null).exports,Is={props:{terms:{type:Object,required:!0}},methods:{definition:function(e){return t=e,n=this.terms,n[xs(t)].replace(/:([\w+]*):([\w+]*):/g,(function(e,t,o){return bs(n[xs(t)],o)})).replace(/:([\w+]*):/g,(function(e,t,o){return bs(n[xs(t)],t)}));var t,n}}},As=(n(322),Object(cs.a)(Is,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("dl",e._l(Object.keys(e.terms),(function(t){return n("div",[n("dt",{staticClass:"defined-term"},[e._v(e._s(t))]),e._v(" "),n("dd",{staticClass:"term-definition",domProps:{innerHTML:e._s(e.definition(t,e.terms))}})])})),0)}),[],!1,null,null,null).exports),_s=[function(e){e.router.addRoutes([{path:"/docs/",redirect:"/docs/cadence"}])},{},function(e){e.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},{},function(e){e.Vue.component("BackToTop",gs)},{},{},function(e){var t=e.Vue;t.component(vs.name,vs),t.mixin({computed:{$readingShow:function(){return this.$page.frontmatter.readingShow}}})},function(e){e.Vue.component("CodeCopy",ds)},function(e){var t=e.Vue;e.options,e.router,e.siteData;t.component("Term",Ts),t.component("Glossary",As)}],Es=["BackToTop","ReadingProgress"];n(187);function js(e,t){if(!(e instanceof t))throw new TypeError("Cannot call a class as a function")}n(102);function Os(e,t){for(var n=0;n<t.length;n++){var o=t[n];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(e,o.key,o)}}function Ws(e,t,n){return t&&Os(e.prototype,t),n&&Os(e,n),e}n(181);function Ps(e,t){return(Ps=Object.setPrototypeOf||function(e,t){return e.__proto__=t,e})(e,t)}n(182);function qs(e){return(qs=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}n(120),n(110);function Rs(e,t){return!t||"object"!==Ta(t)&&"function"!=typeof t?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):t}function Ds(e){var t=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}();return function(){var n,o=qs(e);if(t){var i=qs(this).constructor;n=Reflect.construct(o,arguments,i)}else n=o.apply(this,arguments);return Rs(this,n)}}var Ns=function(e){!function(e,t){if("function"!=typeof t&&null!==t)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(t&&t.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),t&&Ps(e,t)}(n,e);var t=Ds(n);function n(){return js(this,n),t.apply(this,arguments)}return n}(function(){function e(){js(this,e),this.store=new Pi({data:{state:{}}})}return Ws(e,[{key:"$get",value:function(e){return this.store.state[e]}},{key:"$set",value:function(e,t){Pi.set(this.store.state,e,t)}},{key:"$emit",value:function(){var e;(e=this.store).$emit.apply(e,arguments)}},{key:"$on",value:function(){var e;(e=this.store).$on.apply(e,arguments)}}]),e}());Object.assign(Ns.prototype,{getPageAsyncComponent:Ma,getLayoutAsyncComponent:Ga,getAsyncComponent:Ha,getVueComponent:$a});var zs={install:function(e){var t=new Ns;e.$vuepress=t,e.prototype.$vuepress=t}};function Fs(e){e.beforeEach((function(t,n,o){if(Ls(e,t.path))o();else if(/(\/|\.html)$/.test(t.path))if(/\/$/.test(t.path)){var i=t.path.replace(/\/$/,"")+".html";Ls(e,i)?o(i):o()}else o();else{var r=t.path+"/",a=t.path+".html";Ls(e,a)?o(a):Ls(e,r)?o(r):o()}}))}function Ls(e,t){return e.options.routes.filter((function(e){return e.path.toLowerCase()===t.toLowerCase()})).length>0}var Ms={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(e){var t=this.pageKey||this.$parent.$page.key;return Ba("pageKey",t),Pi.component(t)||Pi.component(t,Ma(t)),Pi.component(t)?e(t):e("")}},Gs={functional:!0,props:{slotKey:String,required:!0},render:function(e,t){var n=t.props,o=t.slots;return e("div",{class:["content__".concat(n.slotKey)]},o()[n.slotKey])}},Hs={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},$s=(n(324),n(325),Object(cs.a)(Hs,(function(){var e=this.$createElement,t=this._self._c||e;return t("span",[t("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[t("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),t("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),t("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Us(){return(Us=Object(o.a)(regeneratorRuntime.mark((function e(t){var n,o,i,r;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return n="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:ms.routerBase||ms.base,Fs(o=new Sa({base:n,mode:"history",fallback:!1,routes:ps,scrollBehavior:function(e,t,n){return n||(e.hash?!Pi.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})}})),i={},e.prev=4,e.next=7,Promise.all(_s.filter((function(e){return"function"==typeof e})).map((function(e){return e({Vue:Pi,options:i,router:o,siteData:ms,isServer:t})})));case 7:e.next=12;break;case 9:e.prev=9,e.t0=e.catch(4),console.error(e.t0);case 12:return r=new Pi(Object.assign(i,{router:o,render:function(e){return e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},Es.map((function(t){return e(t)})))])}})),e.abrupt("return",{app:r,router:o});case 14:case"end":return e.stop()}}),e,null,[[4,9]])})))).apply(this,arguments)}Pi.config.productionTip=!1,Pi.use(Sa),Pi.use(zs),Pi.mixin(function(e,t){var n=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Pi;Ca(t),n.$vuepress.$set("siteData",t);var o=e(n.$vuepress.$get("siteData")),i=new o,r=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(i)),a={};return Object.keys(r).reduce((function(e,t){return t.startsWith("$")&&(e[t]=r[t].get),e}),a),{computed:a}}((function(e){return function(){function t(){js(this,t)}return Ws(t,[{key:"setPage",value:function(e){this.__page=e}},{key:"$site",get:function(){return e}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var e,t,n=this.$site.locales,o=void 0===n?{}:n;for(var i in o)"/"===i?t=o[i]:0===this.$page.path.indexOf(i)&&(e=o[i]);return e||t||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var e=this.$page.frontmatter.canonicalUrl;return"string"==typeof e&&e}},{key:"$title",get:function(){var e=this.$page,t=this.$page.frontmatter.metaTitle;if("string"==typeof t)return t;var n=this.$siteTitle,o=e.frontmatter.home?null:e.frontmatter.title||e.title;return n?o?o+" | "+n:n:o||"VuePress"}},{key:"$description",get:function(){var e=function(e){if(e){var t=e.filter((function(e){return"description"===e.name}))[0];if(t)return t.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(e,t){for(var n=0;n<e.length;n++){var o=e[n];if(o.path.toLowerCase()===t.toLowerCase())return o}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),t}()}),ms)),Pi.component("Content",Ms),Pi.component("ContentSlotsDistributor",Gs),Pi.component("OutboundLink",$s),Pi.component("ClientOnly",{functional:!0,render:function(e,t){var n=t.parent,o=t.children;if(n._isMounted)return o;n.$once("hook:mounted",(function(){n.$forceUpdate()}))}}),Pi.component("Layout",Ga("Layout")),Pi.component("NotFound",Ga("NotFound")),Pi.prototype.$withBase=function(e){var t=this.$site.base;return"/"===e.charAt(0)?t+e.slice(1):e},window.__VUEPRESS__={version:"1.7.1",hash:"beb0340"},function(e){return Us.apply(this,arguments)}(!1).then((function(e){var t=e.app;e.router.onReady((function(){t.$mount("#app")}))}))}]);