(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{339:function(t,e,a){"use strict";a.r(e);var r=a(0),s=Object(r.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"storage-scan"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#storage-scan"}},[t._v("#")]),t._v(" Storage scan")]),t._v(" "),e("p",[t._v("It is common to have large data sets partitioned across a large number of hosts or databases, or having billions of files in an Amazon S3 bucket.\nCadence is an ideal solution for implementing the full scan of such data in a scalable and resilient way. The standard pattern\nis to run an "),e("Term",{attrs:{term:"activity"}}),t._v(" (or multiple parallel "),e("Term",{attrs:{term:"activity",show:"activities"}}),t._v(" for partitioned data sets) that performs the scan and heartbeats its progress\nback to Cadence. In the case of a host failure, the "),e("Term",{attrs:{term:"activity"}}),t._v(" is retried on a different host and continues execution from the last reported progress.")],1),t._v(" "),e("p",[t._v("A real-world example:")]),t._v(" "),e("ul",[e("li",[t._v("Cadence internal system "),e("Term",{attrs:{term:"workflow"}}),t._v(" that performs periodic scan of all "),e("Term",{attrs:{term:"workflow_execution"}}),t._v(" records")],1)])])}),[],!1,null,null,null);e.default=s.exports}}]);